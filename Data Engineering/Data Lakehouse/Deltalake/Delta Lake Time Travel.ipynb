{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bb07176-cfb8-4f3e-80c6-559680a3b47b",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f716d2-aaf3-47fe-9ba1-1ddee7f5c1cb",
   "metadata": {},
   "source": [
    "In February 19, 2019, Databricks [announced](https://databricks.com/blog/2019/02/04/introducing-delta-time-travel-for-large-scale-data-lakes.html) the release of the time travel feature. \n",
    "\n",
    "> With this new feature, Delta automatically versions the big data that you store in your data lake, and you can access any historical version of that data. This temporal data management simplifies your data pipeline by making it easy to audit, roll back data in case of accidental bad writes or deletes, and reproduce experiments and reports. Your organization can finally standardize on a clean, centralized,  versioned big data repository in your own cloud storage for your analytics.\n",
    "\n",
    "In this notebook we will explore this feature as well as the problems it is looking to solve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90e2897-375d-42b4-ae65-ebd204be7e02",
   "metadata": {},
   "source": [
    "## Reminders\n",
    "Recall that delta lake is built on top of Apache Spark. Delta lake is the storage layer while Spark is the execution layer.\n",
    "\n",
    "As such Spark is providing a significant portion of the tech stack one would interact with when using delta lake. We load our data using spark, we transform our data using spark, we are using spark. As such, we must be aware of some of the tools in the spark tool belt that will be relevant to the aspect of time travel. \n",
    "\n",
    "The Spark provides a python api called PySpark. PySpark allows python to communicate with the lower level APIs of the execution layer; specifically the Dataframe API which loads and manipulates dataframes, and the SQL API which allows us to execute SQL queries against dataframes the way we would against traditional sql tables (sits on top of dataframes).\n",
    "\n",
    "At this point, to exploit ALL the time travel functionality, there are some things we must do with the spark SQL API. We will point those out as they are encountered."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e316141e-0970-4052-8ccf-8c4fd0259e12",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 1. Challenges Being Addressed\n",
    "\n",
    "## 1.1. Audit data changes\n",
    "Auditing data changes is critical from both in terms of data compliance as well as simple debugging to understand how data has changed over time. Traditionally the audit was tightly coupled with the pipelines responsible for delivering the data. This feature standardizes the audit regardless of how the data got into the data lake.\n",
    "\n",
    "## 1.2. Reproduce experiments & reports\n",
    "During model training, data scientists run various experiments with different parameters on a given set of data. When scientists revisit their experiments after a period of time to reproduce the models, typically the source data has been modified by upstream pipelines. Lot of times, they are caught unaware by such upstream data changes and hence struggle to reproduce their experiments. Some scientists and organizations engineer best practices by creating multiple copies of the data, leading to increased storage costs. The same is true for analysts generating reports.\n",
    "\n",
    "## 1.3. Rollbacks\n",
    "Data pipelines can sometimes write bad data for downstream consumers. This can happen because of issues ranging from infrastructure instabilities to messy data to bugs in the pipeline. For pipelines that do simple appends to directories or a table, rollbacks can easily be addressed by date-based partitioning. With updates and deletes, this can become very complicated, and data engineers typically have to engineer a complex pipeline to deal with such scenarios. This is no longer the case with time travel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5424b5f-7f12-4621-b0be-a0d58a4a7401",
   "metadata": {},
   "source": [
    "# 2. Under The Hood: How does time travel work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073f3b3c-db80-4f04-9655-db6e44257cff",
   "metadata": {},
   "source": [
    "As you write into a Delta table or directory, every operation is automatically versioned. And deltalake provides you with an API for accessing data as of a specified timestamp. But how does this work under the hood?\n",
    "\n",
    "## 2.1. A Brief Explanation of what delta lake is and how it works\n",
    "A great video can be found [here](https://www.youtube.com/watch?v=o-zcZvfSUyIhttps://www.youtube.com/watch?v=o-zcZvfSUyI) and a complimentary article can be found [here](https://databricks.com/blog/2019/08/21/diving-into-delta-lake-unpacking-the-transaction-log.html).\n",
    "\n",
    "A Delta lake is a collection of directories and delta tables stored in a data lake. As the name suggests, a delta table is a data table composed of deltas; in other words, a delta table is the result of applying a series of transactions to an initial state. We will see that a delta table is not a file, but a carefully managed directory consisting of files representing the transactions and data which compose the delta table. When we load a table into memory we are actually assembling the table from data files stored in the directory.\n",
    "\n",
    "As such, when we perform operations on the delta table (i.e. store a snapshot of the state of a spark dataframe), we see the files in this dirctory are modified. \n",
    "\n",
    "The delta table directory contains several types of files\n",
    "1. Compressed parquet files\n",
    "2. Checksums to validate the parwuet files\n",
    "3. Transaction logs\n",
    "4. Checkpoint files (cached state to improve computational efficiency)\n",
    "\n",
    "The delta table has a log directory which stores json log files which record operations beind performed on the delta table. These operations describe how we transform the data from one state to another state. The operations can be categorized as follows:\n",
    "- update metadata\n",
    "- add parquet file\n",
    "- remove parquet file\n",
    "- set transaction (record idempotent transaction id)\n",
    "- change transaction protocol version\n",
    "\n",
    "This log directory is extremely important because it is what provides the information for calculating versions and of data and their coresponding timestamps. The log directory consists of transaction files, each transaction file pertains to a specific version of data. File '00000000000000000000.json' coresponds to version 0 while file '00000000000000000000.json' coresponds to version 1 of the delta table. Each transaction file has a modified data assigned by the operating system. This timestamp attaches a datatime to our version.\n",
    "\n",
    "Below we can see an example detlat table which i have stored in the `data/time-travel-demo.delta` directory\n",
    "\n",
    "```\n",
    "[root@pc]# tree data/time-travel-demo.delta\n",
    "data/time-travel-demo.delta/\n",
    "|-- _delta_log\n",
    "|   |-- 00000000000000000000.json\n",
    "|   `-- 00000000000000000001.json\n",
    "|-- part-00000-41e1a8cd-15d5-4263-9413-f649ad1d51da-c000.snappy.parquet\n",
    "`-- part-00000-e8891ae6-e9d4-449a-8531-de44d41f7669-c000.snappy.parquetdata\n",
    "```\n",
    "\n",
    "We see that the directory is filled with files (the crc files are hidden)\n",
    "\n",
    "```\n",
    "[root@pc]# ls -la data/time-travel-demo.delta/\n",
    "\n",
    "total 3\n",
    "drwxr-xr-x 1 root root   5 May 20 19:29 .\n",
    "drwxr-xr-x 1 root root   2 May 20 18:42 ..\n",
    "-rw-r--r-- 1 root root  16 May 20 18:42 .part-00000-41e1a8cd-15d5-4263-9413-f649ad1d51da-c000.snappy.parquet.crc\n",
    "-rw-r--r-- 1 root root  16 May 20 19:29 .part-00000-e8891ae6-e9d4-449a-8531-de44d41f7669-c000.snappy.parquet.crc\n",
    "drwxr-xr-x 1 root root   2 May 20 19:29 _delta_log\n",
    "-rw-r--r-- 1 root root 735 May 20 18:42 part-00000-41e1a8cd-15d5-4263-9413-f649ad1d51da-c000.snappy.parquet\n",
    "-rw-r--r-- 1 root root 946 May 20 19:29 part-00000-e8891ae6-e9d4-449a-8531-de44d41f7669-c000.snappy.parquet\n",
    "```\n",
    "\n",
    "And we can get the timestamp for the modification of the file\n",
    "\n",
    "```\n",
    "[root@pc]# stat data/time-travel-demo.delta/_delta_log/*\n",
    "\n",
    "  File: 'data/time-travel-demo.delta/_delta_log/00000000000000000000.json'\n",
    "  Size: 819       \tBlocks: 2          IO Block: 262144 regular file\n",
    "Device: 44h/68d\tInode: 1099511789237  Links: 1\n",
    "Access: (0644/-rw-r--r--)  Uid: (    0/    root)   Gid: (    0/    root)\n",
    "Access: 2022-05-20 18:42:13.100546536 +0000\n",
    "Modify: 2022-05-20 18:42:13.341529695 +0000\n",
    "Change: 2022-05-20 18:42:13.691505237 +0000\n",
    " Birth: -\n",
    "  File: 'data/time-travel-demo.delta/_delta_log/00000000000000000001.json'\n",
    "  Size: 1049      \tBlocks: 3          IO Block: 262144 regular file\n",
    "Device: 44h/68d\tInode: 1099511789264  Links: 1\n",
    "Access: (0644/-rw-r--r--)  Uid: (    0/    root)   Gid: (    0/    root)\n",
    "Access: 2022-05-20 19:29:50.672776623 +0000\n",
    "Modify: 2022-05-20 19:29:51.389726484 +0000\n",
    "Change: 2022-05-20 19:29:51.526716905 +0000\n",
    " Birth: -  \n",
    " ```\n",
    " \n",
    "**Note**: While delta lake does use this modification timestamp to attach a timestamp to a version number, it is not exact. We will see that timestamps for version numbers only takethe first three digits of the microsecond field. In this example we would see a timestamp of '2022-05-20 19:29:51.389' attached to version 1. \n",
    "\n",
    "**Note**: If the filesystem timestamps get messed up or out of order, delta lake will add one milisecond to the previoustimestamp to compute a new timestamp for a version. So the filesystem timestamps may not always line up.\n",
    "\n",
    "We will see that deltalake provides a utility for calculating the timestamps ov each version so we dont have to worry!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbd9cb9-9b0f-48ba-a642-6f8c1c62e432",
   "metadata": {},
   "source": [
    "### 2.1.1. A note on concurrency\n",
    "\n",
    "In certain situations it is possible that multiple users will be reading and writing data to the same delta table at the same time. The implimentation is described in [this article](https://databricks.com/blog/2019/08/21/diving-into-delta-lake-unpacking-the-transaction-log.html) but it is outside the scope of this article."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e87fda-834c-4c1e-bb10-85f1b6236842",
   "metadata": {},
   "source": [
    "## 2.1. Fire up spark\n",
    "We assume you already have a working spark implimentation and you have already installed the delta pip package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5e6ec10-86b5-45ec-88ad-7710f5e3ed11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/lib/spark-3.1.1-bin-hadoop2.7/jars/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/lib/spark-3.1.1-bin-hadoop2.7/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-0ab530cd-2901-4322-968e-19fb3fd2fb0d;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;1.0.1 in central\n",
      "\tfound org.antlr#antlr4;4.7 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.7 in central\n",
      "\tfound org.antlr#antlr-runtime;3.5.2 in central\n",
      "\tfound org.antlr#ST4;4.0.8 in central\n",
      "\tfound org.abego.treelayout#org.abego.treelayout.core;1.0.3 in central\n",
      "\tfound org.glassfish#javax.json;1.0.4 in central\n",
      "\tfound com.ibm.icu#icu4j;58.2 in central\n",
      ":: resolution report :: resolve 972ms :: artifacts dl 15ms\n",
      "\t:: modules in use:\n",
      "\tcom.ibm.icu#icu4j;58.2 from central in [default]\n",
      "\tio.delta#delta-core_2.12;1.0.1 from central in [default]\n",
      "\torg.abego.treelayout#org.abego.treelayout.core;1.0.3 from central in [default]\n",
      "\torg.antlr#ST4;4.0.8 from central in [default]\n",
      "\torg.antlr#antlr-runtime;3.5.2 from central in [default]\n",
      "\torg.antlr#antlr4;4.7 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.7 from central in [default]\n",
      "\torg.glassfish#javax.json;1.0.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   8   |   0   |   0   |   0   ||   8   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-0ab530cd-2901-4322-968e-19fb3fd2fb0d\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 8 already retrieved (0kB/16ms)\n",
      "22/05/25 15:05:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "import delta\n",
    "sparkConf = pyspark.SparkConf()\n",
    "sparkConf.setAppName(\"delta-time-travel-demo\")\n",
    "sparkConf.set(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "sparkConf.set(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "sparkConf.set(\"spark.databricks.delta.stateReconstructionValidation.enabled\", \"false\")\n",
    "sparkSessionBuilder = pyspark.sql.SparkSession.builder.config(conf=sparkConf)\n",
    "sparkSession = delta.configure_spark_with_delta_pip(sparkSessionBuilder).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbe2bb3-a7ac-4e2f-b8ba-5bd52f40985e",
   "metadata": {},
   "source": [
    "## 2.2. Write data and observe changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51974ea8-ef38-4d89-969a-b9f7ccde02e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  A|  B|\n",
      "+---+---+\n",
      "|  1|  6|\n",
      "|  2|  7|\n",
      "|  3|  8|\n",
      "|  4|  9|\n",
      "|  5| 10|\n",
      "+---+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "pandas_df = pandas.DataFrame({\n",
    "    \"A\": [1,2,3,4,5],\n",
    "    \"B\": [6,7,8,9,10],\n",
    "})\n",
    "spark_df = sparkSession.createDataFrame(pandas_df)\n",
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29c2ae3-1e92-4d1a-a8a0-160056e14c1d",
   "metadata": {},
   "source": [
    " Before writing our data we will determine where we are writing and will cleanup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1359a95f-cd95-46a5-9076-c20f138ca893",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyprojroot\n",
    "project_root_dir = pyprojroot.here()\n",
    "data_directory = os.path.join(project_root_dir, r\"Example Data Sets\", \"deltalake\")\n",
    "table_name = \"time-travel-demo.delta\"\n",
    "full_delta_table_path = os.path.join(data_directory, table_name)\n",
    "escaped_full_delta_table_path = full_delta_table_path.replace(\" \", \"\\ \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a97bbd4-7bdc-454a-81a2-7b9603798b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf {escaped_full_delta_table_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2681ce9b-14e7-4cf5-b1a9-43db47d5d4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p {escaped_full_delta_table_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1241bdc0-c4dd-40e2-9f39-a806f2d7e2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 0\n",
      "drwxr-xr-x 1 root root 0 May 25 15:10 .\n",
      "drwxr-xr-x 1 root root 1 May 25 15:10 ..\n"
     ]
    }
   ],
   "source": [
    "! ls -la {escaped_full_delta_table_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50b121d-75db-4dae-b92d-db78e226aa3e",
   "metadata": {},
   "source": [
    "We now write the data as a delta table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45a768cf-3856-4c59-b8dd-2092836e4640",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark_df.write.format(\"delta\").save(full_delta_table_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd28f744-7e48-4cf6-b266-ff02965cfedf",
   "metadata": {},
   "source": [
    "We again observe our directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21f86af8-0a35-43ba-a2bd-7272279ef17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspaces/crypto-data/jupyter-pod/ml-training-jupyter-notebooks/Example\\ Data\\ Sets/deltalake/time-travel-demo.delta\n",
      "|-- _delta_log\n",
      "|   `-- 00000000000000000000.json\n",
      "`-- part-00000-43a222b4-7935-47a8-9163-2e8db05e786f-c000.snappy.parquet\n",
      "\n",
      "1 directory, 2 files\n"
     ]
    }
   ],
   "source": [
    "! tree {escaped_full_delta_table_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b751eea4-40ce-4fef-aad4-5d059c1a51e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pyprojroot\n",
    "import os\n",
    "\n",
    "def load_delta_log(log_file_path):\n",
    "    with open(log_file_path, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "        proper_json = \"[\" + \",\".join(lines) + \"]\"\n",
    "        d = json.loads(proper_json)\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "081a7219-77fc-4637-9215-f9ac38f6cb58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"commitInfo\": {\n",
      "            \"timestamp\": 1653491448089,\n",
      "            \"operation\": \"WRITE\",\n",
      "            \"operationParameters\": {\n",
      "                \"mode\": \"ErrorIfExists\",\n",
      "                \"partitionBy\": \"[]\"\n",
      "            },\n",
      "            \"isBlindAppend\": true,\n",
      "            \"operationMetrics\": {\n",
      "                \"numFiles\": \"1\",\n",
      "                \"numOutputBytes\": \"735\",\n",
      "                \"numOutputRows\": \"5\"\n",
      "            }\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"protocol\": {\n",
      "            \"minReaderVersion\": 1,\n",
      "            \"minWriterVersion\": 2\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"metaData\": {\n",
      "            \"id\": \"788223d0-0633-4a55-a0b2-52d051ba76ed\",\n",
      "            \"format\": {\n",
      "                \"provider\": \"parquet\",\n",
      "                \"options\": {}\n",
      "            },\n",
      "            \"schemaString\": \"{\\\"type\\\":\\\"struct\\\",\\\"fields\\\":[{\\\"name\\\":\\\"A\\\",\\\"type\\\":\\\"long\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"B\\\",\\\"type\\\":\\\"long\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}}]}\",\n",
      "            \"partitionColumns\": [],\n",
      "            \"configuration\": {},\n",
      "            \"createdTime\": 1653491446519\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"add\": {\n",
      "            \"path\": \"part-00000-43a222b4-7935-47a8-9163-2e8db05e786f-c000.snappy.parquet\",\n",
      "            \"partitionValues\": {},\n",
      "            \"size\": 735,\n",
      "            \"modificationTime\": 1653491447879,\n",
      "            \"dataChange\": true\n",
      "        }\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "log_file_path = os.path.join(full_delta_table_path, \"_delta_log\",\"00000000000000000000.json\") \n",
    "print(json.dumps(load_delta_log(log_file_path), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91644560-7051-46fa-9054-820488604217",
   "metadata": {},
   "source": [
    "It's important to understand the data written in this file. We have the following list of dictionaries. \n",
    "\n",
    "1. Information about the commit\n",
    "2. Information about the transaction protocol\n",
    "3. [Table metadata](https://docs.databricks.com/delta/delta-batch.html#table-properties) (We will discuss these in more detail later)\n",
    "4. Information about Add operation\n",
    "\n",
    "\n",
    "These are effectively our shapshots and we can see a section for add which tells us that a new part was added to the table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8305aa-f0a8-492b-a797-29afeb46957a",
   "metadata": {},
   "source": [
    "We now modify our data and again write our data to the delta lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d231526-7c55-40e7-ad75-42638ac4f726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|  A|  B|  C|\n",
      "+---+---+---+\n",
      "|  1|  6| 11|\n",
      "|  2|  7| 12|\n",
      "|  3|  8| 13|\n",
      "|  4|  9| 14|\n",
      "|  5| 10| 15|\n",
      "+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id, row_number, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "C = [11,12,13,14,15]\n",
    "spark_udf = udf(lambda i: C[i -1], IntegerType())\n",
    "new_spark_df = spark_df.withColumn(\"C\", spark_udf('A'))\n",
    "new_spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "085b1a1b-4a60-492a-8ed2-7652d2b9cccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 2\n",
      "drwxr-xr-x 1 root root   3 May 25 15:10 .\n",
      "drwxr-xr-x 1 root root   1 May 25 15:10 ..\n",
      "-rw-r--r-- 1 root root  16 May 25 15:10 .part-00000-43a222b4-7935-47a8-9163-2e8db05e786f-c000.snappy.parquet.crc\n",
      "drwxr-xr-x 1 root root   1 May 25 15:10 _delta_log\n",
      "-rw-r--r-- 1 root root 735 May 25 15:10 part-00000-43a222b4-7935-47a8-9163-2e8db05e786f-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "! ls -la {escaped_full_delta_table_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e47d0319-086b-4057-bedd-83adb6efa0ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "new_spark_df\\\n",
    "  .write\\\n",
    "  .format(\"delta\")\\\n",
    "  .mode(\"overwrite\")\\\n",
    "  .option(\"overwriteSchema\", \"true\")\\\n",
    "  .save(full_delta_table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4077e048-e0c5-4e50-bd9c-df52b178b5da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspaces/crypto-data/jupyter-pod/ml-training-jupyter-notebooks/Example\\ Data\\ Sets/deltalake/time-travel-demo.delta\n",
      "|-- _delta_log\n",
      "|   |-- 00000000000000000000.json\n",
      "|   `-- 00000000000000000001.json\n",
      "|-- part-00000-43a222b4-7935-47a8-9163-2e8db05e786f-c000.snappy.parquet\n",
      "`-- part-00000-bc0be56e-f617-406e-8993-7106c4b0801f-c000.snappy.parquet\n",
      "\n",
      "1 directory, 4 files\n"
     ]
    }
   ],
   "source": [
    "! tree {escaped_full_delta_table_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0d2dce-6a67-4e35-b824-d6b6f64fd50e",
   "metadata": {},
   "source": [
    "We can see a new part has now shown up in the log directory. If we look at the log, we can see more entries than just an add (we see an add and a remove operation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5bea3d5d-e6f2-493a-a6fc-a460437e819c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"commitInfo\": {\n",
      "            \"timestamp\": 1653491527742,\n",
      "            \"operation\": \"WRITE\",\n",
      "            \"operationParameters\": {\n",
      "                \"mode\": \"Overwrite\",\n",
      "                \"partitionBy\": \"[]\"\n",
      "            },\n",
      "            \"readVersion\": 0,\n",
      "            \"isBlindAppend\": false,\n",
      "            \"operationMetrics\": {\n",
      "                \"numFiles\": \"1\",\n",
      "                \"numOutputBytes\": \"946\",\n",
      "                \"numOutputRows\": \"5\"\n",
      "            }\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"metaData\": {\n",
      "            \"id\": \"788223d0-0633-4a55-a0b2-52d051ba76ed\",\n",
      "            \"format\": {\n",
      "                \"provider\": \"parquet\",\n",
      "                \"options\": {}\n",
      "            },\n",
      "            \"schemaString\": \"{\\\"type\\\":\\\"struct\\\",\\\"fields\\\":[{\\\"name\\\":\\\"A\\\",\\\"type\\\":\\\"long\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"B\\\",\\\"type\\\":\\\"long\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"C\\\",\\\"type\\\":\\\"integer\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}}]}\",\n",
      "            \"partitionColumns\": [],\n",
      "            \"configuration\": {},\n",
      "            \"createdTime\": 1653491446519\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"add\": {\n",
      "            \"path\": \"part-00000-bc0be56e-f617-406e-8993-7106c4b0801f-c000.snappy.parquet\",\n",
      "            \"partitionValues\": {},\n",
      "            \"size\": 946,\n",
      "            \"modificationTime\": 1653491520845,\n",
      "            \"dataChange\": true\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"remove\": {\n",
      "            \"path\": \"part-00000-43a222b4-7935-47a8-9163-2e8db05e786f-c000.snappy.parquet\",\n",
      "            \"deletionTimestamp\": 1653491527741,\n",
      "            \"dataChange\": true,\n",
      "            \"extendedFileMetadata\": true,\n",
      "            \"partitionValues\": {},\n",
      "            \"size\": 735\n",
      "        }\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "log_file_path = os.path.join(full_delta_table_path, \"_delta_log\",\"00000000000000000001.json\") \n",
    "log = load_delta_log(log_file_path)\n",
    "print(json.dumps(log, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd400b5-71c6-4e5b-86a1-ec2dbe2039f0",
   "metadata": {},
   "source": [
    "Looking closely we can see that a part was removed from the table (the part we added in the first transaction file) and a new part was added."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b502de-9d80-4f1e-8d0f-d2da20b7f0d0",
   "metadata": {},
   "source": [
    "We can also see timestamps associated with each operation (modificationTime for add and deletionTimestamp for remove operations) as well as a timestamp for the entire atomic operation (commitInfo) and a timestamp for the metadata. We "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd011f0d-0b3f-4239-8322-9a8227521b34",
   "metadata": {},
   "source": [
    "## 2.3. Load timestamps for operations from log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5671307-0e24-4920-ba96-d928404d1037",
   "metadata": {},
   "source": [
    "Timestamps are somewhat difficult to interpret. We will load the timestamps into datetime objects so they are more human friendly and so we can see what is going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bf7eefe0-ad46-4935-b554-926104dfc1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v0 Info:\n",
      "Commit: 2022-05-25 15:10:48.089000\n",
      "Add: 2022-05-25 15:10:47.879000\n",
      "Creation: 2022-05-25 15:10:46.519000\n",
      "\n",
      "v1 Info:\n",
      "Commit: 2022-05-25 15:12:07.742000\n",
      "Add: 2022-05-25 15:12:00.845000\n",
      "Remove: 2022-05-25 15:12:07.741000\n",
      "Creation: 2022-05-25 15:10:46.519000\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "log0 = load_delta_log(os.path.join(full_delta_table_path, \"_delta_log\",\"00000000000000000000.json\"))\n",
    "\n",
    "log0_commit_info = log0[0][\"commitInfo\"]\n",
    "log0_commit_info_timestamp = log0_commit_info[\"timestamp\"]\n",
    "log0_commit_info_dt = datetime.datetime.fromtimestamp(log0_commit_info_timestamp/ 1e3)\n",
    "\n",
    "log0_metadata_info = log0[2][\"metaData\"]\n",
    "log0_metadata_info_timestamp = log0_metadata_info[\"createdTime\"]\n",
    "log0_metadata_info_dt = datetime.datetime.fromtimestamp(log0_metadata_info_timestamp/ 1e3)\n",
    "\n",
    "log0_add_info = log0[3][\"add\"]\n",
    "log0_add_info_timestamp = log0_add_info[\"modificationTime\"]\n",
    "log0_add_info_dt = datetime.datetime.fromtimestamp(log0_add_info_timestamp/ 1e3)\n",
    "\n",
    "log1 = load_delta_log(os.path.join(full_delta_table_path, \"_delta_log\",\"00000000000000000001.json\"))\n",
    "\n",
    "log1_commit_info = log1[0][\"commitInfo\"]\n",
    "log1_commit_info_timestamp = log1_commit_info[\"timestamp\"]\n",
    "log1_commit_info_dt = datetime.datetime.fromtimestamp(log1_commit_info_timestamp/ 1e3)\n",
    "\n",
    "log1_metadata_info = log1[1][\"metaData\"]\n",
    "log1_metadata_info_timestamp = log1_metadata_info[\"createdTime\"]\n",
    "log1_metadata_info_dt = datetime.datetime.fromtimestamp(log0_metadata_info_timestamp/ 1e3)\n",
    "\n",
    "log1_add_info = log1[2][\"add\"]\n",
    "log1_add_info_timestamp = log1_add_info[\"modificationTime\"]\n",
    "log1_add_info_dt = datetime.datetime.fromtimestamp(log1_add_info_timestamp / 1e3)\n",
    "\n",
    "log1_remove_info = log1[3][\"remove\"]\n",
    "log1_remove_info_timestamp = log1_remove_info[\"deletionTimestamp\"]\n",
    "log1_remove_info_dt = datetime.datetime.fromtimestamp(log1_remove_info_timestamp/ 1e3)\n",
    "\n",
    "print(\"v0 Info:\")\n",
    "print(f\"Commit: {log0_commit_info_dt}\")\n",
    "print(f\"Add: {log0_add_info_dt}\")\n",
    "print(f\"Creation: {log0_metadata_info_dt}\")\n",
    "print(\"\")\n",
    "print(\"v1 Info:\")\n",
    "print(f\"Commit: {log1_commit_info_dt}\")\n",
    "print(f\"Add: {log1_add_info_dt}\")\n",
    "print(f\"Remove: {log1_remove_info_dt}\")\n",
    "print(f\"Creation: {log1_metadata_info_dt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116eb960-6233-4fdc-be98-b137e9307e4d",
   "metadata": {},
   "source": [
    "We can see that the timestamp from the commitInfo section occurs the latest which logically makes sense."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facf14cf-3eeb-4ce0-ba0b-aff61b9cc14f",
   "metadata": {},
   "source": [
    "# 3. Delta Table History"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6434479-8841-416c-adce-a32c9d9f4fb0",
   "metadata": {},
   "source": [
    "As mentioned previously, a Delta Table provides a mechanism for tracking changes that occur since its creation. There are a few ways to get this versioning info, we will look at those now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e7d019-c7a5-4314-b6db-5bb7c0de4fa0",
   "metadata": {},
   "source": [
    "## 3.1. Viewing table history using the DeltaTable object\n",
    "\n",
    "Delta lake provides the DeltaTable object which hosts a number of useful functions. One of these is the history function. This function returns a dataframe with information related to the commits made to the delta table and to associated version numbers and timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cf8d6b3e-7783-477e-997f-3d3cf4396fb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>version</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>userId</th>\n",
       "      <th>userName</th>\n",
       "      <th>operation</th>\n",
       "      <th>operationParameters</th>\n",
       "      <th>job</th>\n",
       "      <th>notebook</th>\n",
       "      <th>clusterId</th>\n",
       "      <th>readVersion</th>\n",
       "      <th>isolationLevel</th>\n",
       "      <th>isBlindAppend</th>\n",
       "      <th>operationMetrics</th>\n",
       "      <th>userMetadata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2022-05-25 15:12:07.985</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>WRITE</td>\n",
       "      <td>{'mode': 'Overwrite', 'partitionBy': '[]'}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>{'numOutputRows': '5', 'numOutputBytes': '946'...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2022-05-25 15:10:48.657</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>WRITE</td>\n",
       "      <td>{'mode': 'ErrorIfExists', 'partitionBy': '[]'}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>{'numOutputRows': '5', 'numOutputBytes': '735'...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   version               timestamp userId userName operation  \\\n",
       "0        1 2022-05-25 15:12:07.985   None     None     WRITE   \n",
       "1        0 2022-05-25 15:10:48.657   None     None     WRITE   \n",
       "\n",
       "                              operationParameters   job notebook clusterId  \\\n",
       "0      {'mode': 'Overwrite', 'partitionBy': '[]'}  None     None      None   \n",
       "1  {'mode': 'ErrorIfExists', 'partitionBy': '[]'}  None     None      None   \n",
       "\n",
       "   readVersion isolationLevel  isBlindAppend  \\\n",
       "0          0.0           None          False   \n",
       "1          NaN           None           True   \n",
       "\n",
       "                                    operationMetrics userMetadata  \n",
       "0  {'numOutputRows': '5', 'numOutputBytes': '946'...         None  \n",
       "1  {'numOutputRows': '5', 'numOutputBytes': '735'...         None  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Create a pointer tothe delta table (dont confuse with spark dataframe)\n",
    "deltaTable = DeltaTable.forPath(sparkSession, full_delta_table_path)\n",
    "\n",
    "# Get the history as a spark dataframe\n",
    "dt_history_df = deltaTable.history()\n",
    "\n",
    "# Convert the info to pandas to make the presentation nicer\n",
    "dt_history_pd_df = dt_history_df.toPandas()\n",
    "dt_history_pd_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475e548b-befb-494e-b65d-13cf24d99600",
   "metadata": {},
   "source": [
    "**Note**: We see some information in some columns is missing. That will be discussed separaetely.**Note**: We see some information in some columns is missing. That will be discussed separaetely.\n",
    "\n",
    "We see that we can use the version number to get timestamps associated with specific version or look up which version occurred relative to a specific time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b1a5b78a-630e-49f2-a800-09702a4f0c03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2022-05-25 15:12:07.985000')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1_dt = dt_history_pd_df[dt_history_pd_df[\"version\"] == 1][\"timestamp\"].iloc[0]\n",
    "v1_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aa996401-643b-4d54-ae9a-78fc0c769fba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_history_pd_df[dt_history_pd_df[\"timestamp\"] < v1_dt.strftime('%Y-%m-%d %H:%M:%S.%f')][\"version\"].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75de7ad-7f8c-463c-9553-dfebb4d551a2",
   "metadata": {},
   "source": [
    "## 3.2. Viewing table history using Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "93576ef4-cdb9-415c-ac40-af9a172c5de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ANTLR Tool version 4.7 used for code generation does not match the current runtime version 4.8ANTLR Tool version 4.7 used for code generation does not match the current runtime version 4.8"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>version</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>userId</th>\n",
       "      <th>userName</th>\n",
       "      <th>operation</th>\n",
       "      <th>operationParameters</th>\n",
       "      <th>job</th>\n",
       "      <th>notebook</th>\n",
       "      <th>clusterId</th>\n",
       "      <th>readVersion</th>\n",
       "      <th>isolationLevel</th>\n",
       "      <th>isBlindAppend</th>\n",
       "      <th>operationMetrics</th>\n",
       "      <th>userMetadata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2022-05-25 15:12:07.985</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>WRITE</td>\n",
       "      <td>{'mode': 'Overwrite', 'partitionBy': '[]'}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>{'numOutputRows': '5', 'numOutputBytes': '946'...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2022-05-25 15:10:48.657</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>WRITE</td>\n",
       "      <td>{'mode': 'ErrorIfExists', 'partitionBy': '[]'}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>{'numOutputRows': '5', 'numOutputBytes': '735'...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   version               timestamp userId userName operation  \\\n",
       "0        1 2022-05-25 15:12:07.985   None     None     WRITE   \n",
       "1        0 2022-05-25 15:10:48.657   None     None     WRITE   \n",
       "\n",
       "                              operationParameters   job notebook clusterId  \\\n",
       "0      {'mode': 'Overwrite', 'partitionBy': '[]'}  None     None      None   \n",
       "1  {'mode': 'ErrorIfExists', 'partitionBy': '[]'}  None     None      None   \n",
       "\n",
       "   readVersion isolationLevel  isBlindAppend  \\\n",
       "0          0.0           None          False   \n",
       "1          NaN           None           True   \n",
       "\n",
       "                                    operationMetrics userMetadata  \n",
       "0  {'numOutputRows': '5', 'numOutputBytes': '946'...         None  \n",
       "1  {'numOutputRows': '5', 'numOutputBytes': '735'...         None  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkSession.sql(f\"DESCRIBE HISTORY delta.`{full_delta_table_path}`\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c906bca-8efc-412b-8a89-d9773a7a9cb8",
   "metadata": {},
   "source": [
    "## 3.3. Checkpoints\n",
    "\n",
    "In short, checkpoints are a way to make delta lake more performant.\n",
    "\n",
    "Once weâ€™ve made a total of 10 commits to the transaction log, Delta Lake automatcally creates a checkpoint file andstores it in the \\_delta_log subdirectory. This parquet file stores the entire state of the table at a given point in time. The idea is that this checkpoint provides Spark with a shortcut; rather than parsing all the json files and loading the the coresponding part files sequentiall, is simply loads the parquet file. If subsequent transactions do happen after the snapshot, Spark will filter out the irrelevant logs and only process the transactions occurring after the snapshot.\n",
    "\n",
    "A more detailed article can be found [here](https://databricks.com/blog/2019/08/21/diving-into-delta-lake-unpacking-the-transaction-log.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a36431-4c18-453b-b4de-9dd1d94334c5",
   "metadata": {},
   "source": [
    "# 4. Loading a specific versions of a delta table\n",
    "In this section we will explore the ways in which we can load our data. We will see that it is possible to load a specific version of using a version number or a timestamp."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031829ab-2527-43ab-b0b2-5f904205865d",
   "metadata": {},
   "source": [
    "## 4.1. Load delta table using SparkSession.read() function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13303eb7-4a11-4a45-8bee-1c217860ec25",
   "metadata": {},
   "source": [
    "### 4.1.1. Load based on version number"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c28f8fb-bcf4-497a-8d5d-ed4b1501d174",
   "metadata": {},
   "source": [
    "We saw the \\<version>.json file in the \\_delta_log directory. This allows us to load a given version via it's index. Below we can see an example of loading the first version of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "461c49b6-4295-422a-a151-e8e1d82f197d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  A|  B|\n",
      "+---+---+\n",
      "|  1|  6|\n",
      "|  2|  7|\n",
      "|  3|  8|\n",
      "|  4|  9|\n",
      "|  5| 10|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkSession.read.format(\"delta\").option(\"versionAsOf\", 0).load(full_delta_table_path).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5eebce91-7d1c-410f-b767-f28d4dc02cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|  A|  B|  C|\n",
      "+---+---+---+\n",
      "|  1|  6| 11|\n",
      "|  2|  7| 12|\n",
      "|  3|  8| 13|\n",
      "|  4|  9| 14|\n",
      "|  5| 10| 15|\n",
      "+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkSession.read.format(\"delta\").option(\"versionAsOf\", 1).load(full_delta_table_path).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcc8210-cad4-483a-a15d-be3d3d1fa8a1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4.1.2. Load based on timestamp\n",
    "Loading data based on a version number is not that user friendly or intuitive so instead we will look at loading data based on a timestamp."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971c1a6d-18d2-40b1-889c-361813d8d854",
   "metadata": {},
   "source": [
    "#### 4.1.2.1. Note on possible errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b99d03e-dc0c-44ae-9d33-91516e0ae60d",
   "metadata": {},
   "source": [
    "**Note**: There is a limited band of time for which we can pull data as of. There is a minimum and maximum datetime. If we try to pull a datetime that is outside this band, we will see an error thrown which looks like this:\n",
    "```\n",
    "Py4JJavaError: An error occurred while calling o1083.load.\n",
    ": org.apache.spark.sql.delta.DeltaErrors$TimestampEarlierThanCommitRetentionException: The provided timestamp (2022-05-20 18:42:13.0) is before the earliest version available to this\n",
    "table (2022-05-20 18:42:13.341). Please use a timestamp after 2022-05-20 18:42:13.\n",
    "```\n",
    "\n",
    "\n",
    "``` \n",
    "Py4JJavaError: An error occurred while calling o1131.load.\n",
    ": org.apache.spark.sql.delta.DeltaErrors$TemporallyUnstableInputException: The provided timestamp: 2022-05-20 19:30:50.646 is after the latest commit timestamp of\n",
    "2022-05-20 19:29:51.389. If you wish to query this version of the table, please either provide\n",
    "the version with \"VERSION AS OF 1\" or use the exact timestamp\n",
    "of the last commit: \"TIMESTAMP AS OF '2022-05-20 19:29:51'\".\n",
    "```\n",
    "\n",
    "**Note**: I have seen many articles claiming that we must specify the timestamp in yyyyMMddHHmmssSSS format. This is not true as we will see. The date must be a string in the format '%Y-%m-%d %H:%M:%S.%f'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f2c2fd-f4b0-43c3-8d68-a6478b394910",
   "metadata": {},
   "source": [
    "#### 4.1.2.1. Load data using timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d2de5e-f468-4538-9093-4cdfbca356d8",
   "metadata": {},
   "source": [
    "We first pull the timestamps from the history() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d49d23bd-0a10-4e27-acbf-325b5c6f4e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-25 15:10:48.657000\n",
      "2022-05-25 15:12:07.985000\n"
     ]
    }
   ],
   "source": [
    "v0_timestamp = dt_history_pd_df[dt_history_pd_df[\"version\"] == 0][\"timestamp\"].iloc[0]\n",
    "v0_timestamp_str = v0_timestamp.strftime('%Y-%m-%d %H:%M:%S.%f')\n",
    "print(v0_timestamp_str)\n",
    "\n",
    "v1_timestamp = dt_history_pd_df[dt_history_pd_df[\"version\"] == 1][\"timestamp\"].iloc[0]\n",
    "v1_timestamp_str = v1_timestamp.strftime('%Y-%m-%d %H:%M:%S.%f')\n",
    "print(v1_timestamp_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bc2a40-fc77-403b-bd11-a174af775c60",
   "metadata": {},
   "source": [
    "We load the timestamp associated with the first version and we see that the data is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5f13e7ee-a61e-48db-a9b4-5c60f33aae0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  A|  B|\n",
      "+---+---+\n",
      "|  1|  6|\n",
      "|  2|  7|\n",
      "|  3|  8|\n",
      "|  4|  9|\n",
      "|  5| 10|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkSession.read \\\n",
    "  .format(\"delta\") \\\n",
    "  .option(\"timestampAsOf\", v0_timestamp_str) \\\n",
    "  .load(full_delta_table_path).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07300767-5cde-499d-8daf-12dc46741bad",
   "metadata": {},
   "source": [
    "We then load data using a timestamp between the commit operations for v1 and v2. We see that the data resembles v1 as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a1d733a9-4201-40ff-8492-e1d74e0851bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  A|  B|\n",
      "+---+---+\n",
      "|  1|  6|\n",
      "|  2|  7|\n",
      "|  3|  8|\n",
      "|  4|  9|\n",
      "|  5| 10|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt = (v1_timestamp - v0_timestamp)/2\n",
    "mid_point_timestamp = v0_timestamp + dt\n",
    "mid_point_timestamp_str = mid_point_timestamp.strftime('%Y-%m-%d %H:%M:%S.%f')\n",
    "\n",
    "sparkSession.read \\\n",
    "  .format(\"delta\") \\\n",
    "  .option(\"timestampAsOf\", mid_point_timestamp_str) \\\n",
    "  .load(full_delta_table_path).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f789a1-4fb4-4dcc-bef2-b8ff236f9c7d",
   "metadata": {},
   "source": [
    "We can then see that we can load v2 data using the timestamp for v2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "550fa914-6888-4654-b9d3-fab0dc4e3bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|  A|  B|  C|\n",
      "+---+---+---+\n",
      "|  1|  6| 11|\n",
      "|  2|  7| 12|\n",
      "|  3|  8| 13|\n",
      "|  4|  9| 14|\n",
      "|  5| 10| 15|\n",
      "+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkSession.read \\\n",
    "  .format(\"delta\") \\\n",
    "  .option(\"timestampAsOf\", v1_timestamp_str) \\\n",
    "  .load(full_delta_table_path).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628cf801-9f23-4479-a66a-35ed64d3241c",
   "metadata": {},
   "source": [
    "## 4.2. Load using Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1717a60-201c-426b-a650-7620eebd0160",
   "metadata": {},
   "source": [
    "We see that we can also load the delta tables using the Spark SQL API provided by the sql() function on the SparkSession object.\n",
    "\n",
    "For this to work, we must provide the full path path of directory and not the relative path like we do with the read() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "202d1636-cd2c-4c85-ae3c-b8c7d5ad7a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A   B   C\n",
       "0  1   6  11\n",
       "1  2   7  12\n",
       "2  3   8  13\n",
       "3  4   9  14\n",
       "4  5  10  15"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkSession.sql(f\"SELECT * FROM delta.`{full_delta_table_path}`\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115bded5-e14a-41a0-ba8e-6bccf6fa06ef",
   "metadata": {},
   "source": [
    "We can load specific versions by modifying the path to the table to contain a timestamp coresponding to the version of interest. The timestamp must be in the format yyyyMMddHHmmssSSS. As we will see, we are only able to see the first three digits of the microseconds field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e9319ffe-3e9b-4f78-8e58-1b6fba429a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20220525151048657000\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "`/workspaces/crypto-data/jupyter-pod/ml-training-jupyter-notebooks/Example Data Sets/deltalake/time-travel-demo.delta@20220525151048657000` is not a Delta table.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [42]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(v0_timestamp_str_2)\n\u001b[1;32m      3\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_delta_table_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m@\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv0_timestamp_str_2\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m \u001b[43msparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSELECT * FROM delta.`\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mp\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m`\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtoPandas()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pyspark/sql/session.py:723\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    707\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msql\u001b[39m(\u001b[38;5;28mself\u001b[39m, sqlQuery):\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;124;03m\"\"\"Returns a :class:`DataFrame` representing the result of the given query.\u001b[39;00m\n\u001b[1;32m    709\u001b[0m \n\u001b[1;32m    710\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 2.0.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;124;03m    [Row(f1=1, f2='row1'), Row(f1=2, f2='row2'), Row(f1=3, f2='row3')]\u001b[39;00m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 723\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrapped)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: `/workspaces/crypto-data/jupyter-pod/ml-training-jupyter-notebooks/Example Data Sets/deltalake/time-travel-demo.delta@20220525151048657000` is not a Delta table."
     ]
    }
   ],
   "source": [
    "v0_timestamp_str_2 = v0_timestamp.strftime('%Y%m%d%H%M%S%f')\n",
    "print(v0_timestamp_str_2)\n",
    "p = f\"{full_delta_table_path}@{v0_timestamp_str_2}\"\n",
    "\n",
    "sparkSession.sql(f\"SELECT * FROM delta.`{p}`\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a191d5e0-d20a-4238-95b9-6ae068c1bcca",
   "metadata": {},
   "source": [
    "Correcting the timestamp we see things work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7960c6c1-f751-4d29-8c8c-3d24d32bc612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20220525151048657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A   B\n",
       "0  1   6\n",
       "1  2   7\n",
       "2  3   8\n",
       "3  4   9\n",
       "4  5  10"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v0_timestamp_str_2 = v0_timestamp.strftime('%Y%m%d%H%M%S%f')[:-3]\n",
    "print(v0_timestamp_str_2)\n",
    "p = f\"{full_delta_table_path}@{v0_timestamp_str_2}\"\n",
    "\n",
    "sparkSession.sql(f\"SELECT * FROM delta.`{p}`\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cdac1d-beae-4ae8-945d-c459e7f17e74",
   "metadata": {},
   "source": [
    "**Note**: Currently, loading a specific version using the SQL API using the keywords TIMESTAMP AS OF or VERSION AS OF is not supported. There is an open [github issue](https://github.com/delta-io/delta/issues/128) tracking progress."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082c62cb-50ef-4fe8-99da-5fb86a3c84b0",
   "metadata": {},
   "source": [
    "# 5. Delta Table History Retention Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc3d68a-ce8c-4f3e-8540-f4ca618b0b61",
   "metadata": {},
   "source": [
    "To time travel to a previous version, you must retain both the log and the data files for that version.\n",
    "\n",
    "The data files backing a Delta table are never deleted automatically; data files are deleted only when you run VACUUM. VACUUM does not delete Delta log files; log files are automatically cleaned up after checkpoints are written.\n",
    "\n",
    "So to summarize, deleteing data or logs is a manual process. If you run the vacuum() function you will only be able to time travel as far back as the retention policy.\n",
    "\n",
    "The delta lake API provides several methods which impact the retention policy for a delta table. These settings appear to be set on the SparkSession level, not at the table level. \n",
    "\n",
    "According to the [documentation](https://docs.delta.io/latest/delta-utility.html#remove-files-no-longer-referenced-by-a-delta-table), the default retention period of log files is 30 days, configurable through the delta.logRetentionDuration property."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae69068-58fe-49e7-bb01-c590e54721f2",
   "metadata": {},
   "source": [
    "## 5.1. Viewing delta table properties\n",
    "The retention policy for a table is stored as metadata in the table. At this point The only way to access the data is through the Spark SQL API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decb9428-6e76-4709-9505-553f8222edb2",
   "metadata": {},
   "source": [
    "### 5.1.2. Viewing delta table properties using Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56579b2b-8f60-42a8-a8ac-0ee29c336a20",
   "metadata": {},
   "source": [
    "Viewing table properties is only possible throug the SparkSession.sql() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f477ce47-d0c7-43c4-b847-673db77b7b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>delta.minReaderVersion</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>delta.minWriterVersion</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      key value\n",
       "0  delta.minReaderVersion     1\n",
       "1  delta.minWriterVersion     2"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = f\"{full_delta_table_path}@{v0_timestamp_str_2}\"\n",
    "\n",
    "sparkSession.sql(f\"SHOW TBLPROPERTIES delta.`{p}`\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c72c4b-cdc1-4cb0-8f36-94ffe1d2acda",
   "metadata": {},
   "source": [
    "If the table changes over time, we can see properties for specific versions by modifying the path as we saw before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfde314-3180-4128-b651-b665b3084f1f",
   "metadata": {},
   "source": [
    "## 5.2. Setting or modifying delta table properties\n",
    "When we modify table properties we are committing a new version of the table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc7e29b-d244-407a-8817-f7e238380a9f",
   "metadata": {},
   "source": [
    "### 5.2.2. Setting table properties using SQL API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429b751c-c5e2-4f47-94b8-5deed2a073f8",
   "metadata": {},
   "source": [
    "We can run the following query to arbitratily set various table properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ea7a010b-963e-4e4e-93e2-46c5e5bbc018",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>delta.logRetentionDuration</td>\n",
       "      <td>interval 36500000 days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>delta.minReaderVersion</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>delta.minWriterVersion</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>delta.deletedFileRetentionDuration</td>\n",
       "      <td>interval 36500000 days</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  key                   value\n",
       "0          delta.logRetentionDuration  interval 36500000 days\n",
       "1              delta.minReaderVersion                       1\n",
       "2              delta.minWriterVersion                       2\n",
       "3  delta.deletedFileRetentionDuration  interval 36500000 days"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_retention_duration_days = \"interval 36500000 days\"\n",
    "delete_file_retention_duration = \"interval 36500000 days\"\n",
    "\n",
    "sparkSession.sql(f\"\"\"\n",
    "ALTER TABLE delta.`{full_delta_table_path}` SET TBLPROPERTIES (\n",
    "  delta.logRetentionDuration='{log_retention_duration_days}', \n",
    "  delta.deletedFileRetentionDuration='{delete_file_retention_duration}'\n",
    ")\n",
    "\"\"\")\n",
    "    \n",
    "sparkSession.sql(f\"SHOW TBLPROPERTIES delta.`{full_delta_table_path}`\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d910382-d124-4af3-a64a-a16db95b9879",
   "metadata": {},
   "source": [
    "We see that altering the table results in a new transaction being written to the transaction log. Loading the history shows us this the details of this new delta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "008f14e4-f552-4e63-ae59-e9f26569fa10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>version</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>userId</th>\n",
       "      <th>userName</th>\n",
       "      <th>operation</th>\n",
       "      <th>operationParameters</th>\n",
       "      <th>job</th>\n",
       "      <th>notebook</th>\n",
       "      <th>clusterId</th>\n",
       "      <th>readVersion</th>\n",
       "      <th>isolationLevel</th>\n",
       "      <th>isBlindAppend</th>\n",
       "      <th>operationMetrics</th>\n",
       "      <th>userMetadata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2022-05-25 15:20:09.222</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>SET TBLPROPERTIES</td>\n",
       "      <td>{'properties': '{\"delta.logRetentionDuration\":...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>{}</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2022-05-25 15:12:07.985</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>WRITE</td>\n",
       "      <td>{'mode': 'Overwrite', 'partitionBy': '[]'}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>{'numOutputRows': '5', 'numOutputBytes': '946'...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2022-05-25 15:10:48.657</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>WRITE</td>\n",
       "      <td>{'mode': 'ErrorIfExists', 'partitionBy': '[]'}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>{'numOutputRows': '5', 'numOutputBytes': '735'...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   version               timestamp userId userName          operation  \\\n",
       "0        2 2022-05-25 15:20:09.222   None     None  SET TBLPROPERTIES   \n",
       "1        1 2022-05-25 15:12:07.985   None     None              WRITE   \n",
       "2        0 2022-05-25 15:10:48.657   None     None              WRITE   \n",
       "\n",
       "                                 operationParameters   job notebook clusterId  \\\n",
       "0  {'properties': '{\"delta.logRetentionDuration\":...  None     None      None   \n",
       "1         {'mode': 'Overwrite', 'partitionBy': '[]'}  None     None      None   \n",
       "2     {'mode': 'ErrorIfExists', 'partitionBy': '[]'}  None     None      None   \n",
       "\n",
       "   readVersion isolationLevel  isBlindAppend  \\\n",
       "0          1.0           None           True   \n",
       "1          0.0           None          False   \n",
       "2          NaN           None           True   \n",
       "\n",
       "                                    operationMetrics userMetadata  \n",
       "0                                                 {}         None  \n",
       "1  {'numOutputRows': '5', 'numOutputBytes': '946'...         None  \n",
       "2  {'numOutputRows': '5', 'numOutputBytes': '735'...         None  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkSession.sql(f\"DESCRIBE HISTORY delta.`{full_delta_table_path}`\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d93f39e-4361-4fd8-a23d-562605be3e52",
   "metadata": {},
   "source": [
    "We can also see that the table properties are only effective as of a specific version. Looking at the properties of the old version shows that these properties are not present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "36e7214a-5df9-4c19-8aeb-cd55bce26a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>delta.minReaderVersion</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>delta.minWriterVersion</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      key value\n",
       "0  delta.minReaderVersion     1\n",
       "1  delta.minWriterVersion     2"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1_timestamp_str_2 = v1_timestamp.strftime('%Y%m%d%H%M%S%f')[:-3]\n",
    "p = f\"{full_delta_table_path}@{v1_timestamp_str_2}\"\n",
    "\n",
    "sparkSession.sql(f\"SHOW TBLPROPERTIES delta.`{p}`\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ece5e7-f2a5-4eff-87df-43136a1d182a",
   "metadata": {},
   "source": [
    "### 5.2.3. Setting table propeties using SparkConf\n",
    "\n",
    "We can also set table properties globally using the SparkConf object. In doing so, any new tables that are created will inherit these properties. This may be useful when trying to enforce certain behaviors. But note, this will not change the properties of existing tables.\n",
    "\n",
    "Below we can see an example code snippet:\n",
    "\n",
    "```python\n",
    "sparkSession.conf.set(\n",
    "  \"spark.databricks.delta.properties.defaults.autoOptimize.optimizeWrite\", \"true\")\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a8a6b1-23fd-4f21-89fb-b4e937dc5396",
   "metadata": {},
   "source": [
    "## 5.3. Deleting Delta Table History\n",
    "\n",
    "\n",
    "As mentioned previously, the delta table is stored in parts and assembled from those parts. At some point we may want to do a cleanup and delete old files so that the footprint of the table shrinks.\n",
    "\n",
    "You can remove files no longer referenced by a Delta table and are older than the retention threshold by running the vacuum() command on the table.\n",
    "\n",
    "Vacuum deletes only data files, not log files. Log files are deleted automatically and asynchronously after checkpoint operations.\n",
    "\n",
    "We will look more at the retention policy in a later section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6371b74a-cc2d-45a3-be9d-bc7d72f896ba",
   "metadata": {},
   "source": [
    "# 6. Restoration Operations\n",
    "We saw previously that we can load a specific version of data by specifying a version number or associated timestamp. But in some cases, we may want to manipulate our data and revert it to a prior state. An example might be that a problematic ETL process erroneously correpted some data or returned thewrong schema/values. With delta lake, this is an easy command. Resore operations behave like other transactions; the restore creates a new version which looks identical to the desired version.\n",
    "\n",
    "**Note**: The documentation shows several methods for restoring a table using either the DeltaLake object or the Spark SQL API. These methods unfortunately do not work with the current version of delta lake (1.0.1). As a workaround we can simply load a previous version and then write that to the delta lake.\n",
    "\n",
    "More informationregarding restors can be found [here](\n",
    "https://docs.delta.io/latest/delta-utility.html#restore-a-delta-table-to-an-earlier-state)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "72300cce-e678-4617-8d00-79d73407b5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  A|  B|\n",
      "+---+---+\n",
      "|  1|  6|\n",
      "|  2|  7|\n",
      "|  3|  8|\n",
      "|  4|  9|\n",
      "|  5| 10|\n",
      "+---+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "old_table = sparkSession.read.format(\"delta\").option(\"versionAsOf\", 0).load(full_delta_table_path)\n",
    "old_table.write\\\n",
    "  .format(\"delta\")\\\n",
    "  .mode(\"overwrite\")\\\n",
    "  .option(\"overwriteSchema\", \"true\")\\\n",
    "  .save(full_delta_table_path)\n",
    "\n",
    "sparkSession.read.format(\"delta\").load(full_delta_table_path).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f66415-fd8a-4d3c-af6b-164a3fcbd65e",
   "metadata": {},
   "source": [
    "We can see a new version was written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d029aeea-426f-4e3a-9a14-8e82caeab556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>version</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>userId</th>\n",
       "      <th>userName</th>\n",
       "      <th>operation</th>\n",
       "      <th>operationParameters</th>\n",
       "      <th>job</th>\n",
       "      <th>notebook</th>\n",
       "      <th>clusterId</th>\n",
       "      <th>readVersion</th>\n",
       "      <th>isolationLevel</th>\n",
       "      <th>isBlindAppend</th>\n",
       "      <th>operationMetrics</th>\n",
       "      <th>userMetadata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>2022-05-25 15:20:54.991</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>WRITE</td>\n",
       "      <td>{'mode': 'Overwrite', 'partitionBy': '[]'}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2.0</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>{'numOutputRows': '5', 'numOutputBytes': '735'...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2022-05-25 15:20:09.222</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>SET TBLPROPERTIES</td>\n",
       "      <td>{'properties': '{\"delta.logRetentionDuration\":...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>{}</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2022-05-25 15:12:07.985</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>WRITE</td>\n",
       "      <td>{'mode': 'Overwrite', 'partitionBy': '[]'}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>{'numOutputRows': '5', 'numOutputBytes': '946'...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2022-05-25 15:10:48.657</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>WRITE</td>\n",
       "      <td>{'mode': 'ErrorIfExists', 'partitionBy': '[]'}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>{'numOutputRows': '5', 'numOutputBytes': '735'...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   version               timestamp userId userName          operation  \\\n",
       "0        3 2022-05-25 15:20:54.991   None     None              WRITE   \n",
       "1        2 2022-05-25 15:20:09.222   None     None  SET TBLPROPERTIES   \n",
       "2        1 2022-05-25 15:12:07.985   None     None              WRITE   \n",
       "3        0 2022-05-25 15:10:48.657   None     None              WRITE   \n",
       "\n",
       "                                 operationParameters   job notebook clusterId  \\\n",
       "0         {'mode': 'Overwrite', 'partitionBy': '[]'}  None     None      None   \n",
       "1  {'properties': '{\"delta.logRetentionDuration\":...  None     None      None   \n",
       "2         {'mode': 'Overwrite', 'partitionBy': '[]'}  None     None      None   \n",
       "3     {'mode': 'ErrorIfExists', 'partitionBy': '[]'}  None     None      None   \n",
       "\n",
       "   readVersion isolationLevel  isBlindAppend  \\\n",
       "0          2.0           None          False   \n",
       "1          1.0           None           True   \n",
       "2          0.0           None          False   \n",
       "3          NaN           None           True   \n",
       "\n",
       "                                    operationMetrics userMetadata  \n",
       "0  {'numOutputRows': '5', 'numOutputBytes': '735'...         None  \n",
       "1                                                 {}         None  \n",
       "2  {'numOutputRows': '5', 'numOutputBytes': '946'...         None  \n",
       "3  {'numOutputRows': '5', 'numOutputBytes': '735'...         None  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkSession.sql(f\"DESCRIBE HISTORY delta.`{full_delta_table_path}`\").toPandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
