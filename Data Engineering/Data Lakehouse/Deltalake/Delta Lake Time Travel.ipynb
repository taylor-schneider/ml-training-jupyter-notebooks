{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bb07176-cfb8-4f3e-80c6-559680a3b47b",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f716d2-aaf3-47fe-9ba1-1ddee7f5c1cb",
   "metadata": {},
   "source": [
    "In February 19, 2019, Databricks [announced](https://databricks.com/blog/2019/02/04/introducing-delta-time-travel-for-large-scale-data-lakes.html) the release of the time travel feature. \n",
    "\n",
    "> With this new feature, Delta automatically versions the big data that you store in your data lake, and you can access any historical version of that data. This temporal data management simplifies your data pipeline by making it easy to audit, roll back data in case of accidental bad writes or deletes, and reproduce experiments and reports. Your organization can finally standardize on a clean, centralized,  versioned big data repository in your own cloud storage for your analytics.\n",
    "\n",
    "In this notebook we will explore this feature as well as the problems it is looking to solve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e316141e-0970-4052-8ccf-8c4fd0259e12",
   "metadata": {},
   "source": [
    "# 1. Challenges Being Addressed\n",
    "\n",
    "## 1.1. Audit data changes\n",
    "Auditing data changes is critical from both in terms of data compliance as well as simple debugging to understand how data has changed over time. Traditionally the audit was tightly coupled with the pipelines responsible for delivering the data. This feature standardizes the audit regardless of how the data got into the data lake.\n",
    "\n",
    "## 1.2. Reproduce experiments & reports\n",
    "During model training, data scientists run various experiments with different parameters on a given set of data. When scientists revisit their experiments after a period of time to reproduce the models, typically the source data has been modified by upstream pipelines. Lot of times, they are caught unaware by such upstream data changes and hence struggle to reproduce their experiments. Some scientists and organizations engineer best practices by creating multiple copies of the data, leading to increased storage costs. The same is true for analysts generating reports.\n",
    "\n",
    "## 1.3. Rollbacks\n",
    "Data pipelines can sometimes write bad data for downstream consumers. This can happen because of issues ranging from infrastructure instabilities to messy data to bugs in the pipeline. For pipelines that do simple appends to directories or a table, rollbacks can easily be addressed by date-based partitioning. With updates and deletes, this can become very complicated, and data engineers typically have to engineer a complex pipeline to deal with such scenarios. This is no longer the case with time travel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5424b5f-7f12-4621-b0be-a0d58a4a7401",
   "metadata": {},
   "source": [
    "# 2. Under The Hood: How does time travel work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073f3b3c-db80-4f04-9655-db6e44257cff",
   "metadata": {},
   "source": [
    "As you write into a Delta table or directory, every operation is automatically versioned. And deltalake provides you with an API for accessing data as of a specified timestamp. But how does this work under the hood?\n",
    "\n",
    "## 2.1. A Brief Explanation\n",
    "A great video can be found [here](https://www.youtube.com/watch?v=o-zcZvfSUyIhttps://www.youtube.com/watch?v=o-zcZvfSUyI)\n",
    "\n",
    "In short, we can think of a delta table as the result of applying a sequence of changes. We will see that a delta table is not a file, but a directory structure. When we load a table into memory we are actually assembling the table from data files stored in the directory.\n",
    "\n",
    "As such, when we perform operations on the delta table (i.e. store a snapshot of the state of a spark dataframe), we see the files inthis dirctory are modified. \n",
    "\n",
    "The delta table directory contains several types of files\n",
    "1. Compressed parquet files\n",
    "2. Checksums to validate the parwuet files\n",
    "3. Transaction logs\n",
    "4. Checkpoint files (cached state to improve computational efficiency)\n",
    "\n",
    "The delta table has a log directory which stores json log files which record operations beind performed on the delta table. These operations describe how we transform the data from one state to another state. The operations can be categorized as follows:\n",
    "- update metadata\n",
    "- add parquet file\n",
    "- remove parquet file\n",
    "- set transaction (record idempotent transaction id)\n",
    "- change transaction protocol version\n",
    "\n",
    "This log directory is extremely important because it is what provides the information for calculating versions and of data and their coresponding timestamps. The log directory consists of transaction files, each transaction file pertains to a specific version of data. File '00000000000000000000.json' coresponds to version 0 while file '00000000000000000000.json' coresponds to version 1 of the delta table. Each transaction file has a modified data assigned by the operating system. This timestamp attaches a datatime to our version.\n",
    "\n",
    "Below we can see an example detlat table which i have stored in the `data/time-travel-demo.delta` directory\n",
    "\n",
    "```\n",
    "[root@pc]# tree data/time-travel-demo.delta\n",
    "data/time-travel-demo.delta/\n",
    "|-- _delta_log\n",
    "|   |-- 00000000000000000000.json\n",
    "|   `-- 00000000000000000001.json\n",
    "|-- part-00000-41e1a8cd-15d5-4263-9413-f649ad1d51da-c000.snappy.parquet\n",
    "`-- part-00000-e8891ae6-e9d4-449a-8531-de44d41f7669-c000.snappy.parquetdata\n",
    "```\n",
    "\n",
    "We see that the directory is filled with files (the crc files are hidden)\n",
    "\n",
    "```\n",
    "[root@pc]# ls -la data/time-travel-demo.delta/\n",
    "\n",
    "total 3\n",
    "drwxr-xr-x 1 root root   5 May 20 19:29 .\n",
    "drwxr-xr-x 1 root root   2 May 20 18:42 ..\n",
    "-rw-r--r-- 1 root root  16 May 20 18:42 .part-00000-41e1a8cd-15d5-4263-9413-f649ad1d51da-c000.snappy.parquet.crc\n",
    "-rw-r--r-- 1 root root  16 May 20 19:29 .part-00000-e8891ae6-e9d4-449a-8531-de44d41f7669-c000.snappy.parquet.crc\n",
    "drwxr-xr-x 1 root root   2 May 20 19:29 _delta_log\n",
    "-rw-r--r-- 1 root root 735 May 20 18:42 part-00000-41e1a8cd-15d5-4263-9413-f649ad1d51da-c000.snappy.parquet\n",
    "-rw-r--r-- 1 root root 946 May 20 19:29 part-00000-e8891ae6-e9d4-449a-8531-de44d41f7669-c000.snappy.parquet\n",
    "```\n",
    "\n",
    "And we can get the timestamp for the modification of the file\n",
    "\n",
    "```\n",
    "[root@pc]# stat data/time-travel-demo.delta/_delta_log/*\n",
    "\n",
    "  File: 'data/time-travel-demo.delta/_delta_log/00000000000000000000.json'\n",
    "  Size: 819       \tBlocks: 2          IO Block: 262144 regular file\n",
    "Device: 44h/68d\tInode: 1099511789237  Links: 1\n",
    "Access: (0644/-rw-r--r--)  Uid: (    0/    root)   Gid: (    0/    root)\n",
    "Access: 2022-05-20 18:42:13.100546536 +0000\n",
    "Modify: 2022-05-20 18:42:13.341529695 +0000\n",
    "Change: 2022-05-20 18:42:13.691505237 +0000\n",
    " Birth: -\n",
    "  File: 'data/time-travel-demo.delta/_delta_log/00000000000000000001.json'\n",
    "  Size: 1049      \tBlocks: 3          IO Block: 262144 regular file\n",
    "Device: 44h/68d\tInode: 1099511789264  Links: 1\n",
    "Access: (0644/-rw-r--r--)  Uid: (    0/    root)   Gid: (    0/    root)\n",
    "Access: 2022-05-20 19:29:50.672776623 +0000\n",
    "Modify: 2022-05-20 19:29:51.389726484 +0000\n",
    "Change: 2022-05-20 19:29:51.526716905 +0000\n",
    " Birth: -  \n",
    " ```\n",
    " \n",
    "**Note**: While delta lake does use this modification timestamp to attach a timestamp to a version number, it is not exact. We will see that timestamps for version numbers only takethe first three digits of the microsecond field. In this example we would see a timestamp of '2022-05-20 19:29:51.389' attached to version 1. \n",
    "\n",
    "**Note**: If the filesystem timestamps get messed up or out of order, delta lake will add one milisecond to the previoustimestamp to compute a new timestamp for a version. So the filesystem timestamps may not always line up.\n",
    "\n",
    "We will see that deltalake provides a utility for calculating the timestamps ov each version so we dont have to worry!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e87fda-834c-4c1e-bb10-85f1b6236842",
   "metadata": {},
   "source": [
    "## 2.1. Fire up spark\n",
    "We assume you already have a working spark implimentation and you have already installed the delta pip package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d5e6ec10-86b5-45ec-88ad-7710f5e3ed11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import delta\n",
    "sparkConf = pyspark.SparkConf()\n",
    "sparkConf.setAppName(\"delta-time-travel-demo\")\n",
    "sparkConf.set(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "sparkConf.set(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "sparkConf.set(\"spark.databricks.delta.stateReconstructionValidation.enabled\", \"false\")\n",
    "sparkSessionBuilder = pyspark.sql.SparkSession.builder.config(conf=sparkConf)\n",
    "sparkSession = delta.configure_spark_with_delta_pip(sparkSessionBuilder).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbe2bb3-a7ac-4e2f-b8ba-5bd52f40985e",
   "metadata": {},
   "source": [
    "## 2.2. Write data and observe changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "51974ea8-ef38-4d89-969a-b9f7ccde02e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  A|  B|\n",
      "+---+---+\n",
      "|  1|  6|\n",
      "|  2|  7|\n",
      "|  3|  8|\n",
      "|  4|  9|\n",
      "|  5| 10|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "pandas_df = pandas.DataFrame({\n",
    "    \"A\": [1,2,3,4,5],\n",
    "    \"B\": [6,7,8,9,10],\n",
    "})\n",
    "spark_df = sparkSession.createDataFrame(pandas_df)\n",
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29c2ae3-1e92-4d1a-a8a0-160056e14c1d",
   "metadata": {},
   "source": [
    " Before writing our data we have a look at our data directory and see that it is empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1241bdc0-c4dd-40e2-9f39-a806f2d7e2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1\n",
      "drwxr-xr-x 1 root root 2 May 20 18:21 .\n",
      "drwxr-xr-x 1 root root 5 May 20 18:37 ..\n",
      "-rw-r--r-- 1 root root 2 May 20 18:18 .gitignore\n",
      "drwxr-xr-x 1 root root 3 May 20 18:21 time-travel-demo.delta\n"
     ]
    }
   ],
   "source": [
    "! ls -la data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50b121d-75db-4dae-b92d-db78e226aa3e",
   "metadata": {},
   "source": [
    "We now write the data as a delta table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "8c587c58-b43d-406d-ac55-fb2c8d3828b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_table_path = \"data/time-travel-demo.delta\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "45a768cf-3856-4c59-b8dd-2092836e4640",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark_df.write.format(\"delta\").save(delta_table_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd28f744-7e48-4cf6-b266-ff02965cfedf",
   "metadata": {},
   "source": [
    "We again observe our directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "21f86af8-0a35-43ba-a2bd-7272279ef17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\n",
      "`-- time-travel-demo.delta\n",
      "    |-- _delta_log\n",
      "    |   `-- 00000000000000000000.json\n",
      "    `-- part-00000-41e1a8cd-15d5-4263-9413-f649ad1d51da-c000.snappy.parquet\n",
      "\n",
      "2 directories, 2 files\n"
     ]
    }
   ],
   "source": [
    "! tree data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "b751eea4-40ce-4fef-aad4-5d059c1a51e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pyprojroot\n",
    "import os\n",
    "\n",
    "def load_delta_log(log_file_path):\n",
    "    with open(log_file_path, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "        proper_json = \"[\" + \",\".join(lines) + \"]\"\n",
    "        d = json.loads(proper_json)\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "081a7219-77fc-4637-9215-f9ac38f6cb58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"commitInfo\": {\n",
      "            \"timestamp\": 1653072133086,\n",
      "            \"operation\": \"WRITE\",\n",
      "            \"operationParameters\": {\n",
      "                \"mode\": \"ErrorIfExists\",\n",
      "                \"partitionBy\": \"[]\"\n",
      "            },\n",
      "            \"isBlindAppend\": true,\n",
      "            \"operationMetrics\": {\n",
      "                \"numFiles\": \"1\",\n",
      "                \"numOutputBytes\": \"735\",\n",
      "                \"numOutputRows\": \"5\"\n",
      "            }\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"protocol\": {\n",
      "            \"minReaderVersion\": 1,\n",
      "            \"minWriterVersion\": 2\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"metaData\": {\n",
      "            \"id\": \"3af5efe7-073f-4692-9e7b-51d76ced0146\",\n",
      "            \"format\": {\n",
      "                \"provider\": \"parquet\",\n",
      "                \"options\": {}\n",
      "            },\n",
      "            \"schemaString\": \"{\\\"type\\\":\\\"struct\\\",\\\"fields\\\":[{\\\"name\\\":\\\"A\\\",\\\"type\\\":\\\"long\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"B\\\",\\\"type\\\":\\\"long\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}}]}\",\n",
      "            \"partitionColumns\": [],\n",
      "            \"configuration\": {},\n",
      "            \"createdTime\": 1653072132288\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"add\": {\n",
      "            \"path\": \"part-00000-41e1a8cd-15d5-4263-9413-f649ad1d51da-c000.snappy.parquet\",\n",
      "            \"partitionValues\": {},\n",
      "            \"size\": 735,\n",
      "            \"modificationTime\": 1653072132985,\n",
      "            \"dataChange\": true\n",
      "        }\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "log_file_path = os.path.join(delta_table_path, \"_delta_log\",\"00000000000000000000.json\") \n",
    "print(json.dumps(load_delta_log(log_file_path), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91644560-7051-46fa-9054-820488604217",
   "metadata": {},
   "source": [
    "It's important to understand the data written in this file. We have the following list of dictionaries. \n",
    "\n",
    "1. Information about the commit\n",
    "2. Information about the transaction protocol\n",
    "3. [Table metadata](https://docs.databricks.com/delta/delta-batch.html#table-properties) (We will discuss these in more detail later)\n",
    "4. Information about Add operation\n",
    "\n",
    "\n",
    "These are effectively our shapshots and we can see a section for add which tells us that a new part was added to the table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8305aa-f0a8-492b-a797-29afeb46957a",
   "metadata": {},
   "source": [
    "We now modify our data and again write our data to the delta lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "7d231526-7c55-40e7-ad75-42638ac4f726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|  A|  B|  C|\n",
      "+---+---+---+\n",
      "|  1|  6| 11|\n",
      "|  2|  7| 12|\n",
      "|  3|  8| 13|\n",
      "|  4|  9| 14|\n",
      "|  5| 10| 15|\n",
      "+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id, row_number, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "C = [11,12,13,14,15]\n",
    "spark_udf = udf(lambda i: C[i -1], IntegerType())\n",
    "new_spark_df = spark_df.withColumn(\"C\", spark_udf('A'))\n",
    "new_spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "085b1a1b-4a60-492a-8ed2-7652d2b9cccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 2\n",
      "drwxr-xr-x 1 root root   3 May 20 18:42 .\n",
      "drwxr-xr-x 1 root root   2 May 20 18:42 ..\n",
      "-rw-r--r-- 1 root root  16 May 20 18:42 .part-00000-41e1a8cd-15d5-4263-9413-f649ad1d51da-c000.snappy.parquet.crc\n",
      "drwxr-xr-x 1 root root   1 May 20 18:42 _delta_log\n",
      "-rw-r--r-- 1 root root 735 May 20 18:42 part-00000-41e1a8cd-15d5-4263-9413-f649ad1d51da-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "! ls -la data/time-travel-demo.delta/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "e47d0319-086b-4057-bedd-83adb6efa0ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "new_spark_df\\\n",
    "  .write\\\n",
    "  .format(\"delta\")\\\n",
    "  .mode(\"overwrite\")\\\n",
    "  .option(\"overwriteSchema\", \"true\")\\\n",
    "  .save(delta_table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "4077e048-e0c5-4e50-bd9c-df52b178b5da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/time-travel-demo.delta/\n",
      "|-- _delta_log\n",
      "|   |-- 00000000000000000000.json\n",
      "|   `-- 00000000000000000001.json\n",
      "|-- part-00000-41e1a8cd-15d5-4263-9413-f649ad1d51da-c000.snappy.parquet\n",
      "`-- part-00000-e8891ae6-e9d4-449a-8531-de44d41f7669-c000.snappy.parquet\n",
      "\n",
      "1 directory, 4 files\n"
     ]
    }
   ],
   "source": [
    "! tree data/time-travel-demo.delta/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0d2dce-6a67-4e35-b824-d6b6f64fd50e",
   "metadata": {},
   "source": [
    "We can see a new part has now shown up in the log directory. If we look at the log, we can see more entries than just an add (we see an add and a remove operation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "5bea3d5d-e6f2-493a-a6fc-a460437e819c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"commitInfo\": {\n",
      "            \"timestamp\": 1653074990646,\n",
      "            \"operation\": \"WRITE\",\n",
      "            \"operationParameters\": {\n",
      "                \"mode\": \"Overwrite\",\n",
      "                \"partitionBy\": \"[]\"\n",
      "            },\n",
      "            \"readVersion\": 0,\n",
      "            \"isBlindAppend\": false,\n",
      "            \"operationMetrics\": {\n",
      "                \"numFiles\": \"1\",\n",
      "                \"numOutputBytes\": \"946\",\n",
      "                \"numOutputRows\": \"5\"\n",
      "            }\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"metaData\": {\n",
      "            \"id\": \"3af5efe7-073f-4692-9e7b-51d76ced0146\",\n",
      "            \"format\": {\n",
      "                \"provider\": \"parquet\",\n",
      "                \"options\": {}\n",
      "            },\n",
      "            \"schemaString\": \"{\\\"type\\\":\\\"struct\\\",\\\"fields\\\":[{\\\"name\\\":\\\"A\\\",\\\"type\\\":\\\"long\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"B\\\",\\\"type\\\":\\\"long\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"C\\\",\\\"type\\\":\\\"integer\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}}]}\",\n",
      "            \"partitionColumns\": [],\n",
      "            \"configuration\": {},\n",
      "            \"createdTime\": 1653072132288\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"add\": {\n",
      "            \"path\": \"part-00000-e8891ae6-e9d4-449a-8531-de44d41f7669-c000.snappy.parquet\",\n",
      "            \"partitionValues\": {},\n",
      "            \"size\": 946,\n",
      "            \"modificationTime\": 1653074988956,\n",
      "            \"dataChange\": true\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"remove\": {\n",
      "            \"path\": \"part-00000-41e1a8cd-15d5-4263-9413-f649ad1d51da-c000.snappy.parquet\",\n",
      "            \"deletionTimestamp\": 1653074990645,\n",
      "            \"dataChange\": true,\n",
      "            \"extendedFileMetadata\": true,\n",
      "            \"partitionValues\": {},\n",
      "            \"size\": 735\n",
      "        }\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "log_file_path = os.path.join(delta_table_path, \"_delta_log\",\"00000000000000000001.json\") \n",
    "log = load_delta_log(log_file_path)\n",
    "print(json.dumps(log, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd400b5-71c6-4e5b-86a1-ec2dbe2039f0",
   "metadata": {},
   "source": [
    "Looking closely we can see that a part was removed from the table (the part we added in the first transaction file) and a new part was added."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b502de-9d80-4f1e-8d0f-d2da20b7f0d0",
   "metadata": {},
   "source": [
    "We can also see timestamps associated with each operation (modificationTime for add and deletionTimestamp for remove operations) as well as a timestamp for the entire atomic operation (commitInfo) and a timestamp for the metadata. We "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd011f0d-0b3f-4239-8322-9a8227521b34",
   "metadata": {},
   "source": [
    "## 2.3. Load timestamps for operations from log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5671307-0e24-4920-ba96-d928404d1037",
   "metadata": {},
   "source": [
    "Timestamps are somewhat difficult to interpret. We will load the timestamps into datetime objects so they are more human friendly and so we can see what is going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "bf7eefe0-ad46-4935-b554-926104dfc1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v0 Info:\n",
      "Commit: 2022-05-20 18:42:13.086000\n",
      "Add: 2022-05-20 18:42:12.985000\n",
      "Creation: 2022-05-20 18:42:12.288000\n",
      "\n",
      "v1 Info:\n",
      "Commit: 2022-05-20 19:29:50.646000\n",
      "Add: 2022-05-20 19:29:48.956000\n",
      "Remove: 2022-05-20 19:29:50.645000\n",
      "Creation: 2022-05-20 18:42:12.288000\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "log0 = load_delta_log(os.path.join(delta_table_path, \"_delta_log\",\"00000000000000000000.json\"))\n",
    "\n",
    "log0_commit_info = log0[0][\"commitInfo\"]\n",
    "log0_commit_info_timestamp = log0_commit_info[\"timestamp\"]\n",
    "log0_commit_info_dt = datetime.datetime.fromtimestamp(log0_commit_info_timestamp/ 1e3)\n",
    "\n",
    "log0_metadata_info = log0[2][\"metaData\"]\n",
    "log0_metadata_info_timestamp = log0_metadata_info[\"createdTime\"]\n",
    "log0_metadata_info_dt = datetime.datetime.fromtimestamp(log0_metadata_info_timestamp/ 1e3)\n",
    "\n",
    "log0_add_info = log0[3][\"add\"][\"modificationTime\"]\n",
    "log0_add_info_dt = datetime.datetime.fromtimestamp(log0_add_info_timestamp/ 1e3)\n",
    "\n",
    "log1 = load_delta_log(os.path.join(delta_table_path, \"_delta_log\",\"00000000000000000001.json\"))\n",
    "\n",
    "log1_commit_info = log1[0][\"commitInfo\"]\n",
    "log1_commit_info_timestamp = log1_commit_info[\"timestamp\"]\n",
    "log1_commit_info_dt = datetime.datetime.fromtimestamp(log1_commit_info_timestamp/ 1e3)\n",
    "\n",
    "log1_metadata_info = log1[1][\"metaData\"]\n",
    "log1_metadata_info_timestamp = log1_metadata_info[\"createdTime\"]\n",
    "log1_metadata_info_dt = datetime.datetime.fromtimestamp(log0_metadata_info_timestamp/ 1e3)\n",
    "\n",
    "log1_add_info = log1[2][\"add\"]\n",
    "log1_add_info_timestamp = log1_add_info[\"modificationTime\"]\n",
    "log1_add_info_dt = datetime.datetime.fromtimestamp(log1_add_info_timestamp/ 1e3)\n",
    "\n",
    "log1_remove_info = log1[3][\"remove\"]\n",
    "log1_remove_info_timestamp = log1_remove_info[\"deletionTimestamp\"]\n",
    "log1_remove_info_dt = datetime.datetime.fromtimestamp(log1_remove_info_timestamp/ 1e3)\n",
    "\n",
    "print(\"v0 Info:\")\n",
    "print(f\"Commit: {log0_commit_info_dt}\")\n",
    "print(f\"Add: {log0_add_info_dt}\")\n",
    "print(f\"Creation: {log0_metadata_info_dt}\")\n",
    "print(\"\")\n",
    "print(\"v1 Info:\")\n",
    "print(f\"Commit: {log1_commit_info_dt}\")\n",
    "print(f\"Add: {log1_add_info_dt}\")\n",
    "print(f\"Remove: {log1_remove_info_dt}\")\n",
    "print(f\"Creation: {log1_metadata_info_dt}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116eb960-6233-4fdc-be98-b137e9307e4d",
   "metadata": {},
   "source": [
    "We can see that the timestamp from the commitInfo section occurs the latest which logically makes sense."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e7d019-c7a5-4314-b6db-5bb7c0de4fa0",
   "metadata": {},
   "source": [
    "# 3. The DeltaTable Object\n",
    "\n",
    "A caveat of inspecting the log is that the timestamps are not always useful for doing timetravel. There is some black magic potentially going on and thankfully delta lake provides many utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "cf8d6b3e-7783-477e-997f-3d3cf4396fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Create a pointer tothe delta table (dont confuse with spark dataframe)\n",
    "deltaTable = DeltaTable.forPath(sparkSession, delta_table_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebbae58-9354-412b-8f63-c865d1a1775d",
   "metadata": {},
   "source": [
    "## 3.1. The history() function\n",
    "\n",
    "Deltalake provides a DeltaTable object which allows us to query metadata about the table, specifically the history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "b97e15ef-ab95-4e6d-8d37-48d9316802ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>version</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>userId</th>\n",
       "      <th>userName</th>\n",
       "      <th>operation</th>\n",
       "      <th>operationParameters</th>\n",
       "      <th>job</th>\n",
       "      <th>notebook</th>\n",
       "      <th>clusterId</th>\n",
       "      <th>readVersion</th>\n",
       "      <th>isolationLevel</th>\n",
       "      <th>isBlindAppend</th>\n",
       "      <th>operationMetrics</th>\n",
       "      <th>userMetadata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2022-05-20 19:29:51.389</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>WRITE</td>\n",
       "      <td>{'mode': 'Overwrite', 'partitionBy': '[]'}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>{'numOutputRows': '5', 'numOutputBytes': '946'...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2022-05-20 18:42:13.341</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>WRITE</td>\n",
       "      <td>{'mode': 'ErrorIfExists', 'partitionBy': '[]'}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>{'numOutputRows': '5', 'numOutputBytes': '735'...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   version               timestamp userId userName operation  \\\n",
       "0        1 2022-05-20 19:29:51.389   None     None     WRITE   \n",
       "1        0 2022-05-20 18:42:13.341   None     None     WRITE   \n",
       "\n",
       "                              operationParameters   job notebook clusterId  \\\n",
       "0      {'mode': 'Overwrite', 'partitionBy': '[]'}  None     None      None   \n",
       "1  {'mode': 'ErrorIfExists', 'partitionBy': '[]'}  None     None      None   \n",
       "\n",
       "   readVersion isolationLevel  isBlindAppend  \\\n",
       "0          0.0           None          False   \n",
       "1          NaN           None           True   \n",
       "\n",
       "                                    operationMetrics userMetadata  \n",
       "0  {'numOutputRows': '5', 'numOutputBytes': '946'...         None  \n",
       "1  {'numOutputRows': '5', 'numOutputBytes': '735'...         None  "
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the history of the deltatable and store it as a pandas dataframe (for convenience)\n",
    "dt_history_df = deltaTable.history()\n",
    "dt_history_pd_df = dt_history_df.toPandas()\n",
    "dt_history_pd_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475e548b-befb-494e-b65d-13cf24d99600",
   "metadata": {},
   "source": [
    "This useful table will tie a version number to a datetime. It can also provide other userful describing the operation andthe user performing the operation. In our case, we see some information is missing. That will be discussed separaetely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "b1a5b78a-630e-49f2-a800-09702a4f0c03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2022-05-20 19:29:51.389000')"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1_dt = dt_history_pd_df[dt_history_pd_df[\"version\"] == 1][\"timestamp\"].iloc[0]\n",
    "v1_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "aa996401-643b-4d54-ae9a-78fc0c769fba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_history_pd_df[dt_history_pd_df[\"timestamp\"] < v1_dt.strftime('%Y-%m-%d %H:%M:%S.%f')][\"version\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd50aa3-2c71-476a-8acd-22452dccd0cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa2fc652-1e59-441a-85cb-213e6088d1eb",
   "metadata": {},
   "source": [
    "## 3.2. The vacuum() function\n",
    "As mentioned previously, the delta table is stored in parts and assembled from those parts. At some point we may want to do a cleanup and delete old files so that the footprint of the table shrinks.\n",
    "\n",
    "You can remove files no longer referenced by a Delta table and are older than the retention threshold by running the vacuum command on the table.\n",
    "\n",
    "Vacuum deletes only data files, not log files. Log files are deleted automatically and asynchronously after checkpoint operations.\n",
    "\n",
    "We will look more at the retention policy in a later section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a36431-4c18-453b-b4de-9dd1d94334c5",
   "metadata": {},
   "source": [
    "# 3. Load Versioned Data\n",
    "In this section we will explore the ways in which we can load our data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13303eb7-4a11-4a45-8bee-1c217860ec25",
   "metadata": {},
   "source": [
    "## 3.1. Load based on version number"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c28f8fb-bcf4-497a-8d5d-ed4b1501d174",
   "metadata": {},
   "source": [
    "We saw the \\<version>.json file in the \\_delta_log directory. This allows us to load a given version via it's index. Below we can see an example of loading the first version of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "461c49b6-4295-422a-a151-e8e1d82f197d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 51:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  A|  B|\n",
      "+---+---+\n",
      "|  1|  6|\n",
      "|  2|  7|\n",
      "|  3|  8|\n",
      "|  4|  9|\n",
      "|  5| 10|\n",
      "+---+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sparkSession.read.format(\"delta\").option(\"versionAsOf\", 0).load(delta_table_path).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "5eebce91-7d1c-410f-b767-f28d4dc02cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|  A|  B|  C|\n",
      "+---+---+---+\n",
      "|  1|  6| 11|\n",
      "|  2|  7| 12|\n",
      "|  3|  8| 13|\n",
      "|  4|  9| 14|\n",
      "|  5| 10| 15|\n",
      "+---+---+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sparkSession.read.format(\"delta\").option(\"versionAsOf\", 1).load(delta_table_path).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcc8210-cad4-483a-a15d-be3d3d1fa8a1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3.2. Load based on timestamp\n",
    "Loading data based on a version number is not that user friendly or intuitive so instead we will look at loading data based on a timestamp."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971c1a6d-18d2-40b1-889c-361813d8d854",
   "metadata": {},
   "source": [
    "### 3.2.1. Note on possible errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b99d03e-dc0c-44ae-9d33-91516e0ae60d",
   "metadata": {},
   "source": [
    "**Note**: There is a limited band of time for which we can pull data as of. There is a minimum and maximum datetime. If we try to pull a datetime that is outside this band, we will see an error thrown which looks like this:\n",
    "```\n",
    "Py4JJavaError: An error occurred while calling o1083.load.\n",
    ": org.apache.spark.sql.delta.DeltaErrors$TimestampEarlierThanCommitRetentionException: The provided timestamp (2022-05-20 18:42:13.0) is before the earliest version available to this\n",
    "table (2022-05-20 18:42:13.341). Please use a timestamp after 2022-05-20 18:42:13.\n",
    "```\n",
    "\n",
    "\n",
    "``` \n",
    "Py4JJavaError: An error occurred while calling o1131.load.\n",
    ": org.apache.spark.sql.delta.DeltaErrors$TemporallyUnstableInputException: The provided timestamp: 2022-05-20 19:30:50.646 is after the latest commit timestamp of\n",
    "2022-05-20 19:29:51.389. If you wish to query this version of the table, please either provide\n",
    "the version with \"VERSION AS OF 1\" or use the exact timestamp\n",
    "of the last commit: \"TIMESTAMP AS OF '2022-05-20 19:29:51'\".\n",
    "```\n",
    "\n",
    "**Note**: I have seen many articles claiming that we must specify the timestamp in yyyyMMddHHmmssSSS format. This is not true as we will see. The date must be a string in the format '%Y-%m-%d %H:%M:%S.%f'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f2c2fd-f4b0-43c3-8d68-a6478b394910",
   "metadata": {},
   "source": [
    "### 3.2.1. Load data using timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d2de5e-f468-4538-9093-4cdfbca356d8",
   "metadata": {},
   "source": [
    "We first pull the timestamps from the history() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "d49d23bd-0a10-4e27-acbf-325b5c6f4e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-20 18:42:13.341000\n",
      "2022-05-20 19:29:51.389000\n"
     ]
    }
   ],
   "source": [
    "v0_timestamp = dt_history_pd_df[dt_history_pd_df[\"version\"] == 0][\"timestamp\"].iloc[0]\n",
    "v0_timestamp_str = v0_timestamp.strftime('%Y-%m-%d %H:%M:%S.%f')\n",
    "print(v0_timestamp_str)\n",
    "\n",
    "v1_timestamp = dt_history_pd_df[dt_history_pd_df[\"version\"] == 1][\"timestamp\"].iloc[0]\n",
    "v1_timestamp_str = v1_timestamp.strftime('%Y-%m-%d %H:%M:%S.%f')\n",
    "print(v1_timestamp_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bc2a40-fc77-403b-bd11-a174af775c60",
   "metadata": {},
   "source": [
    "We load the timestamp associated with the first version and we see that the data is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "5f13e7ee-a61e-48db-a9b4-5c60f33aae0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  A|  B|\n",
      "+---+---+\n",
      "|  1|  6|\n",
      "|  2|  7|\n",
      "|  3|  8|\n",
      "|  4|  9|\n",
      "|  5| 10|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkSession.read \\\n",
    "  .format(\"delta\") \\\n",
    "  .option(\"timestampAsOf\", v0_timestamp_str) \\\n",
    "  .load(delta_table_path).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07300767-5cde-499d-8daf-12dc46741bad",
   "metadata": {},
   "source": [
    "We then load data using a timestamp between the commit operations for v1 and v2. We see that the data resembles v1 as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "a1d733a9-4201-40ff-8492-e1d74e0851bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  A|  B|\n",
      "+---+---+\n",
      "|  1|  6|\n",
      "|  2|  7|\n",
      "|  3|  8|\n",
      "|  4|  9|\n",
      "|  5| 10|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt = (v1_timestamp - v0_timestamp)/2\n",
    "mid_point_timestamp = v0_timestamp + dt\n",
    "mid_point_timestamp_str = mid_point_timestamp.strftime('%Y-%m-%d %H:%M:%S.%f')\n",
    "\n",
    "sparkSession.read \\\n",
    "  .format(\"delta\") \\\n",
    "  .option(\"timestampAsOf\", mid_point_timestamp_str) \\\n",
    "  .load(delta_table_path).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f789a1-4fb4-4dcc-bef2-b8ff236f9c7d",
   "metadata": {},
   "source": [
    "We can then see that we can load v2 data using the timestamp for v2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "550fa914-6888-4654-b9d3-fab0dc4e3bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|  A|  B|  C|\n",
      "+---+---+---+\n",
      "|  1|  6| 11|\n",
      "|  2|  7| 12|\n",
      "|  3|  8| 13|\n",
      "|  4|  9| 14|\n",
      "|  5| 10| 15|\n",
      "+---+---+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sparkSession.read \\\n",
    "  .format(\"delta\") \\\n",
    "  .option(\"timestampAsOf\", v1_timestamp_str) \\\n",
    "  .load(delta_table_path).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628cf801-9f23-4479-a66a-35ed64d3241c",
   "metadata": {},
   "source": [
    "## 3.3. Load using spark sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "id": "202d1636-cd2c-4c85-ae3c-b8c7d5ad7a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A   B\n",
       "0  1   6\n",
       "1  2   7\n",
       "2  3   8\n",
       "3  4   9\n",
       "4  5  10"
      ]
     },
     "execution_count": 572,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_dir = \"/workspaces/crypto-data/jupyter-pod/ml-training-jupyter-notebooks/Data Engineering/Data Lakehouse/Deltalake\"\n",
    "delta_table_path = \"data/time-travel-demo.delta\"\n",
    "v0_timestamp_str_2 = v0_timestamp.strftime('%Y%m%d%H%M%S%f')[:-3]\n",
    "p = f\"{current_dir}/{delta_table_path}@{v0_timestamp_str_2}\"\n",
    "\n",
    "sparkSession.sql(f\"SELECT * FROM delta.`{p}`\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "id": "62c4b995-42bb-4f66-a1b8-5a87a868b28f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A   B   C\n",
       "0  1   6  11\n",
       "1  2   7  12\n",
       "2  3   8  13\n",
       "3  4   9  14\n",
       "4  5  10  15"
      ]
     },
     "execution_count": 570,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_dir = \"/workspaces/crypto-data/jupyter-pod/ml-training-jupyter-notebooks/Data Engineering/Data Lakehouse/Deltalake\"\n",
    "delta_table_path = \"data/time-travel-demo.delta\"\n",
    "v1_timestamp_str_2 = v1_timestamp.strftime('%Y%m%d%H%M%S%f')[:-3]\n",
    "p = f\"{current_dir}/{delta_table_path}@{v1_timestamp_str_2}\"\n",
    "\n",
    "sparkSession.sql(f\"SELECT * FROM delta.`{p}`\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f40127-3779-4504-a8a6-d0e8b5bce8c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "082c62cb-50ef-4fe8-99da-5fb86a3c84b0",
   "metadata": {},
   "source": [
    "# 4. Retention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc3d68a-ce8c-4f3e-8540-f4ca618b0b61",
   "metadata": {},
   "source": [
    "To time travel to a previous version, you must retain both the log and the data files for that version.\n",
    "\n",
    "The data files backing a Delta table are never deleted automatically; data files are deleted only when you run VACUUM. VACUUM does not delete Delta log files; log files are automatically cleaned up after checkpoints are written.\n",
    "\n",
    "So to summarize, deleteing data or logs is a manual process. If you run the vacuum() function you will only be able to time travel as far back as the retention policy.\n",
    "\n",
    "The delta lake API provides several methods which impact the retention policy for a delta table. These settings appear to be set on the SparkSession level, not at the table level. \n",
    "\n",
    "According to the [documentation](https://docs.delta.io/latest/delta-utility.html#remove-files-no-longer-referenced-by-a-delta-table), the default retention period of log files is 30 days, configurable through the delta.logRetentionDuration property."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae69068-58fe-49e7-bb01-c590e54721f2",
   "metadata": {},
   "source": [
    "## 4.1. Viewing delta table properties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56579b2b-8f60-42a8-a8ac-0ee29c336a20",
   "metadata": {},
   "source": [
    "We saw previously that propterties can be pulled out of the log files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "id": "2e6459d1-4974-4e87-a6ec-0c1b545f14b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Table or view not found for 'DESCRIBE TABLE': `/data/time-travel-demo.delta`; line 2 pos 0;\n'DescribeRelation false\n+- 'UnresolvedTableOrView [/data/time-travel-demo.delta], DESCRIBE TABLE, true\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [513]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# USING DELTA LOCATION \"{delta_table_path}\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# \"data/time-travel-demo.delta\"\u001b[39;00m\n\u001b[1;32m      4\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124mDESCRIBE TABLE `/data/time-travel-demo.delta`;\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m----> 8\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43msparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m result\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pyspark/sql/session.py:723\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    707\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msql\u001b[39m(\u001b[38;5;28mself\u001b[39m, sqlQuery):\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;124;03m\"\"\"Returns a :class:`DataFrame` representing the result of the given query.\u001b[39;00m\n\u001b[1;32m    709\u001b[0m \n\u001b[1;32m    710\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 2.0.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;124;03m    [Row(f1=1, f2='row1'), Row(f1=2, f2='row2'), Row(f1=3, f2='row3')]\u001b[39;00m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 723\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrapped)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Table or view not found for 'DESCRIBE TABLE': `/data/time-travel-demo.delta`; line 2 pos 0;\n'DescribeRelation false\n+- 'UnresolvedTableOrView [/data/time-travel-demo.delta], DESCRIBE TABLE, true\n"
     ]
    }
   ],
   "source": [
    "# USING DELTA LOCATION \"{delta_table_path}\"\n",
    "# \"data/time-travel-demo.delta\"\n",
    "\n",
    "query = \"\"\"\n",
    "DESCRIBE TABLE `/data/time-travel-demo.delta`;\n",
    "\"\"\"\n",
    "\n",
    "result = sparkSession.sql(query)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3003688-8b9b-4b84-b7dc-e0f9152d7110",
   "metadata": {},
   "source": [
    "# DeltaLog Object\n",
    "\n",
    "DeltaLog aka Delta Lake Transaction log Overview: https://databricks.com/blog/2019/08/21/diving-into-delta-lake-unpacking-the-transaction-log.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdf8f7b-52fe-41f5-adca-fc1988233fd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5292796-1c42-4d06-962e-511283fe778e",
   "metadata": {},
   "source": [
    "Based on [this github issue](https://github.com/delta-io/delta/issues/520), there is no python api because this is a private java interface, but we can use py4j to get access to this object anyways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "e4f9dc63-0ef1-4908-b04a-3bbd12ae3bc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JavaObject id=o1302"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = sparkSession.sparkContext\n",
    "deltaLog = sc._jvm.org.apache.spark.sql.delta.DeltaLog.forTable(sparkSession._jsparkSession, delta_table_path)\n",
    "deltaLog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "id": "58042022-8371-475b-b06b-75129ea68f65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['$anonfun$createDataFrame$1',\n",
       " '$anonfun$createRelation$1',\n",
       " '$anonfun$getChanges$1',\n",
       " '$anonfun$getChanges$1$adapted',\n",
       " '$anonfun$getChanges$2',\n",
       " '$anonfun$getChanges$3',\n",
       " 'LAST_CHECKPOINT',\n",
       " '_ipython_canary_method_should_not_exist_',\n",
       " 'apply',\n",
       " 'apply$default$3',\n",
       " 'assertRemovable',\n",
       " 'asyncUpdateTask',\n",
       " 'asyncUpdateTask_$eq',\n",
       " 'checkLogStoreConfConflicts',\n",
       " 'checkpoint',\n",
       " 'checkpointInterval',\n",
       " 'cleanUpExpiredLogs',\n",
       " 'clearCache',\n",
       " 'clock',\n",
       " 'compositeId',\n",
       " 'createDataFrame',\n",
       " 'createDataFrame$default$3',\n",
       " 'createDataFrame$default$4',\n",
       " 'createLogStore',\n",
       " 'createRelation',\n",
       " 'createRelation$default$1',\n",
       " 'createRelation$default$2',\n",
       " 'createRelation$default$3',\n",
       " 'createRelation$default$4',\n",
       " 'createSnapshot',\n",
       " 'currentSnapshot',\n",
       " 'currentSnapshot_$eq',\n",
       " 'dataPath',\n",
       " 'defaultLogStoreClass',\n",
       " 'deltaLogLock',\n",
       " 'deltaRetentionMillis',\n",
       " 'doLogCleanup',\n",
       " 'enableExpiredLogCleanup',\n",
       " 'ensureLogDirectoryExist',\n",
       " 'equals',\n",
       " 'filterFileList',\n",
       " 'filterFileList$default$4',\n",
       " 'findLastCompleteCheckpoint',\n",
       " 'forTable',\n",
       " 'fs',\n",
       " 'getChanges',\n",
       " 'getChanges$default$2',\n",
       " 'getClass',\n",
       " 'getLatestCompleteCheckpointFromList',\n",
       " 'getLogSegmentForVersion',\n",
       " 'getLogSegmentForVersion$default$2',\n",
       " 'getLogSegmentFrom',\n",
       " 'getSnapshotAt',\n",
       " 'getSnapshotAt$default$2',\n",
       " 'getSnapshotAt$default$3',\n",
       " 'getSnapshotAtInit',\n",
       " 'hashCode',\n",
       " 'history',\n",
       " 'initializeForcefully',\n",
       " 'initializeLogIfNecessary',\n",
       " 'initializeLogIfNecessary$default$2',\n",
       " 'invalidateCache',\n",
       " 'isSameLogAs',\n",
       " 'isTraceEnabled',\n",
       " 'lastCheckpoint',\n",
       " 'lastUpdateTimestamp',\n",
       " 'lastUpdateTimestamp_$eq',\n",
       " 'listFrom',\n",
       " 'lockInterruptibly',\n",
       " 'log',\n",
       " 'logConsole',\n",
       " 'logDebug',\n",
       " 'logError',\n",
       " 'logInfo',\n",
       " 'logName',\n",
       " 'logPath',\n",
       " 'logStoreClassConfKey',\n",
       " 'logTrace',\n",
       " 'logWarning',\n",
       " 'manuallyLoadCheckpoint',\n",
       " 'maxSnapshotLineageLength',\n",
       " 'metadata',\n",
       " 'minFileRetentionTimestamp',\n",
       " 'notify',\n",
       " 'notifyAll',\n",
       " 'org$apache$spark$internal$Logging$$log_',\n",
       " 'org$apache$spark$internal$Logging$$log__$eq',\n",
       " 'org$apache$spark$sql$delta$Checkpoints$_setter_$LAST_CHECKPOINT_$eq',\n",
       " 'org$apache$spark$sql$delta$storage$LogStoreProvider$_setter_$defaultLogStoreClass_$eq',\n",
       " 'org$apache$spark$sql$delta$storage$LogStoreProvider$_setter_$logStoreClassConfKey_$eq',\n",
       " 'protocolRead',\n",
       " 'protocolWrite',\n",
       " 'protocolWrite$default$2',\n",
       " 'readChecksum',\n",
       " 'recordDeltaEvent',\n",
       " 'recordDeltaEvent$default$3',\n",
       " 'recordDeltaEvent$default$4',\n",
       " 'recordDeltaOperation',\n",
       " 'recordDeltaOperation$default$3',\n",
       " 'recordEvent',\n",
       " 'recordEvent$default$2',\n",
       " 'recordEvent$default$3',\n",
       " 'recordEvent$default$4',\n",
       " 'recordOperation',\n",
       " 'recordOperation$default$2',\n",
       " 'recordOperation$default$4',\n",
       " 'recordOperation$default$5',\n",
       " 'recordOperation$default$6',\n",
       " 'recordOperation$default$7',\n",
       " 'recordOperation$default$8',\n",
       " 'recordOperation$default$9',\n",
       " 'recordUsage',\n",
       " 'recordUsage$default$3',\n",
       " 'recordUsage$default$4',\n",
       " 'recordUsage$default$5',\n",
       " 'recordUsage$default$6',\n",
       " 'recordUsage$default$7',\n",
       " 'replaceSnapshot',\n",
       " 'rewritePartitionFilters',\n",
       " 'rewritePartitionFilters$default$4',\n",
       " 'snapshot',\n",
       " 'spark',\n",
       " 'startTransaction',\n",
       " 'store',\n",
       " 'tableExists',\n",
       " 'tableId',\n",
       " 'toString',\n",
       " 'tombstoneRetentionMillis',\n",
       " 'truncateDay',\n",
       " 'update',\n",
       " 'update$default$1',\n",
       " 'updateInternal',\n",
       " 'upgradeProtocol',\n",
       " 'upgradeProtocol$default$1',\n",
       " 'verifyDeltaVersions',\n",
       " 'wait',\n",
       " 'withNewTransaction',\n",
       " 'withStatusCode',\n",
       " 'withStatusCode$default$3',\n",
       " 'writeCheckpointFiles']"
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(deltaLog)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "id": "bd2c8806-da75-41aa-825b-54c60c5322cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3af5efe7-073f-4692-9e7b-51d76ced0146'"
      ]
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deltaLog.tableId()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "id": "a3bc2dd8-a617-4f05-a84c-4becfc57517e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['$anonfun$dataSchema$1',\n",
       " '$anonfun$dataSchema$1$adapted',\n",
       " '$anonfun$partitionSchema$1',\n",
       " '$anonfun$schema$1',\n",
       " '$anonfun$schema$2',\n",
       " '$lessinit$greater$default$1',\n",
       " '$lessinit$greater$default$2',\n",
       " '$lessinit$greater$default$3',\n",
       " '$lessinit$greater$default$4',\n",
       " '$lessinit$greater$default$5',\n",
       " '$lessinit$greater$default$6',\n",
       " '$lessinit$greater$default$7',\n",
       " '$lessinit$greater$default$8',\n",
       " 'apply',\n",
       " 'apply$default$1',\n",
       " 'apply$default$2',\n",
       " 'apply$default$3',\n",
       " 'apply$default$4',\n",
       " 'apply$default$5',\n",
       " 'apply$default$6',\n",
       " 'apply$default$7',\n",
       " 'apply$default$8',\n",
       " 'canEqual',\n",
       " 'configuration',\n",
       " 'copy',\n",
       " 'copy$default$1',\n",
       " 'copy$default$2',\n",
       " 'copy$default$3',\n",
       " 'copy$default$4',\n",
       " 'copy$default$5',\n",
       " 'copy$default$6',\n",
       " 'copy$default$7',\n",
       " 'copy$default$8',\n",
       " 'createdTime',\n",
       " 'curried',\n",
       " 'dataSchema',\n",
       " 'description',\n",
       " 'equals',\n",
       " 'fixedTypeColumns',\n",
       " 'format',\n",
       " 'getClass',\n",
       " 'hashCode',\n",
       " 'id',\n",
       " 'json',\n",
       " 'name',\n",
       " 'notify',\n",
       " 'notifyAll',\n",
       " 'partitionColumns',\n",
       " 'partitionSchema',\n",
       " 'productArity',\n",
       " 'productElement',\n",
       " 'productIterator',\n",
       " 'productPrefix',\n",
       " 'schema',\n",
       " 'schemaString',\n",
       " 'toString',\n",
       " 'tupled',\n",
       " 'unapply',\n",
       " 'wait',\n",
       " 'wrap']"
      ]
     },
     "execution_count": 517,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(deltaLog.metadata())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "id": "cfb49e31-bb1c-4224-a58e-d2be4db9a675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"metaData\": {\n",
      "        \"id\": \"3af5efe7-073f-4692-9e7b-51d76ced0146\",\n",
      "        \"format\": {\n",
      "            \"provider\": \"parquet\",\n",
      "            \"options\": {}\n",
      "        },\n",
      "        \"schemaString\": \"{\\\"type\\\":\\\"struct\\\",\\\"fields\\\":[{\\\"name\\\":\\\"A\\\",\\\"type\\\":\\\"long\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"B\\\",\\\"type\\\":\\\"long\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"C\\\",\\\"type\\\":\\\"integer\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}}]}\",\n",
      "        \"partitionColumns\": [],\n",
      "        \"configuration\": {},\n",
      "        \"createdTime\": 1653072132288\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "metadata = json.loads(deltaLog.metadata().json())\n",
    "print(json.dumps(metadata, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "76a1685b-6fd8-407c-b9f2-1a41c06cf7ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['$colon$bslash',\n",
       " '$div$colon',\n",
       " '$minus',\n",
       " '$minus$minus',\n",
       " '$plus',\n",
       " '$plus$plus',\n",
       " '$plus$plus$colon',\n",
       " 'addString',\n",
       " 'aggregate',\n",
       " 'andThen',\n",
       " 'apply',\n",
       " 'apply$mcDD$sp',\n",
       " 'apply$mcDF$sp',\n",
       " 'apply$mcDI$sp',\n",
       " 'apply$mcDJ$sp',\n",
       " 'apply$mcFD$sp',\n",
       " 'apply$mcFF$sp',\n",
       " 'apply$mcFI$sp',\n",
       " 'apply$mcFJ$sp',\n",
       " 'apply$mcID$sp',\n",
       " 'apply$mcIF$sp',\n",
       " 'apply$mcII$sp',\n",
       " 'apply$mcIJ$sp',\n",
       " 'apply$mcJD$sp',\n",
       " 'apply$mcJF$sp',\n",
       " 'apply$mcJI$sp',\n",
       " 'apply$mcJJ$sp',\n",
       " 'apply$mcVD$sp',\n",
       " 'apply$mcVF$sp',\n",
       " 'apply$mcVI$sp',\n",
       " 'apply$mcVJ$sp',\n",
       " 'apply$mcZD$sp',\n",
       " 'apply$mcZF$sp',\n",
       " 'apply$mcZI$sp',\n",
       " 'apply$mcZJ$sp',\n",
       " 'applyOrElse',\n",
       " 'canEqual',\n",
       " 'collect',\n",
       " 'collectFirst',\n",
       " 'companion',\n",
       " 'compose',\n",
       " 'contains',\n",
       " 'copyToArray',\n",
       " 'copyToBuffer',\n",
       " 'count',\n",
       " 'default',\n",
       " 'drop',\n",
       " 'dropRight',\n",
       " 'dropWhile',\n",
       " 'empty',\n",
       " 'equals',\n",
       " 'exists',\n",
       " 'filter',\n",
       " 'filterImpl',\n",
       " 'filterKeys',\n",
       " 'filterNot',\n",
       " 'find',\n",
       " 'flatMap',\n",
       " 'flatten',\n",
       " 'fold',\n",
       " 'foldLeft',\n",
       " 'foldRight',\n",
       " 'forall',\n",
       " 'foreach',\n",
       " 'genericBuilder',\n",
       " 'get',\n",
       " 'getClass',\n",
       " 'getOrElse',\n",
       " 'groupBy',\n",
       " 'grouped',\n",
       " 'hasDefiniteSize',\n",
       " 'hashCode',\n",
       " 'head',\n",
       " 'headOption',\n",
       " 'init',\n",
       " 'inits',\n",
       " 'isDefinedAt',\n",
       " 'isEmpty',\n",
       " 'isTraversableAgain',\n",
       " 'iterator',\n",
       " 'keySet',\n",
       " 'keys',\n",
       " 'keysIterator',\n",
       " 'last',\n",
       " 'lastOption',\n",
       " 'lift',\n",
       " 'map',\n",
       " 'mapValues',\n",
       " 'max',\n",
       " 'maxBy',\n",
       " 'min',\n",
       " 'minBy',\n",
       " 'mkString',\n",
       " 'newBuilder',\n",
       " 'nonEmpty',\n",
       " 'notify',\n",
       " 'notifyAll',\n",
       " 'orElse',\n",
       " 'par',\n",
       " 'parCombiner',\n",
       " 'partition',\n",
       " 'product',\n",
       " 'reduce',\n",
       " 'reduceLeft',\n",
       " 'reduceLeftOption',\n",
       " 'reduceOption',\n",
       " 'reduceRight',\n",
       " 'reduceRightOption',\n",
       " 'repr',\n",
       " 'reversed',\n",
       " 'runWith',\n",
       " 'sameElements',\n",
       " 'scan',\n",
       " 'scanLeft',\n",
       " 'scanRight',\n",
       " 'seq',\n",
       " 'size',\n",
       " 'sizeHintIfCheap',\n",
       " 'slice',\n",
       " 'sliceWithKnownBound',\n",
       " 'sliceWithKnownDelta',\n",
       " 'sliding',\n",
       " 'span',\n",
       " 'splitAt',\n",
       " 'stringPrefix',\n",
       " 'sum',\n",
       " 'tail',\n",
       " 'tails',\n",
       " 'take',\n",
       " 'takeRight',\n",
       " 'takeWhile',\n",
       " 'thisCollection',\n",
       " 'to',\n",
       " 'toArray',\n",
       " 'toBuffer',\n",
       " 'toCollection',\n",
       " 'toIndexedSeq',\n",
       " 'toIterable',\n",
       " 'toIterator',\n",
       " 'toList',\n",
       " 'toMap',\n",
       " 'toSeq',\n",
       " 'toSet',\n",
       " 'toStream',\n",
       " 'toString',\n",
       " 'toTraversable',\n",
       " 'toVector',\n",
       " 'transform',\n",
       " 'transpose',\n",
       " 'unzip',\n",
       " 'unzip3',\n",
       " 'updated',\n",
       " 'values',\n",
       " 'valuesIterator',\n",
       " 'view',\n",
       " 'wait',\n",
       " 'withDefault',\n",
       " 'withDefaultValue',\n",
       " 'withFilter',\n",
       " 'zip',\n",
       " 'zipAll',\n",
       " 'zipWithIndex']"
      ]
     },
     "execution_count": 524,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(deltaLog.metadata().configuration())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5b32a3-5872-466b-915e-8b3fb5763902",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "id": "e5ec8156-e3e7-48ef-a159-35e4eb0c3929",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[col_name: string, data_type: string, comment: string]"
      ]
     },
     "execution_count": 533,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = \"deltaTable\"\n",
    "sparkSession.sql(f\"DROP TABLE IF EXISTS {table}\")\n",
    "sparkSession.sql(f\"CREATE TABLE {table}(a LONG, b String NOT NULL) USING delta\")\n",
    "sparkSession.sql(f\"DESCRIBE TABLE {table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "id": "f4ca9caa-8dc7-4806-bf17-bad911c7c288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+-------+\n",
      "|       col_name|data_type|comment|\n",
      "+---------------+---------+-------+\n",
      "|              a|   bigint|       |\n",
      "|              b|   string|       |\n",
      "|               |         |       |\n",
      "| # Partitioning|         |       |\n",
      "|Not partitioned|         |       |\n",
      "+---------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkSession.sql(f\"DESCRIBE TABLE {table}\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "id": "3248a54c-1152-4748-a0d4-b5990eacc7cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Type</td>\n",
       "      <td>MANAGED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>delta.minReaderVersion</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>delta.minWriterVersion</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      key    value\n",
       "0                    Type  MANAGED\n",
       "1  delta.minReaderVersion        1\n",
       "2  delta.minWriterVersion        2"
      ]
     },
     "execution_count": 536,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkSession.sql(f\"SHOW TBLPROPERTIES {table}\").toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ab4ca1-fb70-4d86-86c7-f297582aed43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "id": "95b484ab-542c-4386-bcb8-ab5319feb4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspaces/crypto-data/jupyter-pod/ml-training-jupyter-notebooks/Data Engineering/Data Lakehouse/Deltalake\n"
     ]
    }
   ],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "id": "4ca7b394-a9c5-427a-8fca-29818e520f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A   B   C\n",
       "0  1   6  11\n",
       "1  2   7  12\n",
       "2  3   8  13\n",
       "3  4   9  14\n",
       "4  5  10  15"
      ]
     },
     "execution_count": 544,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = \"/workspaces/crypto-data/jupyter-pod/ml-training-jupyter-notebooks/Data Engineering/Data Lakehouse/Deltalake/data/time-travel-demo.delta\"\n",
    "sparkSession.sql(f\"SELECT * FROM delta.`{p}`\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "id": "5c770b44-395c-47db-8be9-ddcabf4e314a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>delta.minReaderVersion</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>delta.minWriterVersion</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      key value\n",
       "0  delta.minReaderVersion     1\n",
       "1  delta.minWriterVersion     2"
      ]
     },
     "execution_count": 556,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkSession.sql(f\"SHOW TBLPROPERTIES delta.`{p}`\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "id": "42002ca6-dd12-4fc4-8e1c-1d8e19f43eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 563,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_retention_days=36000000\n",
    "data_rentention_days=36000000\n",
    "\n",
    "sparkSession.sql(f\"\"\"\n",
    "alter table delta.`{p}` set TBLPROPERTIES (\n",
    "  delta.logRetentionDuration = 'interval {log_retention_days} days',\n",
    "  delta.deletedFileRetentionDuration = 'interval {data_rentention_days} days'\n",
    ")\n",
    "\"\"\")      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "id": "50842c8a-1d6d-49e6-8835-a3d5bc4298b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>delta.logRetentionDuration</td>\n",
       "      <td>interval 36000000 days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>delta.minReaderVersion</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>delta.minWriterVersion</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>delta.deletedFileRetentionDuration</td>\n",
       "      <td>interval 36000000 days</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  key                   value\n",
       "0          delta.logRetentionDuration  interval 36000000 days\n",
       "1              delta.minReaderVersion                       1\n",
       "2              delta.minWriterVersion                       2\n",
       "3  delta.deletedFileRetentionDuration  interval 36000000 days"
      ]
     },
     "execution_count": 564,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkSession.sql(f\"SHOW TBLPROPERTIES delta.`{p}`\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8c546e-1b8f-4eb7-ae77-44ba5c28eae3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c56d035-7920-421f-811a-0f1b03fa8e3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de49d51-f665-4c4b-8b4f-248cae8ed88c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30ffe327-b431-4f46-96b5-92cda4a25735",
   "metadata": {},
   "source": [
    "###set retention policy\n",
    "\n",
    "https://stackoverflow.com/questions/64274314/deltalake-how-to-time-travel-infinitely-across-datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "26c19e18-f1d4-40b5-b4e4-b6bbdc249e71",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DeltaTable' object has no attribute 'logRetentionDuration'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [497]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdelta\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtables\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DeltaTable\n\u001b[1;32m      2\u001b[0m dt2 \u001b[38;5;241m=\u001b[39m DeltaTable\u001b[38;5;241m.\u001b[39mforPath(sparkSession, delta_table_path)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mdeltaTable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogRetentionDuration\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DeltaTable' object has no attribute 'logRetentionDuration'"
     ]
    }
   ],
   "source": [
    "from delta.tables import DeltaTable\n",
    "dt2 = DeltaTable.forPath(sparkSession, delta_table_path)\n",
    "deltaTable.logRetentionDuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "id": "5bfd6963-1ade-49a2-96da-a118a14ae2e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/time-travel-demo.delta'"
      ]
     },
     "execution_count": 498,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_table_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "id": "d5369fce-7e94-4cb0-a450-e7433d23f2f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Table or view not found for 'SHOW TBLPROPERTIES': delta.3af5efe7-073f-4692-9e7b-51d76ced0146; line 2 pos 0;\n'ShowTableProperties\n+- 'UnresolvedTableOrView [delta, 3af5efe7-073f-4692-9e7b-51d76ced0146], SHOW TBLPROPERTIES, true\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [500]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# `3af5efe7-073f-4692-9e7b-51d76ced0146`\u001b[39;00m\n\u001b[1;32m      3\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124mSHOW TBLPROPERTIES delta.`3af5efe7-073f-4692-9e7b-51d76ced0146`;\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43msparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m result\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pyspark/sql/session.py:723\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    707\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msql\u001b[39m(\u001b[38;5;28mself\u001b[39m, sqlQuery):\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;124;03m\"\"\"Returns a :class:`DataFrame` representing the result of the given query.\u001b[39;00m\n\u001b[1;32m    709\u001b[0m \n\u001b[1;32m    710\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 2.0.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;124;03m    [Row(f1=1, f2='row1'), Row(f1=2, f2='row2'), Row(f1=3, f2='row3')]\u001b[39;00m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 723\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrapped)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Table or view not found for 'SHOW TBLPROPERTIES': delta.3af5efe7-073f-4692-9e7b-51d76ced0146; line 2 pos 0;\n'ShowTableProperties\n+- 'UnresolvedTableOrView [delta, 3af5efe7-073f-4692-9e7b-51d76ced0146], SHOW TBLPROPERTIES, true\n"
     ]
    }
   ],
   "source": [
    "# `3af5efe7-073f-4692-9e7b-51d76ced0146`\n",
    "\n",
    "query = f\"\"\"\n",
    "SHOW TBLPROPERTIES delta.`3af5efe7-073f-4692-9e7b-51d76ced0146`;\n",
    "\"\"\"\n",
    "\n",
    "result = sparkSession.sql(query)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4953896d-4e98-44ce-abf3-16a2821c8dba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6371b74a-cc2d-45a3-be9d-bc7d72f896ba",
   "metadata": {},
   "source": [
    "# Restoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616bba15-e033-4ceb-8887-37438e5891e3",
   "metadata": {},
   "source": [
    "https://docs.delta.io/latest/delta-utility.html#restore-a-delta-table-to-an-earlier-state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
