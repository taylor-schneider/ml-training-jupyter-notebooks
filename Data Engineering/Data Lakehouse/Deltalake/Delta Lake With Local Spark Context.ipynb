{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "066c5d35-f6a8-4e4a-a966-39a481ff865d",
   "metadata": {},
   "source": [
    "# Overview\n",
    "In previous notebooks we explored [Apache Spark](../../../Machine%20Learning/Big%20Data%20And%20Big%20Compute/Apache%20Spark/README.md) and creating a [Local Spark Context](../../../Machine%20Learning/Big%20Data%20And%20Big%20Compute/Apache%20Spark/Create%20A%20SparkContext%20For%20Locally%20Hosted%20Cluster.ipynb). In this notebook we will look at getting Spark to use Deltal Lake hosted on a local file system. We will also gain an understanding for how Spark data is stored in the Delta Lake and what the Delta Lake is under the hood."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a090b115-e897-4f2c-a48b-7654fc38481f",
   "metadata": {},
   "source": [
    "# 1. Create The Spark Conf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f285fe5-f9c0-4388-a884-7a0cc9a750cf",
   "metadata": {},
   "source": [
    "## 1.1. Ensure Spark Is In The Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd73004f-cc98-4b2d-a4f3-7b9a64a4753d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f793c4f-b59a-4cde-9e50-88dcc5dae877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/usr/lib/spark-3.1.1-bin-hadoop2.7/python', '/usr/lib/spark-3.1.1-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip', '/root/ml-training-jupyter-notebooks/Data Engineering/Data Lakehouse/Deltalake', '/usr/local/lib/python39.zip', '/usr/local/lib/python3.9', '/usr/local/lib/python3.9/lib-dynload', '', '/usr/local/lib/python3.9/site-packages', '/root/ml-training-jupyter-notebooks/Utilities']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d8f33f-1e99-4afa-934b-4b48acd5d254",
   "metadata": {},
   "source": [
    "## 1.2. Create The SparkConf Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "453bde04-e04a-4e74-b6da-100f71cb89df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "997c5129-3cf0-4326-83c7-d2eb470d49d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x7f6b82ff0e80>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkConf = pyspark.SparkConf()\n",
    "sparkConf.setAppName(\"delta-demo\")\n",
    "sparkConf.set(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "sparkConf.set(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7e7ab0-b96d-4717-a396-791319cb7761",
   "metadata": {},
   "source": [
    "## 1.3. Create The Spark SessionBuilder Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96ef1448-8398-4e03-95f9-7b109ab1bc94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.session.SparkSession.Builder at 0x7f6b830ad2b0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkSessionBuilder = pyspark.sql.SparkSession.builder.config(conf=sparkConf)\n",
    "sparkSessionBuilder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20ee91c-be00-4360-b2b8-b1e9625f387a",
   "metadata": {},
   "source": [
    "## 1.4. Create the SparkSession Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "077197d7-94d2-4714-bcec-6466f4c8b404",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/30 20:36:36 WARN Utils: Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 15.4.12.12 instead (on interface eth0)\n",
      "22/03/30 20:36:36 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/lib/spark-3.1.1-bin-hadoop2.7/jars/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/lib/spark-3.1.1-bin-hadoop2.7/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-26282663-ba4b-4c18-8ef3-77e5dd6605c1;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;1.0.1 in central\n",
      "\tfound org.antlr#antlr4;4.7 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.7 in central\n",
      "\tfound org.antlr#antlr-runtime;3.5.2 in central\n",
      "\tfound org.antlr#ST4;4.0.8 in central\n",
      "\tfound org.abego.treelayout#org.abego.treelayout.core;1.0.3 in central\n",
      "\tfound org.glassfish#javax.json;1.0.4 in central\n",
      "\tfound com.ibm.icu#icu4j;58.2 in central\n",
      "downloading https://repo1.maven.org/maven2/io/delta/delta-core_2.12/1.0.1/delta-core_2.12-1.0.1.jar ...\n",
      "\t[SUCCESSFUL ] io.delta#delta-core_2.12;1.0.1!delta-core_2.12.jar (168ms)\n",
      "downloading https://repo1.maven.org/maven2/org/antlr/antlr4/4.7/antlr4-4.7.jar ...\n",
      "\t[SUCCESSFUL ] org.antlr#antlr4;4.7!antlr4.jar (82ms)\n",
      "downloading https://repo1.maven.org/maven2/org/antlr/antlr4-runtime/4.7/antlr4-runtime-4.7.jar ...\n",
      "\t[SUCCESSFUL ] org.antlr#antlr4-runtime;4.7!antlr4-runtime.jar (32ms)\n",
      "downloading https://repo1.maven.org/maven2/org/antlr/antlr-runtime/3.5.2/antlr-runtime-3.5.2.jar ...\n",
      "\t[SUCCESSFUL ] org.antlr#antlr-runtime;3.5.2!antlr-runtime.jar (26ms)\n",
      "downloading https://repo1.maven.org/maven2/org/antlr/ST4/4.0.8/ST4-4.0.8.jar ...\n",
      "\t[SUCCESSFUL ] org.antlr#ST4;4.0.8!ST4.jar (31ms)\n",
      "downloading https://repo1.maven.org/maven2/org/abego/treelayout/org.abego.treelayout.core/1.0.3/org.abego.treelayout.core-1.0.3.jar ...\n",
      "\t[SUCCESSFUL ] org.abego.treelayout#org.abego.treelayout.core;1.0.3!org.abego.treelayout.core.jar(bundle) (23ms)\n",
      "downloading https://repo1.maven.org/maven2/org/glassfish/javax.json/1.0.4/javax.json-1.0.4.jar ...\n",
      "\t[SUCCESSFUL ] org.glassfish#javax.json;1.0.4!javax.json.jar(bundle) (38ms)\n",
      "downloading https://repo1.maven.org/maven2/com/ibm/icu/icu4j/58.2/icu4j-58.2.jar ...\n",
      "\t[SUCCESSFUL ] com.ibm.icu#icu4j;58.2!icu4j.jar (573ms)\n",
      ":: resolution report :: resolve 6559ms :: artifacts dl 986ms\n",
      "\t:: modules in use:\n",
      "\tcom.ibm.icu#icu4j;58.2 from central in [default]\n",
      "\tio.delta#delta-core_2.12;1.0.1 from central in [default]\n",
      "\torg.abego.treelayout#org.abego.treelayout.core;1.0.3 from central in [default]\n",
      "\torg.antlr#ST4;4.0.8 from central in [default]\n",
      "\torg.antlr#antlr-runtime;3.5.2 from central in [default]\n",
      "\torg.antlr#antlr4;4.7 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.7 from central in [default]\n",
      "\torg.glassfish#javax.json;1.0.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   8   |   8   |   8   |   0   ||   8   |   8   |\n",
      "\t---------------------------------------------------------------------\n",
      "\n",
      ":: problems summary ::\n",
      ":::: ERRORS\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/org/sonatype/oss/oss-parent/9/oss-parent-9.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/org/antlr/antlr4-master/4.7/antlr4-master-4.7.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/org/antlr/antlr-master/3.5.2/antlr-master-3.5.2.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/org/sonatype/oss/oss-parent/7/oss-parent-7.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/net/java/jvnet-parent/3/jvnet-parent-3.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/org/glassfish/json/1.0.4/json-1.0.4.jar\n",
      "\n",
      "\n",
      ":: USE VERBOSE OR DEBUG MESSAGE LEVEL FOR MORE DETAILS\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-26282663-ba4b-4c18-8ef3-77e5dd6605c1\n",
      "\tconfs: [default]\n",
      "\t8 artifacts copied, 0 already retrieved (15453kB/102ms)\n",
      "22/03/30 20:36:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://15.4.12.12:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>delta-demo</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f6b82e6f0d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkSession = delta.configure_spark_with_delta_pip(sparkSessionBuilder).getOrCreate()\n",
    "sparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c888b7ea-490c-49cb-be9b-856de2076843",
   "metadata": {},
   "source": [
    "In the output of the cell above we can see that the creation of the SparkSession object cause the spark/java framework to download the maven dependencies from the official repository and store them in a local directoy (/root/.ivy2/jars).\n",
    "\n",
    "While this is a niceuser feature, in production one may want to configure their system to use a private maven repo/cache like artifactory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e91b64b-3902-4f60-acad-d2e5f3407949",
   "metadata": {},
   "source": [
    "# 2. Create And Store A Spark DataFrame\n",
    "Being builton top of Data Lakes, the Delta Lake is made up of raw files. As an abstraction layer the Delta Lake will manage the translation of objects in memory to files in our data store or file system. As an analogy, consider excel; Excel maps objects, like a worksheet, a tab, a cell to a file on the file system. We will see that Delta Lake also provides this type of mapping in addition to a number of other features.\n",
    "\n",
    "We will cover this more in a bit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070cf39f-134e-4a12-b046-007e5648f661",
   "metadata": {},
   "source": [
    "## 2.1. Create A Spark DataFrame\n",
    "We will use pandas as an intermediary when creating the DataFrame because the API is a bit easier to use and will keep us focused on delta lake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b53a5eee-3781-434b-a025-86fa5330daf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|my_column|\n",
      "+---------+\n",
      "|        2|\n",
      "|        4|\n",
      "|        6|\n",
      "|        8|\n",
      "|       10|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "spark_df = sparkSession.createDataFrame(pandas.DataFrame({\"my_column\": [2,4,6,8,10]}))\n",
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5277bae-4e57-4c86-ad27-b9a0683033d8",
   "metadata": {},
   "source": [
    "## 2.2. Determine Where to Save the Spark DataFrame\n",
    "We will leverage the pyprojroot library which will tell us the path to this repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12bbac0e-2cd4-4bf2-acb9-aab9f325621d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/ml-training-jupyter-notebooks\n"
     ]
    }
   ],
   "source": [
    "import pyprojroot\n",
    "project_root_dir  = pyprojroot.here()\n",
    "print(project_root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9e917241-cd57-4889-b6b8-bc61a42f5661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/ml-training-jupyter-notebooks/Example Data Sets/my-table\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "delta_file_name = \"my-table\"\n",
    "data_directory = os.path.join(project_root_dir, \"Example Data Sets\")\n",
    "delta_table_path = os.path.join(data_directory, delta_file_name)\n",
    "print(delta_table_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c495829-39d0-4415-96d6-791f9a9a3534",
   "metadata": {},
   "source": [
    "## 2.3. Save Spark DataFrame To Detla Lake\n",
    "When saving the DataFrame to Delta Lake is is common to refer to the \"thing\" that is written to the storage layer as a \"**Delta Table**\". This is because the data can be read using a number of APIs besides Spark. Thinking about the Excel analogy, a CSV file can be read by Notepad, Excel, Open Office, and more. Delta Tables are the same way.\n",
    "\n",
    "We simply need to specify a path and use methods attached to native Spark objects. These methods are attached dynamically based on the additional configurations we made to our SparkConf object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "726acbb7-78a4-477e-816f-6362d84d17e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "file:/root/ml-training-jupyter-notebooks/Example Data Sets/my-table already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [39]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdelta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelta_table_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/readwriter.py:1109\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1107\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave()\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1109\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/spark-3.1.1-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/lib/spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: file:/root/ml-training-jupyter-notebooks/Example Data Sets/my-table already exists."
     ]
    }
   ],
   "source": [
    "spark_df.write.format(\"delta\").save(delta_table_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d702fa4b-a438-4ab0-ae70-8a85ddc68ad4",
   "metadata": {},
   "source": [
    "## 2.4. Examine The Raw Delta Table\n",
    "We will use a shell command to show Delta Table we just wrote to our filesystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b0aa4a98-6574-4f2c-a455-501c77e74249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 31516\n",
      "drwxr-xr-x.  4 root root      152 Mar 31 01:43 .\n",
      "drwxr-xr-x. 11 root root     4096 Mar 28 16:59 ..\n",
      "-rw-r--r--.  1 root root       12 Mar 28 16:51 .gitignore\n",
      "drwxr-xr-x.  2 root root        6 Mar 31 01:33 .ipynb_checkpoints\n",
      "-rw-r--r--.  1 root root      216 Feb 14 17:46 Test Scores.csv\n",
      "-rw-r--r--.  1 root root       24 Mar 28 16:51 demo_data.csv\n",
      "drwxr-xr-x.  3 root root     4096 Mar 31 01:43 my-table\n",
      "-rw-r--r--.  1 root root 32247139 Feb 14 17:46 nasdaq_2019.csv\n",
      "-rw-r--r--.  1 root root     1660 Mar  8 23:01 results.csv\n"
     ]
    }
   ],
   "source": [
    "! ls -la \"$data_directory\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6dfe5064-a0a3-4902-b176-a90fc0fe8860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 36\n",
      "drwxr-xr-x. 3 root root 4096 Mar 31 01:43 .\n",
      "drwxr-xr-x. 4 root root  152 Mar 31 01:43 ..\n",
      "-rw-r--r--. 1 root root   12 Mar 31 01:43 .part-00000-8ba0f257-17e0-425a-bede-64131a226be8-c000.snappy.parquet.crc\n",
      "-rw-r--r--. 1 root root   12 Mar 31 01:43 .part-00001-5c499980-b76d-46f7-8442-23f7bf7d6cc1-c000.snappy.parquet.crc\n",
      "-rw-r--r--. 1 root root   12 Mar 31 01:43 .part-00002-005ab50c-bcf1-481c-b5c6-8790e810fdcb-c000.snappy.parquet.crc\n",
      "-rw-r--r--. 1 root root   12 Mar 31 01:43 .part-00003-71db195a-baed-4db2-b689-1bf196b27416-c000.snappy.parquet.crc\n",
      "drwxr-xr-x. 2 root root   39 Mar 31 01:43 _delta_log\n",
      "-rw-r--r--. 1 root root  484 Mar 31 01:43 part-00000-8ba0f257-17e0-425a-bede-64131a226be8-c000.snappy.parquet\n",
      "-rw-r--r--. 1 root root  484 Mar 31 01:43 part-00001-5c499980-b76d-46f7-8442-23f7bf7d6cc1-c000.snappy.parquet\n",
      "-rw-r--r--. 1 root root  484 Mar 31 01:43 part-00002-005ab50c-bcf1-481c-b5c6-8790e810fdcb-c000.snappy.parquet\n",
      "-rw-r--r--. 1 root root  492 Mar 31 01:43 part-00003-71db195a-baed-4db2-b689-1bf196b27416-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "! ls -la \"$delta_file_path\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929d680a-a7b9-4267-8641-e346a70a9c32",
   "metadata": {},
   "source": [
    "We can see that the DataFrame is split into chunks and stored compressed parquet files contained in a folder on our file system. [Apache Parquet](https://en.wikipedia.org/wiki/Apache_Parquet) is a free and open-source column-oriented data storage format of the Apache Hadoop ecosystem. Snappy is a [supported compression algorithm](https://spark.apache.org/docs/latest/sql-data-sources-parquet.html) that Spark can use to make the parquet files smaller on disk. Additionally, the Delta Table directory also container CRC files which help the program splitting data into chunks to confirm data was put back together correctly. CRCs are used by a number of Archival programs becised Delta Lake."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4e6c12-3dc0-41a7-b23d-aa3493b8dbc8",
   "metadata": {},
   "source": [
    "## 2.5. Read Delta Table Into Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "789742a9-5635-4cd1-8bc8-c48cdf0d1949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|my_column|\n",
      "+---------+\n",
      "|        8|\n",
      "|       10|\n",
      "|        4|\n",
      "|        2|\n",
      "|        6|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df = sparkSession.read.format(\"delta\").load(delta_table_path)\n",
    "new_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
