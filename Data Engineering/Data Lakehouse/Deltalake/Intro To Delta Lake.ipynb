{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68990e1a-1083-4ea7-aa47-6a825be68dbe",
   "metadata": {},
   "source": [
    "# Overview\n",
    "As we saw in [Intro To The Data Lakehouse Architecture Notebook](../Intro%20To%20Data%20Lakehouse.ipynb), the Databricks Delta Lake is an open source implimentation of the thrid generation archiecture for a data store. It seeks to provide the best features of both data warehouses and data lakes. \n",
    "\n",
    "Delta Lake is implimented as a \"storage layer\" that sits on top of a Data Lake (generation II implimentation). Recall that a Data Lake is essentially a storage system for files (ie. a file system). So the delta lake is essentially an api and a collection of special files stored in a delta lake. These files, called Delta Tables provide a mechanism to store Spark DataFrames in a repository that tracks the evolution of the data over time. This feature is called time travel and we will cover this feature and the inner workings of a Delta Table in a separate [Detla Lake Time Travel Notebook](Delta%20Lake%20Time%20Travel.ipynb)).\n",
    "\n",
    "As we will see, Delta Lake is tightly integrated with spark. So when we talk about a storage layer, what are we talking about? Basically the delta lake is a way for us to efficiently store and manage versions of Spark DataFrames to the hard drive. Delta Lake provides plugins and extensions that work wich spark to allow us to perform the necessary CRUD operations. We will look at this later.\n",
    "\n",
    "Some popular datastores use an API rather than a POSIX filesystem mount point, but this is beyond the scope. The important think here is that Delta Lake sits on top of a Data Lake.\n",
    "\n",
    "There are a number of popular cloud based Data Lake providers that are [supported](https://docs.delta.io/latest/delta-storage.html) including: \n",
    "\n",
    "- Amazon S3\n",
    "- Microsoft Azure storage\n",
    "- HDFS\n",
    "- Google Cloud Storage\n",
    "- Oracle Cloud Infrastructure\n",
    "- IBM Cloud Object Storage\n",
    "\n",
    "But, one can also use a traditional POSIX file system (ie. your C:/ drive or any drive your operating system recognizes), to host your data lake. In my case I am using Ceph as my self-hosted infinitely scalable file system but that is another discussion.\n",
    "\n",
    "For the notebooks in this directory we will use a subdirectory as the root of our data/delta lake."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00379cd9-9933-4f2e-9978-a0710cc0a785",
   "metadata": {},
   "source": [
    "## Features\n",
    "According to the [documentation](https://docs.delta.io/latest/delta-intro.html), Delta Lake offers the following features:\n",
    "\n",
    "- ACID transactions on Spark: Serializable isolation levels ensure that readers never see inconsistent data.\n",
    "- Scalable metadata handling: Leverages Spark distributed processing power to handle all the metadata for petabyte-scale tables with billions of files at ease.\n",
    "- Streaming and batch unification: A table in Delta Lake is a batch table as well as a streaming source and sink. Streaming data ingest, batch historic backfill, interactive queries all just work out of the box.\n",
    "- Schema enforcement: Automatically handles schema variations to prevent insertion of bad records during ingestion.\n",
    "- Time travel: Data versioning enables rollbacks, full historical audit trails, and reproducible machine learning experiments.\n",
    "- Upserts and deletes: Supports merge, update and delete operations to enable complex use cases like change-data-capture, slowly-changing-dimension (SCD) operations, streaming upserts, and so on.\n",
    "\n",
    "In this notebook we will put hands on keyboard to understand these features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84cc872-4abc-4b6b-9154-9563f91d27b9",
   "metadata": {},
   "source": [
    "# 1. Installing Software\n",
    "The Delta Lake is tightly integrated with Apache Spark. Having a look at the [quick start guide](https://docs.delta.io/latest/quick-start.html#set-up-apache-spark-with-delta-lake) we see that Apache Spark (and pyspark if using python) is the main interface for interacting with Delta Lake.\n",
    "\n",
    "\n",
    "\n",
    "Taking a deeper look at the [github page](https://github.com/delta-io/delta) we see that:\n",
    "> Delta Lake is a storage layer that brings scalable, ACID transactions to Apache Spark and other big-data engines.\n",
    "\n",
    "In this section there will be a lot of mentioning spark so if you are not familiar with Apache Spark, I would reccomend reviewing [introductory material](../../../Machine%20Learning/Big%20Data%20And%20Big%20Compute/Apache%20Spark/README.md) in this repository"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2133681-1477-4c9b-9306-defbe6e51097",
   "metadata": {},
   "source": [
    "## 1.1 Install Apache Spark And Pyspark\n",
    "As mentioned in the [introductory material](../../../Machine%20Learning/Big%20Data%20And%20Big%20Compute/Apache%20Spark/README.md) we are running on Spark 3.1.1. Consult this material for information regarding the installation of Spark or pyspark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dc9f80-e5ed-44c0-9137-e31bb8f9309e",
   "metadata": {},
   "source": [
    "## 2.1. Install Delta Lake Packages\n",
    "The documentation was a bit sparse on installing the Delta Lake software. The first thing to decide is which version. According to the [documentation](https://docs.delta.io/latest/releases.html#compatibility-with-apache-spark) we have the following compatability matrix.\n",
    "\n",
    "\n",
    "<table border=\"1\" class=\"docutils\">\n",
    "<colgroup>\n",
    "<col width=\"41%\">\n",
    "<col width=\"59%\">\n",
    "</colgroup>\n",
    "<thead valign=\"bottom\">\n",
    "<tr class=\"row-odd\"><th class=\"head\">Delta Lake version</th>\n",
    "<th class=\"head\">Apache Spark version</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody valign=\"top\">\n",
    "<tr class=\"row-even\"><td> 1.1.x</td>\n",
    "<td> 3.2.x</td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td> 1.0.x</td>\n",
    "<td> 3.1.x</td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td> 0.7.x and 0.8.x</td>\n",
    "<td> 3.0.x</td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td> Below 0.7.0</td>\n",
    "<td> 2.4.2 - 2.4.<em>&lt;latest&gt;</em></td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0993ba27-e113-4bac-8c86-eac048539014",
   "metadata": {},
   "source": [
    "As we have been using Spark 3.1.1 we will be installing Delta Lake 1.0.x.\n",
    "\n",
    "Delta Lake exits as a set of jar's that extend and stack on top of the Apache Spark stack."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da846bf-0ca6-44ab-9584-45a14247fa5b",
   "metadata": {},
   "source": [
    "### 2.1.1. Install delta-spark Python Library\n",
    "This PyPi package contains the Python APIs for using Delta Lake with Apache Spark. This package however does not include the related Scala jar files that are the core of the code base (recall Spark is written in Java/Scala). The jars related by Delta Lake will be fetched at runtime after adding specific configurations to the Spark Driver.\n",
    "\n",
    "\n",
    "For more information see the [pypi index](https://pypi.org/project/delta-spark/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "367df84b-72d8-4dde-bcbd-c32c476894b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://admin:****@15.4.8.11:8082/artifactory/api/pypi/crypto-data-pypi/simple\n",
      "Collecting delta-spark==1.0.1\n",
      "  Downloading http://15.4.8.11:8082/artifactory/api/pypi/crypto-data-pypi/packages/packages/38/93/fb4b1d5e92a35cf5dabf6c2961d6c6c5929f66d4531e2ca1b0eee0fb8628/delta_spark-1.0.1-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: pyspark<3.2.0,>=3.1.0 in /usr/local/lib/python3.10/site-packages (from delta-spark==1.0.1) (3.1.1)\n",
      "Requirement already satisfied: importlib-metadata>=3.10.0 in /usr/local/lib/python3.10/site-packages (from delta-spark==1.0.1) (4.11.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/site-packages (from importlib-metadata>=3.10.0->delta-spark==1.0.1) (3.8.0)\n",
      "Requirement already satisfied: py4j==0.10.9 in /usr/local/lib/python3.10/site-packages (from pyspark<3.2.0,>=3.1.0->delta-spark==1.0.1) (0.10.9)\n",
      "Installing collected packages: delta-spark\n",
      "Successfully installed delta-spark-1.0.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install delta-spark==1.0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570a45df-93a7-4d54-a4a7-ad13b379d7bc",
   "metadata": {},
   "source": [
    "# 2. Runnning Spark with Delta Lake integration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f09e33a-1815-40a8-a7d0-99086cea38bd",
   "metadata": {},
   "source": [
    "As mentioned previously, Delta Lake functionality is provided through Spark. In short, when we create our SparkSession, we will issue some configurations which instruct spark to load the necessary libraries for dealing with deltalake.\n",
    "\n",
    "To see this in action, review the [Delta Lake With Local Spark Context Notebook](Delta%20Lake%20With%20Local%20Spark%20Context.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
