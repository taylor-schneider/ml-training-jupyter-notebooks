{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68990e1a-1083-4ea7-aa47-6a825be68dbe",
   "metadata": {},
   "source": [
    "# Overview\n",
    "As we saw in [Intro To The Data Lakehouse Architecture Notebook](../Intro%20To%20Data%20Lakehouse.ipynb), the Databricks Delta Lake is an open source implimentation of the thrid generation archiecture for a data store. It seeks to provide the best of both data warehouses and data lakes. \n",
    "\n",
    "Delta Lake is implimented as a \"storage layer\" that sits on top of a Data Lake (generation II). Recall that a Data Lake is essentially a storage system for files (ie. a file system). Some popular datastores use an API rather than a POSIX filesystem mount point, but this is beyond the scope. The important think here is that Delta Lake sits on top of a Data Lake.\n",
    "\n",
    "There are a number of popular Data Lake providers that are [supported](https://docs.delta.io/latest/delta-storage.html) including: \n",
    "\n",
    "- Amazon S3\n",
    "- Microsoft Azure storage\n",
    "- HDFS\n",
    "- Google Cloud Storage\n",
    "- Oracle Cloud Infrastructure\n",
    "- IBM Cloud Object Storage\n",
    "\n",
    "But, one can also use a traditional POSIX file system, so we can basically choose any path mounted to our local file system. In my case I am using Ceph as my infinitely scalable file system but that is another discussion.\n",
    "\n",
    "For this notebook we will use a local filesystem path (a local directory) which points to a directory in this repository as our Data Lake. (More on this later)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00379cd9-9933-4f2e-9978-a0710cc0a785",
   "metadata": {},
   "source": [
    "## Features\n",
    "According to the [documentation](https://docs.delta.io/latest/delta-intro.html), Delta Lake offers the following features:\n",
    "\n",
    "- ACID transactions on Spark: Serializable isolation levels ensure that readers never see inconsistent data.\n",
    "- Scalable metadata handling: Leverages Spark distributed processing power to handle all the metadata for petabyte-scale tables with billions of files at ease.\n",
    "- Streaming and batch unification: A table in Delta Lake is a batch table as well as a streaming source and sink. Streaming data ingest, batch historic backfill, interactive queries all just work out of the box.\n",
    "- Schema enforcement: Automatically handles schema variations to prevent insertion of bad records during ingestion.\n",
    "- Time travel: Data versioning enables rollbacks, full historical audit trails, and reproducible machine learning experiments.\n",
    "- Upserts and deletes: Supports merge, update and delete operations to enable complex use cases like change-data-capture, slowly-changing-dimension (SCD) operations, streaming upserts, and so on.\n",
    "\n",
    "In this notebook we will put hands on keyboard to understand these features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84cc872-4abc-4b6b-9154-9563f91d27b9",
   "metadata": {},
   "source": [
    "# 1. Installing Software\n",
    "The Delta Lake is tightly integrated with Apache Spark. Having a look at the [quick start guide](https://docs.delta.io/latest/quick-start.html#set-up-apache-spark-with-delta-lake) we see that Apache Spark (and pyspark if using python) is the main interface for interacting with Delta Lake.\n",
    "\n",
    "\n",
    "\n",
    "Taking a deeper look at the [github page](https://github.com/delta-io/delta) we see that:\n",
    "> Delta Lake is a storage layer that brings scalable, ACID transactions to Apache Spark and other big-data engines.\n",
    "\n",
    "In this section there will be a lot of mentioning spark so if you are not familiar with Apache Spark, I would reccomend reviewing [introductory material](../../../Machine%20Learning/Big%20Data%20And%20Big%20Compute/Apache%20Spark/README.md) in this repository"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2133681-1477-4c9b-9306-defbe6e51097",
   "metadata": {},
   "source": [
    "## 1.1 Install Apache Spark And Pyspark\n",
    "As mentioned in the [introductory material](../../../Machine%20Learning/Big%20Data%20And%20Big%20Compute/Apache%20Spark/README.md) we are running on Spark 3.1.1. Consult this material for information regarding the installation of Spark or pyspark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dc9f80-e5ed-44c0-9137-e31bb8f9309e",
   "metadata": {},
   "source": [
    "## 2.1. Install Delta Lake Packages\n",
    "The documentation was a bit sparse on installing the Delta Lake software. The first thing to decide is which version. According to the [documentation](https://docs.delta.io/latest/releases.html#compatibility-with-apache-spark) we have the following compatability matrix.\n",
    "\n",
    "\n",
    "<table border=\"1\" class=\"docutils\">\n",
    "<colgroup>\n",
    "<col width=\"41%\">\n",
    "<col width=\"59%\">\n",
    "</colgroup>\n",
    "<thead valign=\"bottom\">\n",
    "<tr class=\"row-odd\"><th class=\"head\">Delta Lake version</th>\n",
    "<th class=\"head\">Apache Spark version</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody valign=\"top\">\n",
    "<tr class=\"row-even\"><td> 1.1.x</td>\n",
    "<td> 3.2.x</td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td> 1.0.x</td>\n",
    "<td> 3.1.x</td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td> 0.7.x and 0.8.x</td>\n",
    "<td> 3.0.x</td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td> Below 0.7.0</td>\n",
    "<td> 2.4.2 - 2.4.<em>&lt;latest&gt;</em></td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0993ba27-e113-4bac-8c86-eac048539014",
   "metadata": {},
   "source": [
    "As we have been using Spark 3.1.1 we will be installing Delta Lake 1.0.x.\n",
    "\n",
    "Delta Lake exits as a set of jar's that extend and stack on top of the Apache Spark stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e6d4b4-af67-45e9-a625-8665842e219f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7da846bf-0ca6-44ab-9584-45a14247fa5b",
   "metadata": {},
   "source": [
    "### 2.1.1. Install delta-spark Python Library\n",
    "This PyPi package contains the Python APIs for using Delta Lake with Apache Spark. This package however does not include the related Scala jar files that are the core of the code base (recall Spark is written in Java/Scala). The jars related by Delta Lake will be fetched at runtime after adding specific configurations to the Spark Driver.\n",
    "\n",
    "\n",
    "For more information see the [pypi index](https://pypi.org/project/delta-spark/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "367df84b-72d8-4dde-bcbd-c32c476894b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting delta-spark==1.0.1\n",
      "  Downloading delta_spark-1.0.1-py3-none-any.whl (17 kB)\n",
      "Collecting importlib-metadata>=3.10.0\n",
      "  Downloading importlib_metadata-4.11.3-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: pyspark<3.2.0,>=3.1.0 in /usr/local/lib/python3.9/site-packages (from delta-spark==1.0.1) (3.1.1)\n",
      "Collecting zipp>=0.5\n",
      "  Downloading zipp-3.7.0-py3-none-any.whl (5.3 kB)\n",
      "Requirement already satisfied: py4j==0.10.9 in /usr/local/lib/python3.9/site-packages (from pyspark<3.2.0,>=3.1.0->delta-spark==1.0.1) (0.10.9)\n",
      "Installing collected packages: zipp, importlib-metadata, delta-spark\n",
      "Successfully installed delta-spark-1.0.1 importlib-metadata-4.11.3 zipp-3.7.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.3; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install delta-spark==1.0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570a45df-93a7-4d54-a4a7-ad13b379d7bc",
   "metadata": {},
   "source": [
    "# 2. Runnning Spark with Delta Lake integration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4922d4-e37e-4c9a-96ab-f401cd576b58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed003c09-a923-4693-9a7a-9446b5955178",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3897baec-bedb-4327-a6fb-36733d00e7c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66922e22-269d-4c1b-a2fc-882bac801ce3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
