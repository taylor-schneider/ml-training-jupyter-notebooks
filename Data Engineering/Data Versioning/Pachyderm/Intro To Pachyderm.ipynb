{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a24ba544-1c15-472b-b3c2-9fba99aeafff",
   "metadata": {},
   "source": [
    "# Overview\n",
    "Pachyderm data version control system that runs on kubernetes. Like most VCS implimentations, it uses the client server model and provides a CLI to manipulating repositories.\n",
    "\n",
    "In this article we review the basic concepts and architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b6e6ee-6488-4955-ac49-85ceece956ab",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Basic Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cc491f-a7be-4f08-83ff-916ecf663fc1",
   "metadata": {},
   "source": [
    "Pachyderm is quite robust and can be used in a number of ways:\n",
    "\n",
    "- We can mount the pachyderm file system locally and manage it through the CLI like we would a git repository\n",
    "- We can use Pachyderm pipelines to automate data pipelines and automagically version our data\n",
    "\n",
    "Note: The UI is not designed to manipulate data and there is no CRUD functionality.\n",
    "\n",
    "Before looking at how pachyderm works in practice lets understand the basic concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d35a26-9947-4780-af32-0396608530f9",
   "metadata": {},
   "source": [
    "## The Pachyderm File System (PFS)\n",
    "\n",
    "The Pachyderm File System (PFS) is a loose collection of concepts, an abstraction, which together define the version control functionality. The Pachyderm application orchestrates and manages the data through abstractions which are ultimately stored in Postrgres and S3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4f8171-5690-4030-ad23-d8c9b3b2e3f1",
   "metadata": {},
   "source": [
    "## Repository\n",
    "\n",
    "A [Repository](https://docs.pachyderm.com/latest/concepts/data-concepts/repo/) is a logical container for data. It is conceptualy similar to a git repository. The repository stores our data as well as tracks the changes to the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e363871a-6d6e-405e-b19b-6fdf192250cf",
   "metadata": {},
   "source": [
    "## Versioning\n",
    "\n",
    "Data versioning (History) enables Pachyderm users to go back in time and see the state of a dataset or repository at a particular moment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b3bd74-2a3a-4ae1-8a32-7df7253e1fa3",
   "metadata": {},
   "source": [
    "## Provenance (Lineage)\n",
    "\n",
    "Data provenance (from the French noun provenance which means the place of origin), also known as data lineage, tracks the dependencies and relationships between datasets. It answers the question “Where does the data come from?”, but also “How was the data transformed along the way?”.\n",
    "\n",
    "Pachyderm provides provenance through it's pipeline orchestration. It assumes all data is tracked and transformed through pachyderm (or it's 3rd party integrations)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddf9ae1-653d-40c9-86d8-011a3f792518",
   "metadata": {},
   "source": [
    "## Commit\n",
    "\n",
    "A [commit](https://docs.pachyderm.com/latest/concepts/data-concepts/commit/) in Pachyderm is created automatically whenever data is added to or deleted from a repository. Each commit is an atomic operation which preserves the state of all files in the repository at the time of the commit, similar to a snapshot. Each commit is uniquely identifiable by a UUID and is immutable, meaning that the source data can never change.\n",
    "\n",
    "You can start a commit by running the pachctl start commit command with reference to a specific repository. After you’re done making changes to the repository (put file, delete file, …), you can finish your modifications by running the pachctl finish commit command. This command saves your changes and closes that repository’s commit, indicating the data is ready for processing by downstream pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d606894d-02b6-48d8-b2ff-2c410e182bf0",
   "metadata": {},
   "source": [
    "## Branch\n",
    "\n",
    "A Pachyderm branch is a pointer to a commit that moves along with new commits as they are submitted. In the diagram below we see an example of the pointer being updated below:\n",
    "\n",
    "<center><img src=\"images/pachyderm-branching.png\"><center>\n",
    "\n",
    "In Pachyderm, true merging is not implimented. A user must perform the merge by manually diffing and merging files in a branch, creating a new commit with the merge result, and then updating the branch pointer to point to that new commit. The documentation hosts a rationale for the design decision: basically the argument is that the process of merging binary data is data specific and in some cases does not make sense. For that reason, they have left the implimentation to the user:\n",
    "\n",
    "> The concept of merging binary data from different commits is complex. Ultimately, there are too many edge cases to do it reliably for every type of binary data, because computing a diff between two commits is ultimately meaningless unless you know how to compare the data. For example, we know that text files can be compared line-by-line or a bitmap image pixel by pixel, but how would we compute a diff for, say, binary model files?\n",
    "> \n",
    "> Additionally, the output of a merge is usually a master copy, the official set of files desired. We rarely combine multiple pieces of image data to make one image, and if we are, we have usually created a technique for doing so. In the end, some files will be deleted, some updated, and some added."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6f5be5-049b-41ca-9308-2f39d6392c90",
   "metadata": {},
   "source": [
    "## Job\n",
    "A [job](https://docs.pachyderm.com/latest/concepts/pipeline-concepts/job/) is an execution of a pipeline definition that triggers when new data (ie. a commit) is detected in an input repository."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817ca8dd-f344-4c3f-9ce8-4667f4d5b918",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "\n",
    "A [pipeline](https://docs.pachyderm.com/latest/concepts/pipeline-concepts/pipeline/) is a definition of a process or a transformation that is applied to data (repositories). The process is fully orchestrated by Pachyderm and executes on the underlying kubernetes infrastructure. The process may be defined such that it \"listens\" or \"subscribes\" to repositories and runs a job after witnessing a new commit. The pipeline is ultimately articulated through docker; the pipeline instructs which docker container is run and what command is executed inside the container. As pachyderm is built on kubernetes, the pipeline orchestrates pipeline runs accoss a pool of worker pods hosted on the platform. The pipeline automagically creates an output repository with the same name."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff92477-571f-4091-affd-5889faa48518",
   "metadata": {},
   "source": [
    "### Basic Workflow\n",
    "\n",
    "There are a number of different types of pipelines (Pachyderm calls these use cases). Generally speaking a pipeline has inputs and outputs. Both are mapped to local directories within the container executing the pipeline job.\n",
    "\n",
    "Then the pipeline runs, it runs to specified docker container. Before the container executes the specified command, Pachyderm will create two special directories automagically within the container.\n",
    "- /pfs/<input_repo_name> - stores the data from the input repository\n",
    "- /pfs/out - stores the output data from the transformation which pachyderm adds to the pipeline’s output repo as a new commit\n",
    "\n",
    "The basic workflow is described [here](https://docs.pachyderm.com/latest/getting-started/beginner-tutorial/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465039ad-7dc2-4e39-8f12-486c03df2bac",
   "metadata": {},
   "source": [
    "### Pachyderm Pipeline Specification (PPS)\n",
    "The pipeline is defined using a json file called the [pipeline specification](https://docs.pachyderm.com/latest/reference/pipeline-spec/#pipeline-specification). The following json attributes are required for all pipeline types, referred to as \"use cases\" while the rest depend on the specific use case being described:\n",
    "- pipeline.name\n",
    "- transform - the docker container and command executing the transformation\n",
    "\n",
    "Beyond those, other attributes are conditionally required based on your pipeline’s use case. Pachyderm currently supports the following use cases\n",
    "\n",
    "An example Input pipeline is as folows:\n",
    "\n",
    "```\n",
    "{\n",
    "  \"pipeline\": {\n",
    "    \"project\": 1,\n",
    "    \"name\": \"wordcount\"\n",
    "  },\n",
    "  \"transform\": {\n",
    "    \"image\": \"wordcount-image\",\n",
    "    \"cmd\": [\"/binary\", \"/pfs/data\", \"/pfs/out\"]\n",
    "  },\n",
    "  \"input\": {\n",
    "        \"pfs\": {\n",
    "            \"repo\": \"data\",\n",
    "            \"glob\": \"/*\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015f1223-99fe-4db5-8e7d-dfe627509980",
   "metadata": {},
   "source": [
    "#### Pipeline Use Cases\n",
    "\n",
    "Pachyderm allows for a number of different types pipelines to be defined through the PPS. With each Use case, the PPS will vary and the use cases may sub device into more than one implimentation. The broad categories of pipeline types are as follows: \n",
    "\n",
    "- **Cron** - triggers based on a Cron event rather than a commit event\n",
    "- **Egress** - Pushes the results of a pipeline to an external object store (such as Amazon S3, Google Cloud Storage, or Azure Blob Storage) or a SQL Database (such as Snowflake, postgresql, etc.).\n",
    "- **Input** - Reorganizes the files in a repo according to some predefined logic\n",
    "- **Service** - Exposes the transform container as an serving endpoint\n",
    "- **Spout** - A spout is a type of pipeline that ingests streaming data from an outside source (message queue, database transactions logs, event notifications… \n",
    "- **S3** - Writes results to s3 object store rather than pachyderm repo. Duplicate to Egress\n",
    "\n",
    "Example use cases can be found [here](https://docs.pachyderm.com/latest/reference/pipeline-spec/#pipeline-specification-pps-minimal-spec) and a complete list of PPS can be found [here](https://docs.pachyderm.com/series/pps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc84275f-9e15-4865-bc2d-9a7ba27c1c76",
   "metadata": {},
   "source": [
    "##### Spout\n",
    "\n",
    "A [spout](https://docs.pachyderm.com/latest/concepts/pipeline-concepts/pipeline/spout/) is a type of pipeline that ingests streaming data from an outside source (message queue, database transactions logs, event notifications… ) as schematized in the diagram below.\n",
    "\n",
    "<center><img src=\"images/pachyderm-spout.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8237c41-b28c-4518-b0d3-ea89b8d9c553",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Example Processing Datum\n",
    "Pipelines can be chained together to form an event based DAG. In the [example](https://docs.pachyderm.com/latest/concepts/pipeline-concepts/datum/relationship-between-datums/#datum-processing-example-two-steps-mapreduce-pattern-and-single-datum-provenance-rule) shown below, we highlight a two pipelines pattern where a first pipeline’s glob pattern splits an incoming commit into three datums (called “Datum1” (Red), “Datum2” (Blue), “Datum3” (Purple)), each producing two files each. The files can then be further appended or overwritten with other files to create the final result. Below, a second pipeline appends the content of all files in each directory into one final document.\n",
    "\n",
    "<center><img src=\"images/pachyderm-pipeline-chain.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73087e31-2570-46d4-844b-19b3d4688c48",
   "metadata": {},
   "source": [
    "## Datum\n",
    "\n",
    "A datum is a Pachyderm abstraction that helps in optimizing pipeline processing and tracking data lineage.\n",
    "\n",
    "Datums are a representations of \"units of work\"; things that a pipeline needs to operate on; collection of files. A datum is the smallest indivisible unit of computation within a job. A job can have one, many or no datums. We can use datums to break up a large data set consisting of many files into a set of smaller data sets which can be processed in parallel.\n",
    "\n",
    "In the simplest case, a user will want to define a pipeline that executes on a single repository. For this pachyderm provides\n",
    "- PFS Inputs\n",
    "\n",
    "In more complex cases, the user may want to combine files from multiple repositories or may want to group files into buckets based on some naming convention. For these tasks, pachyderm provides the following:\n",
    "- Cross & Union Inputs\n",
    "- Group Input\n",
    "- Join Input\n",
    "\n",
    "Note: Datums exist only as a pipeline processing property and are not filesystem objects. You can never copy a datum. \n",
    "\n",
    "For more information see the documentation [here](https://docs.pachyderm.com/latest/concepts/pipeline-concepts/datum/relationship-between-datums/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8fa796-5061-4ae6-bd9e-221a5f7df86f",
   "metadata": {},
   "source": [
    "### Datum Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bddbd52-45dd-4c64-8835-570f292690c9",
   "metadata": {},
   "source": [
    "#### PFS Inputs\n",
    "The [documentation](https://docs.pachyderm.com/latest/concepts/pipeline-concepts/datum/#datum-pfs-input-and-glob-pattern) refers to the simplest type of input speficiation as PFS Input. A PFS Input is defined, at a minimum, by:\n",
    "\n",
    "- a repo containing the data you want your pipeline to consider\n",
    "- a branch to watch for commits\n",
    "- and a glob pattern to determine how the input data is partitioned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59b42b2-53ed-468e-b041-d07b415bf586",
   "metadata": {},
   "source": [
    "#### Cross & Union Inputs\n",
    "\n",
    "Pachyderm enables you to combine multiple PFS inputs by using the union and cross operators in the pipeline specification. You can think of union as a disjoint union binary operator and cross as a cartesian product binary operator. In other words: the union will combine thre PFS inputs as subdirectories in a larger logical object that is treated like a PFS input. The cross product will match elements between PFS Inputs into discrete sets and then allow the pipeline to operate on each of these match sets as if it were a PFS Input.\n",
    "\n",
    "For more information see this [documentation](https://docs.pachyderm.com/latest/concepts/pipeline-concepts/datum/cross-union/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674c6f01-dff8-4f74-92f4-835c61171cb9",
   "metadata": {},
   "source": [
    "#### Group Input\n",
    "\n",
    "The group operator allows a user to group similarly named files (from multiple repositories) together based on a glob pattern, which instructs pachyderm how to identify the criteria for the name comparision. The output of a grouping is a set of datums (units of work). Each datumn produced by the group operation points to the subset of files from the PFS Inputs which corespond to that group. The pipeline is then able to operate on each discrete datum deparately.\n",
    "\n",
    "For more information see this [documentation](https://docs.pachyderm.com/latest/concepts/pipeline-concepts/datum/group/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4937745f-913d-43b4-834f-3ea3d89b086d",
   "metadata": {},
   "source": [
    "#### Join Input\n",
    "\n",
    "The join operator, like the group oporator, allows a user to group files together based on cominalities in the file name. More precisely it allows us to group a set of files following one nameing convention with files following a second naming convention using some more complex logic to establish a relationship. We can think of the join operator like a sql join operator; we have two columns that we want to associate with eachother via some relationship or primary key.\n",
    "\n",
    "For more infomration see the [detailed examples](https://github.com/pachyderm/pachyderm/tree/2.5.x/examples/joins) or consule the [documentation](https://docs.pachyderm.com/latest/concepts/pipeline-concepts/datum/join/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a085518e-1529-46ab-b002-1f9498dc4bf3",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407adb64-6d92-4550-97f5-82dffe6ab754",
   "metadata": {},
   "source": [
    "# Architecture\n",
    "\n",
    "Pachyderm is packaged as a kubernetes application. The solution is composed of several services hosted in pods and several other kubernets resources. As the diagram below illustrates, the solution is also built on services which may be provided outside the kubernetes cluster sudh as S3 and a block device provider for persistent storage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c38734d-277f-4221-b659-0f922898f9cb",
   "metadata": {},
   "source": [
    "<center><img src=\"images/pachyderm-architecture-diagram.png\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7befb0-2733-40cc-9e9c-994910d7a32e",
   "metadata": {},
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bd0a0c-c9ce-427b-a596-e6bd1205c045",
   "metadata": {},
   "source": [
    "As discussed in my article about [installing pachyderm](Installing%20Pachyderm.ipynb#Installation-Options) there are multiple installation options. The user may choose a cloud native or on-prem solution. As such, some elements of the architecture may change. Note: The core application is build on native kubernetes resources and custom resource definitions so that should not change too much."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e133aacf-7ca4-4bf7-87af-05176198b242",
   "metadata": {},
   "source": [
    "# Fuse Mount\n",
    "Pachyderm enables you to mount a repository as a local filesystem on your computer by using the pachctl mount command. This command uses the Filesystem in Userspace (FUSE) user interface to export a Pachyderm File System (PFS) to a Unix computer system. This functionality is useful when you want to pull data locally to experiment, review the results of a pipeline, or modify the files in the input repository directly.\n",
    "\n",
    "You can mount a Pachyderm repo in one of the following modes:\n",
    "\n",
    " - Read-only — you can read the mounted files to further experiment with them locally, but cannot modify them.\n",
    " - Read-write — you can read mounted files, modify their contents, and push them back into your centralized Pachyderm input repositories.\n",
    "\n",
    "For more information see the following [documentation](https://docs.pachyderm.com/latest/how-tos/basic-data-operations/export-data-out-pachyderm/mount-repo-to-local-computer/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
