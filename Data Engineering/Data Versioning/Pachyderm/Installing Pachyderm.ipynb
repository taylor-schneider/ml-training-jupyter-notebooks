{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e96703e-ad57-421d-85a9-89d7980eba5c",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "In this notebook we will explore the options for installing pachyderm locally (i.e. in a self-hosted format). This means no commercial requirements like a cloud provider or any manages service or applicance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12da697-ee6c-4ca9-a350-9c16e7053eba",
   "metadata": {},
   "source": [
    "Currently the latest stable version is [2.5.5](https://github.com/pachyderm/pachyderm/releases/tag/v2.5.5) which was released 4/27/2023."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361f1e2b-af4e-420b-94ff-e0981a069c9d",
   "metadata": {},
   "source": [
    "# Installation Options\n",
    "\n",
    "Looking through the documentation we see two options for installing pachyderm: locally or in the cloud. We then see that these two opions break down further. The table below tries to enumerate the options presented or implied in the documentation.\n",
    "\n",
    "- [Local Installation](https://docs.pachyderm.com/2.3.x/getting-started/local-installation/)\n",
    "    - [Docker Desktop](https://docs.pachyderm.com/latest/getting-started/local-deploy/docker/) - A commercial application that allows running containers as well as a single node kubernetes cluster\n",
    "    - [Minikube](https://docs.pachyderm.com/latest/getting-started/local-deploy/minikube/) - An open source tool that allows running a single node kubernetes cluster locally\n",
    "- [On-premisis Installation](https://docs.pachyderm.com/latest/deploy-manage/deploy/on-premises/)\n",
    "    - kubernetes cluster\n",
    "- Cloud Installation\n",
    "    - AWS\n",
    "    - Azure\n",
    "    - GCP\n",
    "\n",
    "As the pachyderm instructions are assuming that local installation is performed on a desktop environment, they offer instructions tailored to Windows, Mac, and Linux.\n",
    "\n",
    "As mentioned earlier, we will be installing to a self-hosted kubernetes cluster. \n",
    "\n",
    "The official instructions for the local installation can be found [here](https://docs.pachyderm.com/latest/getting-started/local-deploy/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c0bd80-6793-45f3-b8f9-483f60a0430e",
   "metadata": {},
   "source": [
    "# Installation On Kubernetes Cluster\n",
    "\n",
    "Regardless of which local installation method we have chosen, the steps should be reletively similar as we are essentially deployign an application to kubernetes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498c9285-fa92-4fdc-95d9-5a7af1333f96",
   "metadata": {},
   "source": [
    "## Install Homebrew (docker desktop / minicube only)\n",
    "\n",
    "The first step in the installation is to install Homebrew (if using linux or mac; For Windows, do documentation lists manual steps that must be undertaken). \n",
    "\n",
    "Homebrew is a package manager. It is used to install other components including:\n",
    "- The kubernetes environment (Docker Desktop / Minicube)\n",
    "- pachctl\n",
    "- helm\n",
    "\n",
    "Originally named Linuxbrew, Homebrew was developed for macOS to provide users with a convenient way to install Linux applications. After the tool gained popularity for its large selection of applications and ease of use, Homebrew developers created a native Linux version.\n",
    "\n",
    "Homebrew is an “add-on” package manager. Homebrew installs packages alongside whatever system it runs on.\n",
    "\n",
    "As we are running on a full blown kubernetes cluster we do not need to use brew."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa03915-d0fd-4ef2-8f32-3356771e2137",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cc59a8-2f34-4fdc-9444-146b4b6ccbd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7eea1517-b2e5-41b7-935c-30859ce0c799",
   "metadata": {},
   "source": [
    "## Install pachctl\n",
    "The pachctl is a command-line tool that you can use to interact with a Pachyderm cluster in your terminal. It is provided as a precompiled binary available from the [github releases page](https://github.com/pachyderm/pachyderm/releases/tag/v2.5.5).\n",
    "\n",
    "```\n",
    "[root@os004k8-master001 ~]# curl -L -O https://github.com/pachyderm/pachyderm/releases/download/v2.5.5/pachctl_2.5.5_linux_amd64.tar.gz\n",
    "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
    "                                 Dload  Upload   Total   Spent    Left  Speed\n",
    "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
    "100 37.1M  100 37.1M    0     0  33.6M      0  0:00:01  0:00:01 --:--:-- 82.4M\n",
    "\n",
    "[root@os004k8-master001 ~]# tar -xzvf pachctl_2.5.5_linux_amd64.tar.gz\n",
    "pachctl_2.5.5_linux_amd64/pachctl\n",
    "\n",
    "[root@os004k8-master001 ~]# cp pachctl_2.5.5_linux_amd64/pachctl /usr/bin/\n",
    "\n",
    "[root@os004k8-master001 ~]# pachctl version\n",
    "COMPONENT           VERSION\n",
    "pachctl             2.5.5\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "**Note**:The official installation instructions can be found [here](https://docs.pachyderm.com/2.3.x/getting-started/local-installation/#install-pachctl)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d71fa6-884f-480c-9bf6-1afa87c7d396",
   "metadata": {},
   "source": [
    "## Install Helm (full kuberentes only)\n",
    "\n",
    "We can think of helm as a package and deployment manager for kubernetes. Helm automates the creation, packaging, configuration, and deployment of Kubernetes applications. It does this through a packaging structure that combines your configuration files into a single reusable format that can be understood and managed by the utility.\n",
    "\n",
    "### Helm Compatibility\n",
    "\n",
    "In order to install helm, we need to figure out which version is comptible with the version of our kubernetes cluster. The helm documentation lists the [compatibility matrix](https://helm.sh/docs/topics/version_skew/) as seen below:\n",
    "\n",
    "\n",
    "|Helm Version|Supported Kubernetes Versions|\n",
    "|------------|-----------------------------|\n",
    "|3.11.x |1.26.x - 1.23.x|\n",
    "|3.10.x|1.25.x - 1.22.x|\n",
    "|3.9.x|1.24.x - 1.21.x|\n",
    "|3.8.x|1.23.x - 1.20.x|\n",
    "|3.7.x|1.22.x - 1.19.x|\n",
    "|3.6.x|1.21.x - 1.18.x|\n",
    "|3.5.x|1.20.x - 1.17.x|\n",
    "|3.4.x|1.19.x - 1.16.x|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4baf59b3-88ed-4ad4-a904-1799a2ce47bf",
   "metadata": {},
   "source": [
    "In my case my case my kubernetes cluster was running version 1.21.14:\n",
    "```\n",
    "[root@os004k8-master001 ~]# kubectl version\n",
    "Client Version: version.Info{Major:\"1\", Minor:\"21\", GitVersion:\"v1.21.9\", GitCommit:\"b631974d68ac5045e076c86a5c66fba6f128dc72\", GitTreeState:\"clean\", BuildDate:\"2022-01-19T17:51:12Z\", GoVersion:\"go1.16.12\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n",
    "Server Version: version.Info{Major:\"1\", Minor:\"21\", GitVersion:\"v1.21.14\", GitCommit:\"0f77da5bd4809927e15d1658fb4aa8f13ad890a5\", GitTreeState:\"clean\", BuildDate:\"2022-06-15T14:11:36Z\", GoVersion:\"go1.16.15\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n",
    "\n",
    "```\n",
    "\n",
    "So this means I can run helm 3.6 to 3.9. I will go with 3.9 as it's the newest version which has had the most burn in time with my version of k8."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e4f855-4458-4bb6-94d6-d8b1e8755bdf",
   "metadata": {},
   "source": [
    "### Download Binaries\n",
    "The official installation instructions can be found [here](https://helm.sh/docs/intro/install/). Every version of helm is distributed as a binary built for x64 arhchitectures. The binaries can be doenloaded from the [github releases page](https://github.com/helm/helm/releases).\n",
    "\n",
    "In my case, [3.9.4](https://github.com/helm/helm/releases/tag/v3.9.4) is the latest version available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04da67bc-aee9-498d-9ed4-f12f081fe845",
   "metadata": {},
   "source": [
    "\n",
    "```\n",
    "[root@os004k8-master001 ~]# curl -O https://get.helm.sh/helm-v3.9.4-linux-amd64.tar.gz\n",
    "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
    "                                 Dload  Upload   Total   Spent    Left  Speed\n",
    "100 13.3M  100 13.3M    0     0  20.1M      0 --:--:-- --:--:-- --:--:-- 20.1M\n",
    "\n",
    "[root@os004k8-master001 ~]# tar -zxvf helm-v3.9.4-linux-amd64.tar.gz\n",
    "linux-amd64/\n",
    "linux-amd64/helm\n",
    "linux-amd64/LICENSE\n",
    "linux-amd64/README.md\n",
    "\n",
    "[root@os004k8-master001 ~]# linux-amd64/helm version\n",
    "WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /root/.kube/config\n",
    "WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /root/.kube/config\n",
    "version.BuildInfo{Version:\"v3.9.4\", GitCommit:\"dbc6d8e20fe1d58d50e6ed30f09a04a77e4c68db\", GitTreeState:\"clean\", GoVersion:\"go1.17.13\"}\n",
    "\n",
    "[root@os004k8-master001 ~]# cp linux-amd64/helm /usr/bin/\n",
    "[root@os004k8-master001 ~]# helm version\n",
    "WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /root/.kube/config\n",
    "WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /root/.kube/config\n",
    "version.BuildInfo{Version:\"v3.9.4\", GitCommit:\"dbc6d8e20fe1d58d50e6ed30f09a04a77e4c68db\", GitTreeState:\"clean\", GoVersion:\"go1.17.13\"}\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1041bc-ba96-4d1c-b81e-e0bfbf29b984",
   "metadata": {},
   "source": [
    "### Connect helm to kubernetes cluster\n",
    "In order to allow helm to install packages on kuernetes, it needs to be able to access information about the cluster. This is typically done via the kube config file. A plain text file that contains the configurations and secrets necessary for a cli to connect and authenticate against a kubernetes cluster. For example, the kubectl and kubeadm programs use this file.\n",
    "\n",
    "Helm will default to using whatever your current Kubernetes context is, as specified in the $HOME/. kube/config file. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c392367-cfe1-4e2c-b115-5c9eb0a8533b",
   "metadata": {},
   "source": [
    "### Add Helm Chart Repository\n",
    "\n",
    "The heml package format is referred to as a chart. Similar to regular OS packages, helm charts are provided by repositories. The package manager (helmp) is configured to point to repositories to allow users to download and install packages from those repositories. Artifact Hub is a public repository providing open source helm charts.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6caf90c-65f1-45c0-9389-461228be3ac1",
   "metadata": {},
   "source": [
    "We want to add the repo for the pachyderm repo so we can install the app on our cluster.\n",
    "\n",
    "```\n",
    "[root@os004k8-master001 ~]# helm repo add pachyderm https://helm.pachyderm.com\n",
    "WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /root/.kube/config\n",
    "WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /root/.kube/config\n",
    "\"pachyderm\" has been added to your repositories\n",
    "\n",
    "[root@os004k8-master001 ~]# helm repo update\n",
    "WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /root/.kube/config\n",
    "WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /root/.kube/config\n",
    "Hang tight while we grab the latest from your chart repositories...\n",
    "...Successfully got an update from the \"pachyderm\" chart repository\n",
    "Update Complete. ⎈Happy Helming!⎈\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21244e9b-f753-4857-a100-b374c1cefad9",
   "metadata": {},
   "source": [
    "### Inspect Helm Chart\n",
    "We can ask helm for a definition of the pachyderm chart\n",
    "\n",
    "```\n",
    "[root@os004k8-master001 ~]# helm show chart pachyderm/pachyderm\n",
    "WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /root/.kube/config\n",
    "WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /root/.kube/config\n",
    "annotations:\n",
    "  artifacthub.io/license: Apache-2.0\n",
    "  artifacthub.io/links: |\n",
    "    - name: \"Pachyderm\"\n",
    "      url: https://www.pachyderm.com/\n",
    "    - name: \"Pachyderm repo\"\n",
    "      url: https://github.com/pachyderm/pachyderm\n",
    "    - name: \"Chart repo\"\n",
    "      url: https://github.com/pachyderm/helmchart\n",
    "  artifacthub.io/prerelease: \"false\"\n",
    "apiVersion: v2\n",
    "appVersion: 2.5.5\n",
    "dependencies:\n",
    "- condition: postgresql.enabled\n",
    "  name: postgresql\n",
    "  repository: file://./dependencies/postgresql\n",
    "  version: 10.8.0\n",
    "- condition: pachd.lokiDeploy\n",
    "  name: loki-stack\n",
    "  repository: https://grafana.github.io/helm-charts\n",
    "  version: 2.8.1\n",
    "description: Explainable, repeatable, scalable data science\n",
    "home: https://www.pachyderm.com/\n",
    "icon: https://www.pachyderm.com/wp-content/themes/pachyderm/assets/img/favicons/favicon-32x32.png\n",
    "keywords:\n",
    "- data science\n",
    "kubeVersion: '>= 1.16.0-0'\n",
    "name: pachyderm\n",
    "sources:\n",
    "- https://github.com/pachyderm/pachyderm\n",
    "- https://github.com/pachyderm/helmchart\n",
    "type: application\n",
    "version: 2.5.5\n",
    "```\n",
    "\n",
    "**Note**: More information about the helm chart format can be found here: https://helm.sh/docs/topics/charts/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23336cde-ed8f-43b2-9198-ae5f71492c7d",
   "metadata": {},
   "source": [
    "#### Inspect Dependencies\n",
    "\n",
    "Taking a closer look at the chart, we can wee there are dependencies listed for this chart:\n",
    "\n",
    "```\n",
    "...\n",
    "dependencies:\n",
    "- condition: postgresql.enabled\n",
    "  name: postgresql\n",
    "  repository: file://./dependencies/postgresql\n",
    "  version: 10.8.0\n",
    "- condition: pachd.lokiDeploy\n",
    "  name: loki-stack\n",
    "  repository: https://grafana.github.io/helm-charts\n",
    "  version: 2.8.1\n",
    "...\n",
    "```\n",
    "\n",
    "For the first dependency, we see that there is an instruction to install the postgresql chart from from a local source (the source code for this chart is specified as a relative reference). Looking at the repository in github I was able to find the code [here](https://github.com/pachyderm/pachyderm/blob/master/etc/helm/pachyderm/dependencies/postgresql/Chart.yaml). This points to  a vanilla postgress installtion [provided by bitnami](https://github.com/bitnami/charts/tree/main/bitnami/postgresql).\n",
    "\n",
    "The second dependency points to a package called [loki-stack](https://github.com/grafana/helm-charts/blob/main/charts/loki-stack/Chart.yaml). This package is provided by the grafana project and hosts the Loki service. Grafana is the open source analytics and monitoring solution. Loki is a log aggregation system designed to store and query logs from applications and infrastructure. Loki and Grafana work together to store and to query and display the logs respectively. \n",
    "The official instructions for installing Grafana Loki canbe found [here](https://grafana.com/docs/loki/latest/installation/helm/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0067c84a-3d65-46bd-98a3-616d30bae053",
   "metadata": {},
   "source": [
    "#### Inspect Values\n",
    "Helm was designed so that the helm charts could be defined in a flexible and customizable way. One of the ways this is facilitated is through the values object. Helm assumes the chart is a template and allows users to specify values which map into the configurations hosted in the chart. In this way a user might have a single chart for multiple database deployments; a separate values file could be used to configure each instance (i.e. set the password etc.).\n",
    "\n",
    "We can see the values that are packaged with the helm chart in the [git repository](https://github.com/pachyderm/pachyderm/blob/master/etc/helm/pachyderm/values.yaml) or we can ask helm to tell us what values are associated with a given chart with the following:\n",
    "\n",
    "```\n",
    "### \\#\n",
    "[root@os004k8-master001 ~]# helm show values pachyderm/pachyderm\n",
    "# SPDX-FileCopyrightText: Pachyderm, Inc. <info@pachyderm.com>\n",
    "# SPDX-License-Identifier: Apache-2.0\n",
    "\n",
    "# Deploy Target configures the storage backend to use and cloud provider\n",
    "# settings (storage classes, etc). It must be one of GOOGLE, AMAZON,\n",
    "# MINIO, MICROSOFT, CUSTOM or LOCAL.\n",
    "deployTarget: \"\"\n",
    "\n",
    "global:\n",
    "  postgresql:\n",
    "    # postgresqlUsername is the username to access the pachyderm and dex databases\n",
    "    postgresqlUsername: \"pachyderm\"\n",
    "    # postgresqlPassword to access the postgresql database.  We set a default well-known password to\n",
    "    # facilitate easy upgrades when testing locally.  Any sort of install that needs to be secure\n",
    "    # must specify a secure password here, or provide the postgresqlExistingSecretName and\n",
    "    # postgresqlExistingSecretKey secret.  If using an external Postgres instance (CloudSQL / RDS /\n",
    "    # etc.), this is the password that Pachyderm will use to connect to it.\n",
    "    postgresqlPassword: \"insecure-user-password\"\n",
    "    # When installing a local Postgres instance, postgresqlPostgresPassword defines the root\n",
    "    # ('postgres') user's password.  It must remain consistent between upgrades, and must be\n",
    "    # explicitly set to a value if security is desired.  Pachyderm does not use this account; this\n",
    "    # password is only required so that administrators can manually perform administrative tasks.\n",
    "    postgresqlPostgresPassword: \"insecure-root-password\"\n",
    "    # The auth type to use with postgres and pg-bouncer. md5 is the default\n",
    "    postgresqlAuthType: \"md5\"\n",
    "    # If you want to supply the postgresql password in an existing secret, leave Password blank and\n",
    "    # Supply the name of the existing secret in the namespace and the key in that secret with the password\n",
    "    postgresqlExistingSecretName: \"\"\n",
    "    postgresqlExistingSecretKey: \"\"\n",
    "    # postgresqlDatabase is the database name where pachyderm data will be stored\n",
    "    postgresqlDatabase: \"pachyderm\"\n",
    "    # The postgresql database host to connect to. Defaults to postgres service in subchart\n",
    "    postgresqlHost: \"postgres\"\n",
    "    # The postgresql database port to connect to. Defaults to postgres server in subchart\n",
    "    postgresqlPort: \"5432\"\n",
    "    # postgresqlSSL is the SSL mode to use for pg-bouncer connecting to Postgres, for the default local postgres it is disabled\n",
    "    postgresqlSSL: \"disable\"\n",
    "    # CA Certificate required to connect to Postgres\n",
    "    postgresqlSSLCACert: \"\"\n",
    "    # TLS Secret with cert/key to connect to Postgres\n",
    "    postgresqlSSLSecret: \"\"\n",
    "    # Indicates the DB name that dex connects to\n",
    "    # Indicates the DB name that dex connects to. Defaults to \"Dex\" if not set.\n",
    "    identityDatabaseFullNameOverride: \"\"\n",
    "  # imagePullSecrets allow you to pull images from private repositories, these will also be added to pipeline workers\n",
    "  # https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/\n",
    "  # Example:\n",
    "  # imagePullSecrets:\n",
    "  #   - regcred\n",
    "  imagePullSecrets: []\n",
    "  # when set, the certificate file in pachd-tls-cert will be loaded as the root certificate for pachd, console, and enterprise-server pods\n",
    "  customCaCerts: false\n",
    "  # Sets the HTTP/S proxy server address for console, pachd, and enterprise server.  (This is for\n",
    "  # traffic leaving the cluster, not traffic coming into the cluster.)\n",
    "  proxy: \"\"\n",
    "  # If proxy is set, this allows you to set a comma-separated list of destinations that bypass the proxy\n",
    "  noProxy: \"\"\n",
    "  # Set security context runAs users. If running on openshift, set enabled to false as openshift creates its own contexts.\n",
    "  securityContexts:\n",
    "    enabled: true\n",
    "\n",
    "console:\n",
    "  # enabled controls whether the console manifests are created or not.\n",
    "  enabled: true\n",
    "  annotations: {}\n",
    "  image:\n",
    "    # repository is the image repo to pull from; together with tag it\n",
    "    # replicates the --console-image & --registry arguments to pachctl\n",
    "    # deploy.\n",
    "    repository: \"pachyderm/haberdashery\"\n",
    "    pullPolicy: \"IfNotPresent\"\n",
    "    # tag is the image repo to pull from; together with repository it\n",
    "    # replicates the --console-image argument to pachctl deploy.\n",
    "    tag: \"2.5.5-1\"\n",
    "  priorityClassName: \"\"\n",
    "  nodeSelector: {}\n",
    "  tolerations: []\n",
    "  # podLabels specifies labels to add to the console pod.\n",
    "  podLabels: {}\n",
    "  # resources specifies the resource request and limits.\n",
    "  resources:\n",
    "    {}\n",
    "    #limits:\n",
    "    #  cpu: \"1\"\n",
    "    #  memory: \"2G\"\n",
    "    #requests:\n",
    "    #  cpu: \"1\"\n",
    "    #  memory: \"2G\"\n",
    "  config:\n",
    "    reactAppRuntimeIssuerURI: \"\" # Inferred if running locally or using ingress\n",
    "    oauthRedirectURI: \"\" # Infered if running locally or using ingress\n",
    "    oauthClientID: \"console\"\n",
    "    oauthClientSecret: \"\" # Autogenerated on install if blank\n",
    "    # oauthClientSecretSecretName is used to set the OAuth Client Secret via an existing k8s secret.\n",
    "    # The value is pulled from the key, \"OAUTH_CLIENT_SECRET\".\n",
    "    oauthClientSecretSecretName: \"\"\n",
    "    graphqlPort: 4000\n",
    "    pachdAddress: \"pachd-peer:30653\"\n",
    "    disableTelemetry: false # Disables analytics and error data collection\n",
    "\n",
    "  service:\n",
    "    annotations: {}\n",
    "    # labels specifies labels to add to the console service.\n",
    "    labels: {}\n",
    "    # type specifies the Kubernetes type of the console service.\n",
    "    type: ClusterIP\n",
    "\n",
    "etcd:\n",
    "  affinity: {}\n",
    "  annotations: {}\n",
    "  # dynamicNodes sets the number of nodes in the etcd StatefulSet.  It\n",
    "  # is analogous to the --dynamic-etcd-nodes argument to pachctl\n",
    "  # deploy.\n",
    "  dynamicNodes: 1\n",
    "  image:\n",
    "    repository: \"pachyderm/etcd\"\n",
    "    tag: \"v3.5.5\"\n",
    "    pullPolicy: \"IfNotPresent\"\n",
    "  # maxTxnOps sets the --max-txn-ops in the container args\n",
    "  maxTxnOps: 10000\n",
    "  priorityClassName: \"\"\n",
    "  nodeSelector: {}\n",
    "  # podLabels specifies labels to add to the etcd pod.\n",
    "  podLabels: {}\n",
    "  # resources specifies the resource request and limits\n",
    "  resources:\n",
    "    {}\n",
    "    #limits:\n",
    "    #  cpu: \"1\"\n",
    "    #  memory: \"2G\"\n",
    "    #requests:\n",
    "    #  cpu: \"1\"\n",
    "    #  memory: \"2G\"\n",
    "  # storageClass indicates the etcd should use an existing\n",
    "  # StorageClass for its storage.  It is analogous to the\n",
    "  # --etcd-storage-class argument to pachctl deploy.\n",
    "  # More info for setting up storage classes on various cloud providers:\n",
    "  # AWS: https://docs.aws.amazon.com/eks/latest/userguide/storage-classes.html\n",
    "  # GCP: https://cloud.google.com/compute/docs/disks/performance#disk_types\n",
    "  # Azure: https://docs.microsoft.com/en-us/azure/aks/concepts-storage#storage-classes\n",
    "  storageClass: \"\"\n",
    "  # storageSize specifies the size of the volume to use for etcd.\n",
    "  # Recommended Minimum Disk size for Microsoft/Azure: 256Gi  - 1,100 IOPS https://azure.microsoft.com/en-us/pricing/details/managed-disks/\n",
    "  # Recommended Minimum Disk size for Google/GCP: 50Gi        - 1,500 IOPS https://cloud.google.com/compute/docs/disks/performance\n",
    "  # Recommended Minimum Disk size for Amazon/AWS: 500Gi (GP2) - 1,500 IOPS https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html\n",
    "  storageSize: 10Gi\n",
    "  service:\n",
    "    # annotations specifies annotations to add to the etcd service.\n",
    "    annotations: {}\n",
    "    # labels specifies labels to add to the etcd service.\n",
    "    labels: {}\n",
    "    # type specifies the Kubernetes type of the etcd service.\n",
    "    type: ClusterIP\n",
    "  tolerations: []\n",
    "\n",
    "enterpriseServer:\n",
    "  enabled: false\n",
    "  affinity: {}\n",
    "  annotations: {}\n",
    "  tolerations: []\n",
    "  priorityClassName: \"\"\n",
    "  nodeSelector: {}\n",
    "  service:\n",
    "    type: ClusterIP\n",
    "    apiGRPCPort: 31650\n",
    "    prometheusPort: 31656\n",
    "    oidcPort: 31657\n",
    "    identityPort: 31658\n",
    "    s3GatewayPort: 31600\n",
    "  # There are three options for TLS:\n",
    "  # 1. Disabled\n",
    "  # 2. Enabled, existingSecret, specify secret name\n",
    "  # 3. Enabled, newSecret, must specify cert, key and name\n",
    "  tls:\n",
    "    enabled: false\n",
    "    secretName: \"\"\n",
    "    newSecret:\n",
    "      create: false\n",
    "      crt: \"\"\n",
    "      key: \"\"\n",
    "  resources:\n",
    "    {}\n",
    "    #limits:\n",
    "    #  cpu: \"1\"\n",
    "    #  memory: \"2G\"\n",
    "    #requests:\n",
    "    #  cpu: \"1\"\n",
    "    #  memory: \"2G\"\n",
    "  # podLabels specifies labels to add to the pachd pod.\n",
    "  podLabels: {}\n",
    "  clusterDeploymentID: \"\"\n",
    "  image:\n",
    "    repository: \"pachyderm/pachd\"\n",
    "    pullPolicy: \"IfNotPresent\"\n",
    "    # tag defaults to the chart’s specified appVersion.\n",
    "    tag: \"\"\n",
    "\n",
    "ingress:\n",
    "  enabled: false\n",
    "  annotations: {}\n",
    "  host: \"\"\n",
    "  # when set to true, uriHttpsProtoOverride will add the https protocol to the ingress URI routes without configuring certs\n",
    "  uriHttpsProtoOverride: false\n",
    "  # There are three options for TLS:\n",
    "  # 1. Disabled\n",
    "  # 2. Enabled, existingSecret, specify secret name\n",
    "  # 3. Enabled, newSecret, must specify cert, key, secretName and set newSecret.create to true\n",
    "  tls:\n",
    "    enabled: false\n",
    "    secretName: \"\"\n",
    "    newSecret:\n",
    "      create: false\n",
    "      crt: \"\"\n",
    "      key: \"\"\n",
    "\n",
    "# loki-stack contains values that will be passed to the loki-stack subchart\n",
    "loki-stack:\n",
    "  loki:\n",
    "    serviceAccount:\n",
    "      automountServiceAccountToken: false\n",
    "    persistence:\n",
    "      enabled: true\n",
    "      accessModes:\n",
    "        - ReadWriteOnce\n",
    "      size: 10Gi\n",
    "      # More info for setting up storage classes on various cloud providers:\n",
    "      # AWS: https://docs.aws.amazon.com/eks/latest/userguide/storage-classes.html\n",
    "      # GCP: https://cloud.google.com/compute/docs/disks/performance#disk_types\n",
    "      # Azure: https://docs.microsoft.com/en-us/azure/aks/concepts-storage#storage-classes\n",
    "      storageClassName: \"\"\n",
    "      annotations: {}\n",
    "      priorityClassName: \"\"\n",
    "      nodeSelector: {}\n",
    "      tolerations: []\n",
    "    config:\n",
    "      server:\n",
    "        grpc_server_max_recv_msg_size: 67108864 # 64MiB\n",
    "      query_scheduler:\n",
    "        grpc_client_config:\n",
    "          max_send_msg_size: 67108864 # 64MiB\n",
    "      limits_config:\n",
    "        retention_period: 24h\n",
    "        retention_stream:\n",
    "          - selector: '{suite=\"pachyderm\"}'\n",
    "            priority: 1\n",
    "            period: 168h # = 1 week\n",
    "  grafana:\n",
    "    enabled: false\n",
    "  promtail:\n",
    "    config:\n",
    "      clients:\n",
    "        - url: \"http://{{ .Release.Name }}-loki:3100/loki/api/v1/push\"\n",
    "      snippets:\n",
    "        # The scrapeConfigs section is copied from loki-stack-2.6.4\n",
    "        # The pipeline_stages.match stanza has been added to prevent multiple lokis in a cluster from mixing their logs.\n",
    "        scrapeConfigs: |\n",
    "          - job_name: kubernetes-pods\n",
    "            pipeline_stages:\n",
    "              {{- toYaml .Values.config.snippets.pipelineStages | nindent 4 }}\n",
    "              - match:\n",
    "                  selector: '{namespace!=\"{{ .Release.Namespace }}\"}'\n",
    "                  action: drop\n",
    "            kubernetes_sd_configs:\n",
    "              - role: pod\n",
    "            relabel_configs:\n",
    "              - source_labels:\n",
    "                  - __meta_kubernetes_pod_controller_name\n",
    "                regex: ([0-9a-z-.]+?)(-[0-9a-f]{8,10})?\n",
    "                action: replace\n",
    "                target_label: __tmp_controller_name\n",
    "              - source_labels:\n",
    "                  - __meta_kubernetes_pod_label_app_kubernetes_io_name\n",
    "                  - __meta_kubernetes_pod_label_app\n",
    "                  - __tmp_controller_name\n",
    "                  - __meta_kubernetes_pod_name\n",
    "                regex: ^;*([^;]+)(;.*)?$\n",
    "                action: replace\n",
    "                target_label: app\n",
    "              - source_labels:\n",
    "                  - __meta_kubernetes_pod_label_app_kubernetes_io_instance\n",
    "                  - __meta_kubernetes_pod_label_release\n",
    "                regex: ^;*([^;]+)(;.*)?$\n",
    "                action: replace\n",
    "                target_label: instance\n",
    "              - source_labels:\n",
    "                  - __meta_kubernetes_pod_label_app_kubernetes_io_component\n",
    "                  - __meta_kubernetes_pod_label_component\n",
    "                regex: ^;*([^;]+)(;.*)?$\n",
    "                action: replace\n",
    "                target_label: component\n",
    "              {{- if .Values.config.snippets.addScrapeJobLabel }}\n",
    "              - replacement: kubernetes-pods\n",
    "                target_label: scrape_job\n",
    "              {{- end }}\n",
    "              {{- toYaml .Values.config.snippets.common | nindent 4 }}\n",
    "              {{- with .Values.config.snippets.extraRelabelConfigs }}\n",
    "              {{- toYaml . | nindent 4 }}\n",
    "              {{- end }}\n",
    "        pipelineStages:\n",
    "          - cri: {}\n",
    "        common:\n",
    "          # This is copy and paste of existing actions, so we don't lose them.\n",
    "          # Cf. https://github.com/grafana/loki/issues/3519#issuecomment-1125998705\n",
    "          - action: replace\n",
    "            source_labels:\n",
    "              - __meta_kubernetes_pod_node_name\n",
    "            target_label: node_name\n",
    "          - action: replace\n",
    "            source_labels:\n",
    "              - __meta_kubernetes_namespace\n",
    "            target_label: namespace\n",
    "          - action: replace\n",
    "            replacement: $1\n",
    "            separator: /\n",
    "            source_labels:\n",
    "              - namespace\n",
    "              - app\n",
    "            target_label: job\n",
    "          - action: replace\n",
    "            source_labels:\n",
    "              - __meta_kubernetes_pod_name\n",
    "            target_label: pod\n",
    "          - action: replace\n",
    "            source_labels:\n",
    "              - __meta_kubernetes_pod_container_name\n",
    "            target_label: container\n",
    "          - action: replace\n",
    "            replacement: /var/log/pods/*$1/*.log\n",
    "            separator: /\n",
    "            source_labels:\n",
    "              - __meta_kubernetes_pod_uid\n",
    "              - __meta_kubernetes_pod_container_name\n",
    "            target_label: __path__\n",
    "          - action: replace\n",
    "            regex: true/(.*)\n",
    "            replacement: /var/log/pods/*$1/*.log\n",
    "            separator: /\n",
    "            source_labels:\n",
    "              - __meta_kubernetes_pod_annotationpresent_kubernetes_io_config_hash\n",
    "              - __meta_kubernetes_pod_annotation_kubernetes_io_config_hash\n",
    "              - __meta_kubernetes_pod_container_name\n",
    "            target_label: __path__\n",
    "          - action: keep\n",
    "            regex: pachyderm\n",
    "            source_labels:\n",
    "              - __meta_kubernetes_pod_label_suite\n",
    "          # this gets all kubernetes labels as well\n",
    "          - action: labelmap\n",
    "            regex: __meta_kubernetes_pod_label_(.+)\n",
    "    # Tolerations for promtail pods. Promtail must run on any node where pachyderm resources will run or you won't get any logs for them\n",
    "    # For example, GKE gpu nodes have a default taint of nvidia.com/gpu=present:NoSchedule so if you use GPUs we wouldn't have logs\n",
    "    tolerations: []\n",
    "    livenessProbe:\n",
    "      failureThreshold: 5\n",
    "      tcpSocket:\n",
    "        port: http-metrics\n",
    "      initialDelaySeconds: 10\n",
    "      periodSeconds: 10\n",
    "      successThreshold: 1\n",
    "      timeoutSeconds: 1\n",
    "\n",
    "# The pachw controller creates a pool of pachd instances running in 'pachw' mode which can dynamically scale to handle\n",
    "# storage related tasks\n",
    "pachw:\n",
    "  # When set to true, inheritFromPachd defaults below configuration options like 'resources' and 'tolerations' to\n",
    "  # values from pachd. These values can be overridden by defining the corresponding pachw values below.\n",
    "  # When set to false, a nil value will be used by default instead. Some configuration variables will always use their\n",
    "  # corresponding pachd value, regardless of whether 'inheritFromPachd' is true, such as 'serviceAccountName'\n",
    "  inheritFromPachd: true\n",
    "  # inSidecars is enabled by default to also process storage related tasks in pipeline storage sidecars like version 2.4 or less.\n",
    "  # when enabled, pachw instances can still run in their own dedicated kubernetes deployment if maxReplicas is greater than 0.\n",
    "  # For more control of where pachw instances run, 'inSidecars' can be disabled.\n",
    "  inSidecars: true\n",
    "  maxReplicas: 1\n",
    "  # minReplicas: 0\n",
    "  # We recommend defining resources when running pachw with a high value of maxReplicas.\n",
    "  #resources:\n",
    "  #  limits:\n",
    "  #    cpu: \"1\"\n",
    "  #    memory: \"2G\"\n",
    "  #  requests:\n",
    "  #    cpu: \"1\"\n",
    "  #  memory: \"2G\"\n",
    "  #\n",
    "  #tolerations: []\n",
    "  #affinity: {}\n",
    "  #nodeSelector: {}\n",
    "pachd:\n",
    "  enabled: true\n",
    "  preflightChecks:\n",
    "    # if enabled runs kube validation preflight checks.\n",
    "    enabled: true\n",
    "  affinity: {}\n",
    "  annotations: {}\n",
    "  # clusterDeploymentID sets the Pachyderm cluster ID.\n",
    "  clusterDeploymentID: \"\"\n",
    "  configJob:\n",
    "    annotations: {}\n",
    "  # goMaxProcs is passed as GOMAXPROCS to the pachd container.  pachd can automatically pick an\n",
    "  # optimal GOMAXPROCS from the configured CPU limit, but this overrides it.\n",
    "  goMaxProcs: 0\n",
    "  # goMemLimit is passed as GOMEMLIMIT to the pachd container. pachd can automatically pick an\n",
    "  # optimal GOMEMLIMIT from the configured memory request or limit, but this overrides it.  This is a string\n",
    "  # because it can be something like '256MiB'.\n",
    "  goMemLimit: \"\"\n",
    "  # gcPercent sets the initial garbage collection target percentage.\n",
    "  gcPercent: 0\n",
    "  image:\n",
    "    repository: \"pachyderm/pachd\"\n",
    "    pullPolicy: \"IfNotPresent\"\n",
    "    # tag defaults to the chart’s specified appVersion.\n",
    "    # This sets the worker image tag as well (they should be kept in lock step)\n",
    "    tag: \"\"\n",
    "  logLevel: \"info\"\n",
    "  disableLogSampling: false\n",
    "  developmentLogger: false\n",
    "  # If lokiDeploy is true, a Pachyderm-specific instance of Loki will\n",
    "  # be deployed.\n",
    "  lokiDeploy: true\n",
    "  # lokiLogging enables Loki logging if set.\n",
    "  lokiLogging: true\n",
    "  metrics:\n",
    "    # enabled sets the METRICS environment variable if set.\n",
    "    enabled: true\n",
    "    # endpoint should be the URL of the metrics endpoint.\n",
    "    endpoint: \"\"\n",
    "  priorityClassName: \"\"\n",
    "  nodeSelector: {}\n",
    "  # podLabels specifies labels to add to the pachd pod.\n",
    "  podLabels: {}\n",
    "  # resources specifies the resource requests and limits\n",
    "  # replicas sets the number of pachd running pods\n",
    "  replicas: 1\n",
    "  resources:\n",
    "    {}\n",
    "    #limits:\n",
    "    #  cpu: \"1\"\n",
    "    #  memory: \"2G\"\n",
    "    #requests:\n",
    "    #  cpu: \"1\"\n",
    "    #  memory: \"2G\"\n",
    "  # requireCriticalServersOnly only requires the critical pachd\n",
    "  # servers to startup and run without errors.  It is analogous to the\n",
    "  # --require-critical-servers-only argument to pachctl deploy.\n",
    "  requireCriticalServersOnly: false\n",
    "  # If enabled, External service creates a service which is safe to\n",
    "  # be exposed externally\n",
    "  externalService:\n",
    "    enabled: false\n",
    "    # (Optional) specify the existing IP Address of the load balancer\n",
    "    loadBalancerIP: \"\"\n",
    "    apiGRPCPort: 30650\n",
    "    s3GatewayPort: 30600\n",
    "    annotations: {}\n",
    "  service:\n",
    "    # labels specifies labels to add to the pachd service.\n",
    "    labels: {}\n",
    "    # type specifies the Kubernetes type of the pachd service.\n",
    "    type: \"ClusterIP\"\n",
    "    annotations: {}\n",
    "    apiGRPCPort: 30650\n",
    "    prometheusPort: 30656\n",
    "    oidcPort: 30657\n",
    "    identityPort: 30658\n",
    "    s3GatewayPort: 30600\n",
    "    #apiGrpcPort:\n",
    "    #  expose: true\n",
    "    #  port: 30650\n",
    "  # DEPRECATED: activateEnterprise is no longer used.\n",
    "  activateEnterprise: false\n",
    "  ## if pachd.activateEnterpriseMember is set, enterprise will be activated and connected to an existing enterprise server.\n",
    "  ## if pachd.enterpriseLicenseKey is set, enterprise will be activated.\n",
    "  activateEnterpriseMember: false\n",
    "  ## if pachd.activateAuth is set, auth will be bootstrapped by the config-job.\n",
    "  activateAuth: true\n",
    "  ## the license key used to activate enterprise features\n",
    "  enterpriseLicenseKey: \"\"\n",
    "  # enterpriseLicenseKeySecretName is used to pass the enterprise license key value via an existing k8s secret.\n",
    "  # The value is pulled from the key, \"enterprise-license-key\".\n",
    "  enterpriseLicenseKeySecretName: \"\"\n",
    "  # if a token is not provided, a secret will be autogenerated on install and stored in the k8s secret 'pachyderm-bootstrap-config.rootToken'\n",
    "  rootToken: \"\"\n",
    "  # rootTokenSecretName is used to pass the rootToken value via an existing k8s secret\n",
    "  # The value is pulled from the key, \"root-token\".\n",
    "  rootTokenSecretName: \"\"\n",
    "  # if a secret is not provided, a secret will be autogenerated on install and stored in the k8s secret 'pachyderm-bootstrap-config.enterpriseSecret'\n",
    "  enterpriseSecret: \"\"\n",
    "  # enterpriseSecretSecretName is used to pass the enterprise secret value via an existing k8s secret.\n",
    "  # The value is pulled from the key, \"enterprise-secret\".\n",
    "  enterpriseSecretSecretName: \"\"\n",
    "  # if a secret is not provided, a secret will be autogenerated on install and stored in the k8s secret 'pachyderm-bootstrap-config.authConfig.clientSecret'\n",
    "  oauthClientID: pachd\n",
    "  oauthClientSecret: \"\"\n",
    "  # oauthClientSecretSecretName is used to set the OAuth Client Secret via an existing k8s secret.\n",
    "  # The value is pulled from the key, \"pachd-oauth-client-secret\".\n",
    "  oauthClientSecretSecretName: \"\"\n",
    "  oauthRedirectURI: \"\"\n",
    "  # DEPRECATED: enterpriseRootToken is deprecated, in favor of enterpriseServerToken\n",
    "  # NOTE only used if pachd.activateEnterpriseMember == true\n",
    "  enterpriseRootToken: \"\"\n",
    "  # DEPRECATED: enterpriseRootTokenSecretName is deprecated in favor of enterpriseServerTokenSecretName\n",
    "  # enterpriseRootTokenSecretName is used to pass the enterpriseRootToken value via an existing k8s secret.\n",
    "  # The value is pulled from the key, \"enterprise-root-token\".\n",
    "  enterpriseRootTokenSecretName: \"\"\n",
    "  # enterpriseServerToken represents a token that can authenticate to a separate pachyderm enterprise server,\n",
    "  # and is used to complete the enterprise member registration process for this pachyderm cluster.\n",
    "  # The user backing this token should have either the licenseAdmin & identityAdmin roles assigned, or\n",
    "  # the clusterAdmin role.\n",
    "  # NOTE: only used if pachd.activateEnterpriseMember == true\n",
    "  enterpriseServerToken: \"\"\n",
    "  # enterpriseServerTokenSecretName is used to pass the enterpriseServerToken value via an existing k8s secret.\n",
    "  # The value is pulled from the key, \"enterprise-server-token\".\n",
    "  enterpriseServerTokenSecretName: \"\"\n",
    "  # only used if pachd.activateEnterpriseMember == true\n",
    "  enterpriseServerAddress: \"\"\n",
    "  enterpriseCallbackAddress: \"\"\n",
    "  # Indicates to pachd whether dex is embedded in its process.\n",
    "  localhostIssuer: \"\" # \"true\", \"false\", or \"\" (used string as bool doesn't support empty value)\n",
    "  # set the initial pachyderm cluster role bindings, mapping a user to their list of roles\n",
    "  # ex.\n",
    "  # pachAuthClusterRoleBindings:\n",
    "  #   robot:wallie:\n",
    "  #   - repoReader\n",
    "  #   robot:eve:\n",
    "  #   - repoWriter\n",
    "  pachAuthClusterRoleBindings: {}\n",
    "  # additionalTrustedPeers is used to configure the identity service to recognize additional OIDC clients as trusted peers of pachd.\n",
    "  # For example, see the following example or the dex docs (https://dexidp.io/docs/custom-scopes-claims-clients/#cross-client-trust-and-authorized-party).\n",
    "  # additionalTrustedPeers:\n",
    "  #   - example-app\n",
    "  additionalTrustedPeers: []\n",
    "  serviceAccount:\n",
    "    create: true\n",
    "    additionalAnnotations: {}\n",
    "    name: \"pachyderm\" #TODO Set default in helpers / Wire up in templates\n",
    "  storage:\n",
    "    # backend configures the storage backend to use.  It must be one\n",
    "    # of GOOGLE, AMAZON, MINIO, MICROSOFT or LOCAL. This is set automatically\n",
    "    # if deployTarget is GOOGLE, AMAZON, MICROSOFT, or LOCAL\n",
    "    backend: \"\"\n",
    "    amazon:\n",
    "      # bucket sets the S3 bucket to use.\n",
    "      bucket: \"\"\n",
    "      # cloudFrontDistribution sets the CloudFront distribution in the\n",
    "      # storage secrets.  It is analogous to the\n",
    "      # --cloudfront-distribution argument to pachctl deploy.\n",
    "      cloudFrontDistribution: \"\"\n",
    "      customEndpoint: \"\"\n",
    "      # disableSSL disables SSL.  It is analogous to the --disable-ssl\n",
    "      # argument to pachctl deploy.\n",
    "      disableSSL: false\n",
    "      # id sets the Amazon access key ID to use.  Together with secret\n",
    "      # and token, it implements the functionality of the\n",
    "      # --credentials argument to pachctl deploy.\n",
    "      id: \"\"\n",
    "      # logOptions sets various log options in Pachyderm’s internal S3\n",
    "      # client.  Comma-separated list containing zero or more of:\n",
    "      # 'Debug', 'Signing', 'HTTPBody', 'RequestRetries',\n",
    "      # 'RequestErrors', 'EventStreamBody', or 'all'\n",
    "      # (case-insensitive).  See 'AWS SDK for Go' docs for details.\n",
    "      # logOptions is analogous to the --obj-log-options argument to\n",
    "      # pachctl deploy.\n",
    "      logOptions: \"\"\n",
    "      # maxUploadParts sets the maximum number of upload parts.  It is\n",
    "      # analogous to the --max-upload-parts argument to pachctl\n",
    "      # deploy.\n",
    "      maxUploadParts: 10000\n",
    "      # verifySSL performs SSL certificate verification.  It is the\n",
    "      # inverse of the --no-verify-ssl argument to pachctl deploy.\n",
    "      verifySSL: true\n",
    "      # partSize sets the part size for object storage uploads.  It is\n",
    "      # analogous to the --part-size argument to pachctl deploy.  It\n",
    "      # has to be a string due to Helm and YAML parsing integers as\n",
    "      # floats.  Cf. https://github.com/helm/helm/issues/1707\n",
    "      partSize: \"5242880\"\n",
    "      # region sets the AWS region to use.\n",
    "      region: \"\"\n",
    "      # retries sets the number of retries for object storage\n",
    "      # requests.  It is analogous to the --retries argument to\n",
    "      # pachctl deploy.\n",
    "      retries: 10\n",
    "      # reverse reverses object storage paths.  It is analogous to the\n",
    "      # --reverse argument to pachctl deploy.\n",
    "      reverse: true\n",
    "      # secret sets the Amazon secret access key to use.  Together with id\n",
    "      # and token, it implements the functionality of the\n",
    "      # --credentials argument to pachctl deploy.\n",
    "      secret: \"\"\n",
    "      # timeout sets the timeout for object storage requests.  It is\n",
    "      # analogous to the --timeout argument to pachctl deploy.\n",
    "      timeout: \"5m\"\n",
    "      # token optionally sets the Amazon token to use.  Together with\n",
    "      # id and secret, it implements the functionality of the\n",
    "      # --credentials argument to pachctl deploy.\n",
    "      token: \"\"\n",
    "      # uploadACL sets the upload ACL for object storage uploads.  It\n",
    "      # is analogous to the --upload-acl argument to pachctl deploy.\n",
    "      uploadACL: \"bucket-owner-full-control\"\n",
    "    google:\n",
    "      bucket: \"\"\n",
    "      # cred is a string containing a GCP service account private key,\n",
    "      # in object (JSON or YAML) form.  A simple way to pass this on\n",
    "      # the command line is with the set-file flag, e.g.:\n",
    "      #\n",
    "      #  helm install pachd -f my-values.yaml --set-file storage.google.cred=creds.json pachyderm/pachyderm\n",
    "      cred: \"\"\n",
    "      # Example:\n",
    "      # cred: |\n",
    "      #  {\n",
    "      #    \"type\": \"service_account\",\n",
    "      #    \"project_id\": \"…\",\n",
    "      #    \"private_key_id\": \"…\",\n",
    "      #    \"private_key\": \"-----BEGIN PRIVATE KEY-----\\n…\\n-----END PRIVATE KEY-----\\n\",\n",
    "      #    \"client_email\": \"…@….iam.gserviceaccount.com\",\n",
    "      #    \"client_id\": \"…\",\n",
    "      #    \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n",
    "      #    \"token_uri\": \"https://oauth2.googleapis.com/token\",\n",
    "      #    \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n",
    "      #    \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/…%40….iam.gserviceaccount.com\"\n",
    "      #  }\n",
    "    local:\n",
    "      # hostPath indicates the path on the host where the PFS metadata\n",
    "      # will be stored.  It must end in /.  It is analogous to the\n",
    "      # --host-path argument to pachctl deploy.\n",
    "      hostPath: \"\"\n",
    "      requireRoot: true #Root required for hostpath, but we run rootless in CI\n",
    "    microsoft:\n",
    "      container: \"\"\n",
    "      id: \"\"\n",
    "      secret: \"\"\n",
    "    minio:\n",
    "      # minio bucket name\n",
    "      bucket: \"\"\n",
    "      # the minio endpoint. Should only be the hostname:port, no http/https.\n",
    "      endpoint: \"\"\n",
    "      # the username/id with readwrite access to the bucket.\n",
    "      id: \"\"\n",
    "      # the secret/password of the user with readwrite access to the bucket.\n",
    "      secret: \"\"\n",
    "      # enable https for minio with \"true\" defaults to \"false\"\n",
    "      secure: \"\"\n",
    "      # Enable S3v2 support by setting signature to \"1\". This feature is being deprecated\n",
    "      signature: \"\"\n",
    "    # putFileConcurrencyLimit sets the maximum number of files to\n",
    "    # upload or fetch from remote sources (HTTP, blob storage) using\n",
    "    # PutFile concurrently.  It is analogous to the\n",
    "    # --put-file-concurrency-limit argument to pachctl deploy.\n",
    "    putFileConcurrencyLimit: 100\n",
    "    # uploadConcurrencyLimit sets the maximum number of concurrent\n",
    "    # object storage uploads per Pachd instance.  It is analogous to\n",
    "    # the --upload-concurrency-limit argument to pachctl deploy.\n",
    "    uploadConcurrencyLimit: 100\n",
    "    # The shard size corresponds to the total size of the files in a shard.\n",
    "    # The shard count corresponds to the total number of files in a shard.\n",
    "    # If either criteria is met, a shard will be created.\n",
    "    # values are strings\n",
    "    compactionShardSizeThreshold: \"0\"\n",
    "    compactionShardCountThreshold: \"0\"\n",
    "    memoryThreshold: 0\n",
    "    levelFactor: 0\n",
    "    maxFanIn: 10\n",
    "    maxOpenFileSets: 50\n",
    "    # diskCacheSize and memoryCacheSize are defined in units of 8 Mb chunks. The default is 100 chunks which is 800 Mb.\n",
    "    diskCacheSize: 100\n",
    "    memoryCacheSize: 100\n",
    "  ppsWorkerGRPCPort: 1080\n",
    "  # the number of seconds between pfs's garbage collection cycles.\n",
    "  # if this value is set to 0, it will default to pachyderm's internal configuration.\n",
    "  # if this value is less than 0, it will turn off garbage collection.\n",
    "  storageGCPeriod: 0\n",
    "  # the number of seconds between chunk garbage colletion cycles.\n",
    "  # if this value is set to 0, it will default to pachyderm's internal configuration.\n",
    "  # if this value is less than 0, it will turn off chunk garbage collection.\n",
    "  storageChunkGCPeriod: 0\n",
    "  # There are three options for TLS:\n",
    "  # 1. Disabled\n",
    "  # 2. Enabled, existingSecret, specify secret name\n",
    "  # 3. Enabled, newSecret, must specify cert, key and name\n",
    "  tls:\n",
    "    enabled: false\n",
    "    secretName: \"\"\n",
    "    newSecret:\n",
    "      create: false\n",
    "      crt: \"\"\n",
    "      key: \"\"\n",
    "  tolerations: []\n",
    "  worker:\n",
    "    image:\n",
    "      repository: \"pachyderm/worker\"\n",
    "      pullPolicy: \"IfNotPresent\"\n",
    "      # Worker tag is set under pachd.image.tag (they should be kept in lock step)\n",
    "    serviceAccount:\n",
    "      create: true\n",
    "      additionalAnnotations: {}\n",
    "      # name sets the name of the worker service account.  Analogous to\n",
    "      # the --worker-service-account argument to pachctl deploy.\n",
    "      name: \"pachyderm-worker\" #TODO Set default in helpers / Wire up in templates\n",
    "  rbac:\n",
    "    # create indicates whether RBAC resources should be created.\n",
    "    # Setting it to false is analogous to passing --no-rbac to pachctl\n",
    "    # deploy.\n",
    "    create: true\n",
    "  # Set up default resources for pipelines that don't include any requests or limits.  The values\n",
    "  # are k8s resource quantities, so \"1Gi\", \"2\", etc.  Set to \"0\" to disable setting any defaults.\n",
    "  defaultPipelineCPURequest: \"\"\n",
    "  defaultPipelineMemoryRequest: \"\"\n",
    "  defaultPipelineStorageRequest: \"\"\n",
    "kubeEventTail:\n",
    "  # Deploys a lightweight app that watches kubernetes events and echos them to logs.\n",
    "  enabled: true\n",
    "  # clusterScope determines whether kube-event-tail should watch all events or just events in its namespace.\n",
    "  clusterScope: false\n",
    "  image:\n",
    "    repository: pachyderm/kube-event-tail\n",
    "    pullPolicy: \"IfNotPresent\"\n",
    "    tag: \"v0.0.7\"\n",
    "  resources:\n",
    "    limits:\n",
    "      cpu: \"1\"\n",
    "      memory: 100Mi\n",
    "    requests:\n",
    "      cpu: 100m\n",
    "      memory: 45Mi\n",
    "\n",
    "pgbouncer:\n",
    "  service:\n",
    "    type: ClusterIP\n",
    "  annotations: {}\n",
    "  priorityClassName: \"\"\n",
    "  nodeSelector: {}\n",
    "  tolerations: []\n",
    "  image:\n",
    "    repository: pachyderm/pgbouncer\n",
    "    tag: 1.16.2\n",
    "  resources:\n",
    "    {}\n",
    "    #limits:\n",
    "    #  cpu: \"1\"\n",
    "    #  memory: \"2G\"\n",
    "    #requests:\n",
    "    #  cpu: \"1\"\n",
    "    #  memory: \"2G\"\n",
    "  # maxConnections specifies the maximum number of concurrent connections into pgbouncer.\n",
    "  maxConnections: 1000\n",
    "  # defaultPoolSize specifies the maximum number of concurrent connections from pgbouncer to the postgresql database.\n",
    "  defaultPoolSize: 20\n",
    "\n",
    "# Note: Postgres values control the Bitnami Postgresql Subchart\n",
    "postgresql:\n",
    "  # enabled controls whether to install postgres or not.\n",
    "  # If not using the built in Postgres, you must specify a Postgresql\n",
    "  # database server to connect to in global.postgresql\n",
    "  # The enabled value is watched by the 'condition' set on the Postgresql\n",
    "  # dependency in Chart.yaml\n",
    "  enabled: true\n",
    "  image:\n",
    "    repository: pachyderm/postgresql\n",
    "    tag: \"13.3.0\"\n",
    "  # DEPRECATED from pachyderm 2.1.5\n",
    "  initdbScripts:\n",
    "    dex.sh: |\n",
    "      #!/bin/bash\n",
    "      set -e\n",
    "      psql -v ON_ERROR_STOP=1 --username \"$POSTGRES_USER\" --dbname \"$POSTGRES_DB\" <<-EOSQL\n",
    "        CREATE DATABASE dex;\n",
    "        GRANT ALL PRIVILEGES ON DATABASE dex TO \"$POSTGRES_USER\";\n",
    "      EOSQL\n",
    "  fullnameOverride: postgres\n",
    "  persistence:\n",
    "    # Specify the storage class for the postgresql Persistent Volume (PV)\n",
    "    # See notes in Bitnami chart values.yaml file for more information.\n",
    "    # More info for setting up storage classes on various cloud providers:\n",
    "    # AWS: https://docs.aws.amazon.com/eks/latest/userguide/storage-classes.html\n",
    "    # GCP: https://cloud.google.com/compute/docs/disks/performance#disk_types\n",
    "    # Azure: https://docs.microsoft.com/en-us/azure/aks/concepts-storage#storage-classes\n",
    "    storageClass: \"\"\n",
    "    # storageSize specifies the size of the volume to use for postgresql\n",
    "    # Recommended Minimum Disk size for Microsoft/Azure: 256Gi  - 1,100 IOPS https://azure.microsoft.com/en-us/pricing/details/managed-disks/\n",
    "    # Recommended Minimum Disk size for Google/GCP: 50Gi        - 1,500 IOPS https://cloud.google.com/compute/docs/disks/performance\n",
    "    # Recommended Minimum Disk size for Amazon/AWS: 500Gi (GP2) - 1,500 IOPS https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html\n",
    "    size: 10Gi\n",
    "    labels:\n",
    "      suite: pachyderm\n",
    "  primary:\n",
    "    priorityClassName: \"\"\n",
    "    nodeSelector: {}\n",
    "    tolerations: []\n",
    "  readReplicas:\n",
    "    priorityClassName: \"\"\n",
    "    nodeSelector: {}\n",
    "    tolerations: []\n",
    "\n",
    "cloudsqlAuthProxy:\n",
    "  # connectionName may be found by running `gcloud sql instances describe INSTANCE_NAME --project PROJECT_ID`\n",
    "  connectionName: \"\"\n",
    "  serviceAccount: \"\"\n",
    "  iamLogin: false\n",
    "  port: 5432\n",
    "  enabled: false\n",
    "  image:\n",
    "    # repository is the image repo to pull from; together with tag it\n",
    "    # replicates the --dash-image & --registry arguments to pachctl\n",
    "    # deploy.\n",
    "    repository: \"gcr.io/cloudsql-docker/gce-proxy\"\n",
    "    pullPolicy: \"IfNotPresent\"\n",
    "    # tag is the image repo to pull from; together with repository it\n",
    "    # replicates the --dash-image argument to pachctl deploy.\n",
    "    tag: \"1.23.0\"\n",
    "  priorityClassName: \"\"\n",
    "  nodeSelector: {}\n",
    "  tolerations: []\n",
    "  # podLabels specifies labels to add to the dash pod.\n",
    "  podLabels: {}\n",
    "  # resources specifies the resource request and limits.\n",
    "  resources: {}\n",
    "  #  requests:\n",
    "  #    # The proxy's memory use scales linearly with the number of active\n",
    "  #    # connections. Fewer open connections will use less memory. Adjust\n",
    "  #    # this value based on your application's requirements.\n",
    "  #    memory: \"\"\n",
    "  #    # The proxy's CPU use scales linearly with the amount of IO between\n",
    "  #    # the database and the application. Adjust this value based on your\n",
    "  #    # application's requirements.\n",
    "  #    cpu: \"\"\n",
    "  service:\n",
    "    # labels specifies labels to add to the cloudsql auth proxy service.\n",
    "    labels: {}\n",
    "    # type specifies the Kubernetes type of the cloudsql auth proxy service.\n",
    "    type: ClusterIP\n",
    "\n",
    "oidc:\n",
    "  issuerURI: \"\" #Inferred if running locally or using ingress\n",
    "  requireVerifiedEmail: false\n",
    "  # IDTokenExpiry is parsed into golang's time.Duration: https://pkg.go.dev/time#example-ParseDuration\n",
    "  IDTokenExpiry: 24h\n",
    "  # (Optional) If set, enables OIDC rotation tokens, and specifies the duration where they are valid.\n",
    "  # RotationTokenExpiry is parsed into golang's time.Duration: https://pkg.go.dev/time#example-ParseDuration\n",
    "  RotationTokenExpiry: 48h\n",
    "  # (Optional) Only set in cases where the issuerURI is not user accessible (ie. localhost install)\n",
    "  userAccessibleOauthIssuerHost: \"\"\n",
    "  ## to set up upstream IDPs, set pachd.mockIDP to false,\n",
    "  ## and populate the pachd.upstreamIDPs with an array of Dex Connector configurations.\n",
    "  ## See the example below or https://dexidp.io/docs/connectors/\n",
    "  # upstreamIDPs:\n",
    "  #   - id: idpConnector\n",
    "  #     config:\n",
    "  #       issuer: \"\"\n",
    "  #       clientID: \"\"\n",
    "  #       clientSecret: \"\"\n",
    "  #       redirectURI: \"http://localhost:30658/callback\"\n",
    "  #       insecureEnableGroups: true\n",
    "  #       insecureSkipEmailVerified: true\n",
    "  #       insecureSkipIssuerCallbackDomainCheck: true\n",
    "  #       forwardedLoginParams:\n",
    "  #       - login_hint\n",
    "  #     name: idpConnector\n",
    "  #     type: oidc\n",
    "  #\n",
    "  #   - id: okta\n",
    "  #     config:\n",
    "  #       issuer: \"https://dev-84362674.okta.com\"\n",
    "  #       clientID: \"client_id\"\n",
    "  #       clientSecret: \"notsecret\"\n",
    "  #       redirectURI: \"http://localhost:30658/callback\"\n",
    "  #       insecureEnableGroups: true\n",
    "  #       insecureSkipEmailVerified: true\n",
    "  #       insecureSkipIssuerCallbackDomainCheck: true\n",
    "  #       forwardedLoginParams:\n",
    "  #       - login_hint\n",
    "  #     name: okta\n",
    "  #     type: oidc\n",
    "  upstreamIDPs: []\n",
    "  # upstreamIDPsSecretName is used to pass the upstreamIDPs value via an existing k8s secret.\n",
    "  # The value is pulled from the secret key, \"upstream-idps\".\n",
    "  upstreamIDPsSecretName: \"\"\n",
    "  # Some dex configurations (like Google) require a credential file. Whatever secret is included in this\n",
    "  # below secret will be mounted to the pachd pod at /dexcreds/ so for example serviceAccountFilePath: /dexcreds/googleAuth.json\n",
    "  dexCredentialSecretName: \"\"\n",
    "  mockIDP: true\n",
    "  # additionalClients specifies a list of clients for the cluster to recognize\n",
    "  # See the ecample below or the dex docs (https://dexidp.io/docs/using-dex/#configuring-your-app).\n",
    "  # additionalOIDCClient:\n",
    "  #   - id: example-app\n",
    "  #     secret: example-app-secret\n",
    "  #     name: 'Example App'\n",
    "  #     redirectURIs:\n",
    "  #     - 'http://127.0.0.1:5555/callback'\n",
    "  additionalClients: []\n",
    "  additionalClientsSecretName: \"\"\n",
    "  #TODO scopes:\n",
    "\n",
    "testConnection:\n",
    "  image:\n",
    "    repository: alpine\n",
    "    tag: latest\n",
    "\n",
    "# The proxy is a service to handle all Pachyderm traffic (S3, Console, OIDC, Dex, GRPC) on a single\n",
    "# port; good for exposing directly to the Internet.\n",
    "proxy:\n",
    "  # If enabled, create a proxy deployment (based on the Envoy proxy) and a service to expose it.  If\n",
    "  # ingress is also enabled, any Ingress traffic will be routed through the proxy before being sent\n",
    "  # to pachd or Console.\n",
    "  enabled: true\n",
    "  # The external hostname (including port if nonstandard) that the proxy will be reachable at.\n",
    "  # If you have ingress enabled and an ingress hostname defined, the proxy will use that.\n",
    "  # Ingress will be deprecated in the future so configuring the proxy host instead is recommended.\n",
    "  host: \"\"\n",
    "  # The number of proxy replicas to run.  1 should be fine, but if you want more for higher\n",
    "  # availability, that's perfectly reasonable.  Each replica can handle 50,000 concurrent\n",
    "  # connections.  There is an affinity rule to prefer scheduling the proxy pods on the same node as\n",
    "  # pachd, so a number here that matches the number of pachd replicas is a fine configuration.\n",
    "  # (Note that we don't guarantee to keep the proxy<->pachd traffic on-node or even in-region.)\n",
    "  replicas: 1\n",
    "  # The envoy image to pull.\n",
    "  image:\n",
    "    repository: \"envoyproxy/envoy-distroless\"\n",
    "    tag: \"v1.24.1\"\n",
    "    pullPolicy: \"IfNotPresent\"\n",
    "  # Set up resources.  The proxy is configured to shed traffic before using 500MB of RAM, so that's\n",
    "  # a resonable memory limit.  It doesn't need much CPU.\n",
    "  resources:\n",
    "    requests:\n",
    "      cpu: 100m\n",
    "      memory: 512Mi\n",
    "    limits:\n",
    "      memory: 512Mi\n",
    "  # Any additional labels to add to the pods.  These are also added to the deployment and service\n",
    "  # selectors.\n",
    "  labels: {}\n",
    "  # Any additional annotations to add to the pods.\n",
    "  annotations: {}\n",
    "  # A nodeSelector statement for each pod in the proxy Deployment, if desired.\n",
    "  nodeSelector: {}\n",
    "  # A tolerations statement for each pod in the proxy Deployment, if desired.\n",
    "  tolerations: []\n",
    "  # A priority class name for each pod in the proxy Deployment, if desired.\n",
    "  priorityClassName: \"\"\n",
    "  # Configure the service that routes traffic to the proxy.\n",
    "  service:\n",
    "    # The type of service can be ClusterIP, NodePort, or LoadBalancer.\n",
    "    type: ClusterIP\n",
    "    # If the service is a LoadBalancer, you can specify the IP address to use.\n",
    "    loadBalancerIP: \"\"\n",
    "    # The port to serve plain HTTP traffic on.\n",
    "    httpPort: 80\n",
    "    # The port to serve HTTPS traffic on, if enabled below.\n",
    "    httpsPort: 443\n",
    "    # If the service is a NodePort, you can specify the port to receive HTTP traffic on.\n",
    "    httpNodePort: 30080\n",
    "    httpsNodePort: 30443\n",
    "    # Any additional annotations to add.\n",
    "    annotations: {}\n",
    "    # Any additional labels to add to the service itself (not the selector!).\n",
    "    labels: {}\n",
    "    # The proxy can also serve each backend service on a numbered port, and will do so for any port\n",
    "    # not numbered 0 here.  If this service is of type NodePort, the port numbers here will be used\n",
    "    # for the node port, and will need to be in the node port range.\n",
    "    legacyPorts:\n",
    "      console: 0 # legacy 30080, conflicts with default httpNodePort\n",
    "      grpc: 0 # legacy 30650\n",
    "      s3Gateway: 0 # legacy 30600\n",
    "      oidc: 0 # legacy 30657\n",
    "      identity: 0 # legacy 30658\n",
    "      metrics: 0 # legacy 30656\n",
    "    # externalTrafficPolicy determines cluster-wide routing policy; see \"kubectl explain\n",
    "    # service.spec.externalTrafficPolicy\".\n",
    "    externalTrafficPolicy: \"\"\n",
    "  # Configuration for TLS (SSL, HTTPS).\n",
    "  tls:\n",
    "    # If true, enable TLS serving.  Enabling TLS is incompatible with support for legacy ports (you\n",
    "    # can't get a generally-trusted certificate for port numbers), and disables support for\n",
    "    # cleartext communication (cleartext requests will redirect to the secure server, and HSTS\n",
    "    # headers are set to prevent downgrade attacks).\n",
    "    #\n",
    "    # Note that if you are planning on putting the proxy behind an ingress controller, you probably\n",
    "    # want to configure TLS for the ingress controller, not the proxy.  This is intended for the\n",
    "    # case where the proxy is exposed directly to the Internet.  (It is possible to have your\n",
    "    # ingress controller talk to the proxy over TLS, in which case, it's fine to enable TLS here in\n",
    "    # addition to in the ingress section above.)\n",
    "    enabled: false\n",
    "    # The secret containing \"tls.key\" and \"tls.crt\" keys that contain PEM-encoded private key and\n",
    "    # certificate material.  Generate one with \"kubectl create secret tls <name> --key=tls.key\n",
    "    # --cert=tls.cert\".  This format is compatible with the secrets produced by cert-manager, and\n",
    "    # the proxy will pick up new data when cert-manager rotates the certificate.\n",
    "    secretName: \"\"\n",
    "    # If set, generate the secret from values here.  This is intended only for unit tests.\n",
    "    secret: {}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e166e78a-5714-4e68-bfc0-8a4f3fd8efd1",
   "metadata": {},
   "source": [
    "## Deploy Persistent Volumes Kubernetes\n",
    "\n",
    "The pachyderm solution will rely on several services to make the solution work. Such as etcd, postgresql, loki, and others. These services in particular will require access to storage. \n",
    "\n",
    "We will provide storage to kubernetes hosted services through a kubernetes resource called Persistend Volumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7289d842-22cf-4f7f-a65e-533e6fe476ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54a052d4-bf2f-4b36-a65e-b0fac2797e87",
   "metadata": {},
   "source": [
    "### Understanding Volumes\n",
    "> Volumes\n",
    ">\n",
    "> On-disk files in a container are ephemeral, which presents some problems for non-trivial applications when running in containers. One problem occurs when a container crashes or is stopped. Container state is not saved so all of the files that were created or modified during the lifetime of the container are lost. During a crash, kubelet restarts the container with a clean state. Another problem occurs when multiple containers are running in a Pod and need to share files. It can be challenging to setup and access a shared filesystem across all of the containers. The Kubernetes volume abstraction solves both of these problems.\n",
    ">\n",
    "> ...\n",
    ">\n",
    "> Kubernetes supports many types of volumes. A Pod can use any number of volume types simultaneously. Ephemeral volume types have a lifetime of a pod, but persistent volumes exist beyond the lifetime of a pod. When a pod ceases to exist, Kubernetes destroys ephemeral volumes; however, Kubernetes does not destroy persistent volumes. For any kind of volume in a given pod, data is preserved across container restarts.\n",
    ">\n",
    "> At its core, a volume is a directory, possibly with some data in it, which is accessible to the containers in a pod. How that directory comes to be, the medium that backs it, and the contents of it are determined by the particular volume type used.\n",
    ">\n",
    "\n",
    "> https://kubernetes.io/docs/concepts/storage/volumes/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25e0a9a-b8b4-4515-bd0b-8663774c5dab",
   "metadata": {},
   "source": [
    "### Understanding Persistent Volumes\n",
    "Persistent Volumes are a non-ephemeral volume implimentation. They are a kubernetes resource which provides storage.\n",
    "\n",
    "> A PersistentVolume (PV) is a piece of storage in the cluster that has been provisioned by an administrator or dynamically provisioned using Storage Classes. It is a resource in the cluster just like a node is a cluster resource. PVs are volume plugins like Volumes, but have a lifecycle independent of any individual Pod that uses the PV. This API object captures the details of the implementation of the storage, be that NFS, iSCSI, or a cloud-provider-specific storage system.\n",
    ">\n",
    "> https://kubernetes.io/docs/concepts/storage/persistent-volumes/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd7bf6d-dbfb-4d70-a472-5fc1243c4277",
   "metadata": {},
   "source": [
    "Persistent Volume Claims are requests to have resources allocated to them.\n",
    "\n",
    "> A PersistentVolumeClaim (PVC) is a request for storage by a user. It is similar to a Pod. Pods consume node resources and PVCs consume PV resources. Pods can request specific levels of resources (CPU and Memory). Claims can request specific size and access modes (e.g., they can be mounted ReadWriteOnce, ReadOnlyMany or ReadWriteMany, see AccessModes).\n",
    ">\n",
    "> https://kubernetes.io/docs/concepts/storage/persistent-volumes/\n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c35265-989f-436f-a71e-3e0aba9948c6",
   "metadata": {},
   "source": [
    "### Understanding Storage Classes\n",
    "\n",
    "The Persistant Volume Claim allows a user to request access to specific storage resources. But in some cases, we may want to logically classify our storage and then make a selection from a particular classification. For example if we want a 7200 rpm disk vs a 5200 rpm disk. Storage Classes provide this abstraction:\n",
    "\n",
    "> A StorageClass provides a way for administrators to describe the \"classes\" of storage they offer. Different classes might map to quality-of-service levels, or to backup policies, or to arbitrary policies determined by the cluster administrators. Kubernetes itself is unopinionated about what classes represent. This concept is sometimes called \"profiles\" in other storage systems\n",
    "> \n",
    "> https://kubernetes.io/docs/concepts/storage/storage-classes/\n",
    ">\n",
    "> While PersistentVolumeClaims allow a user to consume abstract storage resources, it is common that users need PersistentVolumes with varying properties, such as performance, for different problems. Cluster administrators need to be able to offer a variety of PersistentVolumes that differ in more ways than size and access modes, without exposing users to the details of how those volumes are implemented. For these needs, there is the StorageClass resource.\n",
    "> \n",
    "> https://kubernetes.io/docs/concepts/storage/persistent-volumes/\n",
    ">\n",
    "> Each StorageClass has a provisioner that determines what volume plugin is used for provisioning PVs. This field must be specified.\n",
    ">\n",
    "> https://kubernetes.io/docs/concepts/storage/storage-classes/#provisioner\n",
    "\n",
    "Kubernetes supports the following Volumn plugins and provisioners:\n",
    "\n",
    "|Volume Plugin | Internal Provisioner | Config Example |\n",
    "|--------------|----------------------|----------------|\n",
    "|AWSElasticBlockStore | ✓ | AWS EBS |\n",
    "|AzureFile | ✓ | Azure File |\n",
    "|AzureDisk | ✓ | Azure Disk |\n",
    "|CephFS | - | - |\n",
    "|Cinder | ✓ | OpenStack Cinder |\n",
    "|FC | - | - |\n",
    "|FlexVolume | - | -\n",
    "|GCEPersistentDisk | ✓ | GCE PD |\n",
    "|iSCSI | - | - |\n",
    "|NFS | - | NFS |\n",
    "|RBD | ✓ | Ceph RBD |\n",
    "|VsphereVolume | ✓ | vSphere |\n",
    "|PortworxVolume | ✓ | Portworx Volume |\n",
    "|Local | - | Local |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd089cd6-4fa6-4508-bd60-51a3a543c0df",
   "metadata": {},
   "source": [
    "### The NFS Storage Class\n",
    "\n",
    "For my deployment I will keep things simple and use an NFS storage volume. In the past I have used ceph, but in this case I will keep things very simple.\n",
    "\n",
    "**Note**: Kubernetes doesn't include an internal NFS provisioner. You need to use an external provisioner to create a StorageClass for NFS. Here are some examples:\n",
    "\n",
    "- NFS Ganesha server and external provisioner\n",
    "- NFS subdir external provisioner\n",
    "\n",
    "Configuration example and documentation can be found [here](https://kubernetes.io/docs/concepts/storage/storage-classes/#nfs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25829d84-e2cd-41fd-8464-e100cdac3d4f",
   "metadata": {},
   "source": [
    "#### Installing the NFS server\n",
    "\n",
    "https://dev.to/prajwalmithun/setup-nfs-server-client-in-linux-and-unix-27id\n",
    "\n",
    "```\n",
    "[root@localhost ~]# yum -y install nfs-utils\n",
    "[root@localhost ~]# vi /etc/exports\n",
    "[root@localhost ~]# cat /etc/exports\n",
    "/nfs_exports   *(rw,root_squash,sync,no_subtree_check)\n",
    "[root@localhost ~]# mkdir /nfs_exports\n",
    "[root@localhost ~]# chmod 777 /nfs_exports\n",
    "[root@localhost ~]# systemctl enable rpcbind\n",
    "[root@localhost ~]# systemctl enable nfs-server\n",
    "[root@localhost ~]# systemctl enable nfs-lock\n",
    "[root@localhost ~]# systemctl enable nfs-idmap\n",
    "[root@localhost ~]# systemctl start rpcbind\n",
    "[root@localhost ~]# systemctl start nfs-server\n",
    "[root@localhost ~]# systemctl start nfs-lock\n",
    "[root@localhost ~]# systemctl start nfs-idmap\n",
    "[root@localhost ~]# systemctl status nfs\n",
    "● nfs-server.service - NFS server and services\n",
    "   Loaded: loaded (/usr/lib/systemd/system/nfs-server.service; disabled; vendor preset: disabled)\n",
    "   Active: active (exited) since Mon 2023-05-01 18:12:08 EDT; 2s ago\n",
    "  Process: 27255 ExecStartPost=/bin/sh -c if systemctl -q is-active gssproxy; then systemctl reload gssproxy ; fi (code=exited, status=0/SUCCESS)\n",
    "  Process: 27253 ExecStart=/usr/sbin/rpc.nfsd $RPCNFSDARGS (code=exited, status=0/SUCCESS)\n",
    "  Process: 27250 ExecStartPre=/usr/sbin/exportfs -r (code=exited, status=0/SUCCESS)\n",
    " Main PID: 27253 (code=exited, status=0/SUCCESS)\n",
    "    Tasks: 0\n",
    "   Memory: 0B\n",
    "   CGroup: /system.slice/nfs-server.service\n",
    "   \n",
    "[root@localhost ~]# exportfs\n",
    "/nfs_exports    <world>\n",
    "```\n",
    "\n",
    "**Note**: Make sure your firewall is properly configured"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815a5d58-fb3a-4df1-9beb-5a136ddac160",
   "metadata": {},
   "source": [
    "#### Install NFS Client on K8 Nodes\n",
    "This need to be installed on the mater and the workers\n",
    "\n",
    "```\n",
    "[root@os004k8-master001 ~]# yum -y install nfs-utils\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec95e2f-2e3b-4ef0-ac4a-dddc487b53ed",
   "metadata": {},
   "source": [
    "#### Test NFS Connection\n",
    "Test we can mount the nfs server and have the permissions to create files and directories\n",
    "\n",
    "```\n",
    "[root@os004k8-master001 ~]# mount -t nfs 15.4.22.101:/nfs_exports /mnt/test\n",
    "[root@os004k8-master001 ~]# mkdir /mnt/test/test\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abff5f37-dc2a-46e5-90a7-c333b10a7695",
   "metadata": {},
   "source": [
    "#### Installing NFS Subdir External Provisioner Using Helm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f596b52-1bdc-484c-bdba-e87ad7054da2",
   "metadata": {
    "tags": []
   },
   "source": [
    "Official instructions can be found [here](https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner)\n",
    "```\n",
    "[root@os004k8-master001 ~]# helm repo add nfs-subdir-external-provisioner https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner/\n",
    "WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /root/.kube/config\n",
    "WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /root/.kube/config\n",
    "\"nfs-subdir-external-provisioner\" has been added to your repositories\n",
    "\n",
    "[root@os004k8-master001 ~]# helm install nfs-subdir-external-provisioner nfs-subdir-external-provisioner/nfs-subdir-external-provisioner  --set nfs.server=15.4.22.101 --set nfs.path=/nfs_exports\n",
    "WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /root/.kube/config\n",
    "WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /root/.kube/config\n",
    "NAME: nfs-subdir-external-provisioner\n",
    "LAST DEPLOYED: Mon May  1 18:24:08 2023\n",
    "NAMESPACE: default\n",
    "STATUS: deployed\n",
    "REVISION: 1\n",
    "TEST SUITE: None\n",
    "\n",
    "[root@os004k8-master001 ~]# kubectl get deployment nfs-subdir-external-provisioner\n",
    "NAME                              READY   UP-TO-DATE   AVAILABLE   AGE\n",
    "nfs-subdir-external-provisioner   1/1     1            1           102s\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d62891-d66b-4290-8ca5-11de6406ed97",
   "metadata": {},
   "source": [
    "#### Define Storage Class\n",
    "\n",
    "When we deployed the helm chart in the previous step, a storage class was created for us:\n",
    "\n",
    "```\n",
    "[root@os004k8-master001 pachyderm]# kubectl get storageclass\n",
    "NAME         PROVISIONER                                     RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE\n",
    "nfs-client   cluster.local/nfs-subdir-external-provisioner   Delete          Immediate           true                   6m31s\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4113fdbc-f739-4a03-b217-cdab147ba8bc",
   "metadata": {},
   "source": [
    "#### Test the installation\n",
    "\n",
    "```\n",
    "[root@os004k8-master001 pachyderm]# cat test-claim.yaml\n",
    "kind: PersistentVolumeClaim\n",
    "apiVersion: v1\n",
    "metadata:\n",
    "  name: test-claim\n",
    "spec:\n",
    "  storageClassName: nfs-client\n",
    "  accessModes:\n",
    "    - ReadWriteMany\n",
    "  resources:\n",
    "    requests:\n",
    "      storage: 1Mi\n",
    "      \n",
    "[root@os004k8-master001 pachyderm]# kubectl apply -f test-claim.yaml\n",
    "persistentvolumeclaim/test-claim created\n",
    "[root@os004k8-master001 pachyderm]# kubectl get persistentvolumeclaim/test-claim\n",
    "NAME         STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE\n",
    "test-claim   Bound    pvc-c83b7c08-3ff0-4a3d-948e-16436326f31f   1Mi        RWX            nfs-client     10s\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "[root@os004k8-master001 pachyderm]# cat test-pod.yaml\n",
    "kind: Pod\n",
    "apiVersion: v1\n",
    "metadata:\n",
    "  name: test-pod\n",
    "spec:\n",
    "  containers:\n",
    "  - name: test-pod\n",
    "    image: busybox:stable\n",
    "    command:\n",
    "      - \"/bin/sh\"\n",
    "    args:\n",
    "      - \"-c\"\n",
    "      - \"touch /mnt/SUCCESS && exit 0 || exit 1\"\n",
    "    volumeMounts:\n",
    "      - name: nfs-pvc\n",
    "        mountPath: \"/mnt\"\n",
    "  restartPolicy: \"Never\"\n",
    "  volumes:\n",
    "    - name: nfs-pvc\n",
    "      persistentVolumeClaim:\n",
    "        claimName: test-claim\n",
    "\n",
    "[root@os004k8-master001 pachyderm]# kubectl apply -f test-pod.yaml\n",
    "pod/test-pod created\n",
    "\n",
    "[root@os004k8-master001 pachyderm]# kubectl get pod/test-pod\n",
    "NAME       READY   STATUS      RESTARTS   AGE\n",
    "test-pod   0/1     Completed   0          7s\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0303d1ea-343c-48bd-940c-d5b9a2d11ef9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cb51b0-2de5-43db-8214-455df9246047",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4cfe5d03-302e-41fa-ba77-b9f4c348b31e",
   "metadata": {},
   "source": [
    "### Deploy Minio Object Store\n",
    "\n",
    "> An object store is used by Pachyderm’s pachd for storing all your data. The object store you use must be accessible via a low-latency, high-bandwidth connection.\n",
    ">\n",
    "> Storage providers like MinIO (the most common and officially supported option), EMC’s ECS, Ceph, or SwiftStack provide S3-compatible access to enterprise storage for on-premises deployment.\n",
    "> \n",
    "> https://docs.pachyderm.com/latest/deploy-manage/deploy/on-premises/#on-premises-sizing-and-configuring-the-object-store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a529771-dee4-45e2-ba91-4e58bf14122b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed972be8-aade-4925-9f70-90ab47b8912f",
   "metadata": {},
   "source": [
    "## Create Values File For Helm\n",
    "\n",
    "We need to create a vluaes file to inform helm of the configurations for our storage class.\n",
    "\n",
    "```\n",
    "[root@os004k8-master001 pachyderm]# vi values.yaml\n",
    "[root@os004k8-master001 pachyderm]# cat values.yaml\n",
    "etcd:\n",
    "  storageClass: nfs-client\n",
    "  size: 10Gi\n",
    "\n",
    "postgresql:\n",
    "  persistence:\n",
    "    storageClass: nfs-client\n",
    "    size: 10Gi\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50816b77-b007-4dc9-8aae-279966bc4035",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd71aad4-fba3-4d31-a209-4fc555bc3963",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea4b8ec-3983-4be0-a5ee-30d9008129e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbed87b1-46cf-43a1-a706-8239b35047ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "771b6bca-c889-4043-9427-1a21313eecfb",
   "metadata": {},
   "source": [
    "## Install Pachyderm using helm\n",
    "\n",
    "Pachyderm uses the client server model. The pachD damon is packaged as a kubernetes pod and the pachctl cli connects to the server (the daemon running in the pod) to execute commands etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23e4404-2a71-452d-890a-45c44b55f198",
   "metadata": {},
   "source": [
    "We can ask helm to list all the charts it can find. In my case I only have one repo (the pachyderm repo) and we can list out all the charts available for a given repo:\n",
    "\n",
    "```\n",
    "[root@os004k8-master001 ~]# helm repo list\n",
    "NAME            URL\n",
    "pachyderm       https://helm.pachyderm.com\n",
    "\n",
    "[root@os004k8-master001 ~]# helm search repo -l -r pachyderm 2>/dev/null\n",
    "NAME                    CHART VERSION   APP VERSION     DESCRIPTION\n",
    "pachyderm/pachyderm     2.5.5           2.5.5           Explainable, repeatable, scalable data science\n",
    "pachyderm/pachyderm     2.5.4           2.5.4           Explainable, repeatable, scalable data science\n",
    "pachyderm/pachyderm     2.5.3           2.5.3           Explainable, repeatable, scalable data science\n",
    "pachyderm/pachyderm     2.5.2           2.5.2           Explainable, repeatable, scalable data science\n",
    "pachyderm/pachyderm     2.5.1           2.5.1           Explainable, repeatable, scalable data science\n",
    "pachyderm/pachyderm     2.5.0           2.5.0           Explainable, repeatable, scalable data science\n",
    "pachyderm/pachyderm     2.4.6           2.4.6           Explainable, repeatable, scalable data science\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39251f4d-b60a-4716-82e0-acdfa1703d70",
   "metadata": {},
   "source": [
    "We can then ask helm to install our preferred version:\n",
    "\n",
    "```\n",
    "[root@os004k8-master001 ~]# helm install pachd pachyderm/pachyderm --set deployTarget=LOCAL --set proxy.enabled=true --set proxy.service.type=LoadBalancer --version 2.5.5\n",
    "W0501 12:39:42.848489   21435 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+\n",
    "W0501 12:39:43.115008   21435 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+\n",
    "NAME: pachd\n",
    "LAST DEPLOYED: Mon May  1 12:39:40 2023\n",
    "NAMESPACE: default\n",
    "STATUS: deployed\n",
    "REVISION: 1\n",
    "NOTES:\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59349d3-bab2-4ee0-aa0f-c3a6ad815dde",
   "metadata": {},
   "source": [
    "**Note**: In the command above, we are providing a number of parameters to the `helm install` command. The `--version` parameter instructs the help utility which specific version to install. The `--set` parameter instructs helm to override a value specified in a helm chart. Values, as we will discuss, are arbitrary settings that can override default settings or pvide values for templates in a helm chart. As the helm chart is written in yaml and consists of complex objects, the values specified in the ``--set` argument may consist of a json path description of the field being accessed. For example the value proxy.service.type is modifying a value for the type attribute of the service object on the proxy object. Fr more information on the arguments helm install accepts, see the [official documentation](https://helm.sh/docs/helm/helm_install/) for more detail.\n",
    "\n",
    "**Note**: While the `--set` parameter can be used to override individual chart settings, the `helm install` command also allows the user to spcify a values.yaml file (i.e. a values file) to do a bulk override. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0996f0de-9d12-4bc8-8441-2faed25deef2",
   "metadata": {},
   "source": [
    "#### Review Install Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4ae6b7-0c81-496c-bc7d-2d6c4410260e",
   "metadata": {},
   "source": [
    "The deployTarget parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2094a7b-bb83-4743-929f-096897596cd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ee6cad6-964c-4240-94b0-30ef96eaab4f",
   "metadata": {},
   "source": [
    "example values.yaml with minio (s3)\n",
    "https://git.app.uib.no/caleno/helm-charts/-/blob/57dc2b1fe0d1f0a2d3400610e411eda5c3e1417c/stable/pachyderm/values.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a508d3-6076-442e-b838-2b22f47614fb",
   "metadata": {},
   "source": [
    "minio deployment instructions\n",
    "https://academic.oup.com/bioinformatics/article/35/5/839/5068160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30168cc-6611-453c-8206-1dcba247e989",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92af33a1-08a3-4485-a17c-8a169d89b320",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bc0c4f-00df-4347-a6db-4acafea91cfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0fc5dffc-6ffe-495e-b6c6-f518879f136a",
   "metadata": {},
   "source": [
    "We can then ask kubernetes about the status of the pods it has spun up to host the pachyderm solution.\n",
    "\n",
    "```\n",
    "[root@os004k8-master001 ~]# kubectl get pods -A\n",
    "NAMESPACE     NAME                                                   READY   STATUS              RESTARTS   AGE\n",
    "default       console-587c67787f-cm8sg                               0/1     ContainerCreating   0          38s\n",
    "default       etcd-0                                                 0/1     Pending             0          38s\n",
    "default       pachd-bd45db8cd-4bh5l                                  0/1     ContainerCreating   0          35s\n",
    "default       pachd-loki-0                                           0/1     Pending             0          38s\n",
    "default       pachd-promtail-64qpn                                   0/1     ContainerCreating   0          39s\n",
    "default       pachd-promtail-jtrkt                                   0/1     ContainerCreating   0          39s\n",
    "default       pachd-promtail-kjlcq                                   0/1     ContainerCreating   0          38s\n",
    "default       pachd-promtail-nljnp                                   0/1     ContainerCreating   0          38s\n",
    "default       pachd-promtail-xp82l                                   0/1     ContainerCreating   0          39s\n",
    "default       pachd-promtail-z6n65                                   0/1     ContainerCreating   0          38s\n",
    "default       pachyderm-kube-event-tail-6c6598cd5-pcthr              0/1     ContainerCreating   0          38s\n",
    "default       pachyderm-proxy-7f4545985c-zf2bv                       0/1     ContainerCreating   0          38s\n",
    "default       pg-bouncer-88dbc966b-l4xzs                             0/1     ContainerCreating   0          38s\n",
    "default       postgres-0                                             0/1     Pending             0          38s\n",
    "kube-system   coredns-558bd4d5db-478nt                               1/1     Running             1          17h\n",
    "kube-system   coredns-558bd4d5db-x42cd                               1/1     Running             1          17h\n",
    "kube-system   etcd-os004k8-master001.foobar.com                      1/1     Running             125        17h\n",
    "kube-system   kube-apiserver-os004k8-master001.foobar.com            1/1     Running             1          17h\n",
    "kube-system   kube-controller-manager-os004k8-master001.foobar.com   1/1     Running             1          17h\n",
    "kube-system   kube-proxy-2l94p                                       1/1     Running             1          16h\n",
    "kube-system   kube-proxy-2m6hr                                       1/1     Running             1          17h\n",
    "kube-system   kube-proxy-f4vbn                                       1/1     Running             1          16h\n",
    "kube-system   kube-proxy-kzz98                                       1/1     Running             1          17h\n",
    "kube-system   kube-proxy-plhkk                                       1/1     Running             1          17h\n",
    "kube-system   kube-proxy-sm9wf                                       1/1     Running             1          17h\n",
    "kube-system   kube-proxy-wcl4n                                       1/1     Running             1          16h\n",
    "kube-system   kube-scheduler-os004k8-master001.foobar.com            1/1     Running             1          17h\n",
    "kube-system   weave-net-472vx                                        2/2     Running             4          17h\n",
    "kube-system   weave-net-4lmgh                                        2/2     Running             4          17h\n",
    "kube-system   weave-net-6w7qx                                        2/2     Running             4          16h\n",
    "kube-system   weave-net-fkzm5                                        2/2     Running             3          16h\n",
    "kube-system   weave-net-g4sjk                                        2/2     Running             2          17h\n",
    "kube-system   weave-net-k7l4s                                        2/2     Running             3          17h\n",
    "kube-system   weave-net-qs929                                        2/2     Running             4          16h\n",
    "```\n",
    "\n",
    "**Note**: This will take some time before all the pods are running. Remember, the kubernetes cluster is going to download a bunch of docker images from dockerhub. This may take some time. It then neets to start the pods (containers) and wait for their internal services to come online."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acac3a13-d3ac-4091-8529-ebba650148c9",
   "metadata": {},
   "source": [
    "### Troubleshooting Pods Not Ready\n",
    "If the helm installation fails (the pods never become ready) we can uninstall using the following:\n",
    "\n",
    "```\n",
    "[root@os004k8-master001 ~]# helm uninstall pachd\n",
    "W0501 12:58:20.207995   30002 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+\n",
    "release \"pachd\" uninstalled\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68d8802-84cf-4365-bdf4-067098f5bd87",
   "metadata": {},
   "source": [
    "We can use the describe command to interrogate why a pod is not running and \n",
    "\n",
    "```\n",
    "[root@os004k8-master001 ~]# kubectl describe pod pachd-loki-0\n",
    "Name:           pachd-loki-0\n",
    "Namespace:      default\n",
    "Priority:       0\n",
    "Node:           <none>\n",
    "Labels:         app=loki\n",
    "                controller-revision-hash=pachd-loki-5bc57fd4dd\n",
    "                name=pachd-loki\n",
    "                release=pachd\n",
    "                statefulset.kubernetes.io/pod-name=pachd-loki-0\n",
    "Annotations:    checksum/config: 9688827d4f9db7e59b48154e6433ef91fdc762d5b7545bfbe786f8d75c4de68a\n",
    "                prometheus.io/port: http-metrics\n",
    "                prometheus.io/scrape: true\n",
    "Status:         Pending\n",
    "IP:\n",
    "IPs:            <none>\n",
    "Controlled By:  StatefulSet/pachd-loki\n",
    "Containers:\n",
    "  loki:\n",
    "    Image:       grafana/loki:2.6.1\n",
    "    Ports:       3100/TCP, 9095/TCP, 7946/TCP\n",
    "    Host Ports:  0/TCP, 0/TCP, 0/TCP\n",
    "    Args:\n",
    "      -config.file=/etc/loki/loki.yaml\n",
    "    Liveness:     http-get http://:http-metrics/ready delay=45s timeout=1s period=10s #success=1 #failure=3\n",
    "    Readiness:    http-get http://:http-metrics/ready delay=45s timeout=1s period=10s #success=1 #failure=3\n",
    "    Environment:  <none>\n",
    "    Mounts:\n",
    "      /data from storage (rw)\n",
    "      /etc/loki from config (rw)\n",
    "      /tmp from tmp (rw)\n",
    "Conditions:\n",
    "  Type           Status\n",
    "  PodScheduled   False\n",
    "Volumes:\n",
    "  storage:\n",
    "    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)\n",
    "    ClaimName:  storage-pachd-loki-0\n",
    "    ReadOnly:   false\n",
    "  tmp:\n",
    "    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)\n",
    "    Medium:\n",
    "    SizeLimit:  <unset>\n",
    "  config:\n",
    "    Type:        Secret (a volume populated by a Secret)\n",
    "    SecretName:  pachd-loki\n",
    "    Optional:    false\n",
    "QoS Class:       BestEffort\n",
    "Node-Selectors:  <none>\n",
    "Tolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n",
    "                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\n",
    "Events:\n",
    "  Type     Reason            Age                  From               Message\n",
    "  ----     ------            ----                 ----               -------\n",
    "  Warning  FailedScheduling  45s (x3 over 2m12s)  default-scheduler  0/7 nodes are available: 7 pod has unbound immediate PersistentVolumeClaims.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c533250e-2fb1-4de4-9fb0-34de30ef808c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b569303-fdb4-4f6e-b624-ef2852335dde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf76324a-78a1-494f-b2cb-9d4cb7466063",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "893376a1-e780-4bf4-b057-d0b6c6d4fbfa",
   "metadata": {},
   "source": [
    "In the events section, I see the pod is not running because it failed to schedule. This failure shows an associated message of \"pod has unbound immediate PersistentVolumeClaims\". I see that the pod is trying to run a loki container provided by the grafana project. I googled this error to try an understand the root cause."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ff4826-8bdb-42c6-89d0-2a40b66eb559",
   "metadata": {},
   "source": [
    "I managed to find an [issue](https://community.grafana.com/t/helm-installation-with-persisistent-storage-does-not-bind-storage/45672) from an issue tracker mentioning a similar issue.\n",
    "\n",
    "The first question was to list out the persistent volume claims with a `kubectl get pvc`. Mine was as follows:\n",
    "\n",
    "```\n",
    "[root@os004k8-master001 ~]# kubectl get PersistentVolumeClaims\n",
    "NAME                   STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE\n",
    "data-postgres-0        Pending                                                     128m\n",
    "etcd-storage-etcd-0    Pending                                                     128m\n",
    "storage-pachd-loki-0   Pending                                                     128m\n",
    "```\n",
    "\n",
    "Describing the resource I see the following:\n",
    "\n",
    "```\n",
    "[root@os004k8-master001 ~]# kubectl describe pvc storage-pachd-loki-0\n",
    "Name:          storage-pachd-loki-0\n",
    "Namespace:     default\n",
    "StorageClass:\n",
    "Status:        Pending\n",
    "Volume:\n",
    "Labels:        app=loki\n",
    "               release=pachd\n",
    "Annotations:   <none>\n",
    "Finalizers:    [kubernetes.io/pvc-protection]\n",
    "Capacity:\n",
    "Access Modes:\n",
    "VolumeMode:    Filesystem\n",
    "Used By:       pachd-loki-0\n",
    "Events:\n",
    "  Type    Reason         Age                     From                         Message\n",
    "  ----    ------         ----                    ----                         -------\n",
    "  Normal  FailedBinding  4m25s (x502 over 129m)  persistentvolume-controller  no persistent volumes available for this claim and no storage class is set\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8640d6c2-3002-4336-9bc5-a48e82b1ce6e",
   "metadata": {},
   "source": [
    "Having a look at the deployment manifest I see:\n",
    "\n",
    "```\n",
    "[root@os004k8-master001 ~]# kubectl get pvc storage-pachd-loki-0 -o yaml\n",
    "apiVersion: v1\n",
    "kind: PersistentVolumeClaim\n",
    "metadata:\n",
    "  creationTimestamp: \"2023-05-01T16:39:45Z\"\n",
    "  finalizers:\n",
    "  - kubernetes.io/pvc-protection\n",
    "  labels:\n",
    "    app: loki\n",
    "    release: pachd\n",
    "  name: storage-pachd-loki-0\n",
    "  namespace: default\n",
    "  resourceVersion: \"34516\"\n",
    "  uid: 33d258a0-32a3-4911-a54a-46f57a2c757e\n",
    "spec:\n",
    "  accessModes:\n",
    "  - ReadWriteOnce\n",
    "  resources:\n",
    "    requests:\n",
    "      storage: 10Gi\n",
    "  volumeMode: Filesystem\n",
    "status:\n",
    "  phase: Pending\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e6dc7f-260c-455a-abb1-6183fc308305",
   "metadata": {},
   "source": [
    "I wanted to understand what a PVC was. I had a look at the official kubernetes documentation to dig in.\n",
    "\n",
    "> The PersistentVolume subsystem provides an API for users and administrators that abstracts details of how storage is provided from how it is consumed. To do this, we introduce two new API resources: PersistentVolume and PersistentVolumeClaim.\n",
    ">\n",
    "> A PersistentVolume (PV) is a piece of storage in the cluster that has been provisioned by an administrator or dynamically provisioned using Storage Classes. It is a resource in the cluster just like a node is a cluster resource. PVs are volume plugins like Volumes, but have a lifecycle independent of any individual Pod that uses the PV. This API object captures the details of the implementation of the storage, be that NFS, iSCSI, or a cloud-provider-specific storage system.\n",
    ">\n",
    "> A PersistentVolumeClaim (PVC) is a request for storage by a user. It is similar to a Pod. Pods consume node resources and PVCs consume PV resources. Pods can request specific levels of resources (CPU and Memory). Claims can request specific size and access modes (e.g., they can be mounted ReadWriteOnce, ReadOnlyMany or ReadWriteMany, see AccessModes).\n",
    ">\n",
    "> https://kubernetes.io/docs/concepts/storage/persistent-volumes/\n",
    "\n",
    "At this point I think I am understanding the problem: the helm chart deployed pods which required PersistentVolumes, but those volumes do not exist. I wondered why and continued reading:\n",
    "\n",
    "> Provisioning\n",
    ">\n",
    ">There are two ways PVs may be provisioned: statically or dynamically.\n",
    ">\n",
    "> Static\n",
    ">\n",
    "> A cluster administrator creates a number of PVs. They carry the details of the real storage, which is available for use by cluster users. They exist in the Kubernetes API and are available for consumption.\n",
    ">\n",
    "> Dynamic\n",
    ">\n",
    "> When none of the static PVs the administrator created match a user's PersistentVolumeClaim, the cluster may try to dynamically provision a volume specially for the PVC. This provisioning is based on StorageClasses: the PVC must request a storage class and the administrator must have created and configured that class for dynamic provisioning to occur. Claims that request the class \"\" effectively disable dynamic provisioning for themselves.\n",
    ">\n",
    "> To enable dynamic storage provisioning based on storage class, the cluster administrator needs to enable the DefaultStorageClass admission controller on the API server. This can be done, for example, by ensuring that DefaultStorageClass is among the comma-delimited, ordered list of values for the --enable-admission-plugins flag of the API server component. For more information on API server command-line flags, check kube-apiserver documentation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfa81fa-cf7e-483f-9af2-49e26bd90775",
   "metadata": {},
   "source": [
    "So, if there are no volumes, the volume claims can never be satisfied. So my next question is: \"Are resources being defined/requested but not being deployed correctly (i.e. something is failing) or are resources not being defined but are being requested?\"\n",
    "\n",
    "I checked and confirmed I do not have any persistent volumes provisioned on my cluster:\n",
    "\n",
    "```\n",
    "[root@os004k8-master001 ~]# kubectl get PersistentVolumes\n",
    "No resources found\n",
    "\n",
    "```\n",
    "\n",
    "I would expect that if something was defined and a deployment failed I would see it listed in that output. This is telling me that nothing was provisioned.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eeb6537-bc51-45a3-82e9-58635f050db6",
   "metadata": {},
   "source": [
    "To make sure this suspicion was correct I had a look at loki documentation. It does [mention](https://grafana.com/docs/loki/latest/installation/helm/configure-storage/) that in order to use loki, stoage must be configured. The storage configuration section in the values.yaml file accepts a number of parameters. \n",
    "\n",
    "> Configure storage\n",
    "> The scalable installation requires a managed object store such as AWS S3 or Google Cloud Storage or a self-hosted store such as Minio. The single binary installation can only use the filesystem for storage.\n",
    ">\n",
    "> This guide assumes Loki will be installed in on of the modes above and that a values.yaml has been created.\n",
    ">\n",
    "> https://grafana.com/docs/loki/latest/installation/helm/configure-storage/\n",
    "\n",
    "I had a look at the [values.yaml file from the git repository](https://github.com/grafana/helm-charts/blob/main/charts/loki-stack/values.yaml) and saw that it did not have any definitions specific to a storage provider.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d45ce3-7ade-46dc-87f0-7af4e8935141",
   "metadata": {},
   "source": [
    "I was wondering if the documentation ever worked. Does Docker Desktop or minikube allow dynamic storage provisioning by default? MY quess was no, and after looking at the docs and how to articles I belive that assumption was correct. So how does this thing deploy!?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca93cfd3-c32b-4d2d-997f-325c6191ed3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfbb1d0-14aa-4a4e-bb37-748a9796f581",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ed9a57-66f2-4d6f-bc82-a746ed7dfdd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c47eba-c8e3-430a-97cb-4fbd33cda30f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7e92e04-899f-4336-a622-5eb916b35d69",
   "metadata": {},
   "source": [
    "I can ask kubernetes to show me the deployments on the system.\n",
    "\n",
    "```\n",
    "[root@os004k8-master001 ~]# kubectl get deployments\n",
    "NAME                        READY   UP-TO-DATE   AVAILABLE   AGE\n",
    "console                     1/1     1            1           99m\n",
    "pachd                       0/1     1            0           99m\n",
    "pachw                       0/0     0            0           99m\n",
    "pachyderm-kube-event-tail   1/1     1            1           99m\n",
    "pachyderm-proxy             1/1     1            1           99m\n",
    "pg-bouncer                  1/1     1            1           99m\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad33857-b389-41d0-b9cc-fcd71fbac2d6",
   "metadata": {},
   "source": [
    "I can then ask for the deployment manifest (yaml file) that describes a given deployment.\n",
    "\n",
    "```\n",
    "[root@os004k8-master001 ~]# kubectl get deployment pachd -o yaml\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  annotations:\n",
    "    deployment.kubernetes.io/revision: \"1\"\n",
    "    meta.helm.sh/release-name: pachd\n",
    "    meta.helm.sh/release-namespace: default\n",
    "  creationTimestamp: \"2023-05-01T16:59:51Z\"\n",
    "  generation: 1\n",
    "  labels:\n",
    "    app: pachd\n",
    "    app.kubernetes.io/managed-by: Helm\n",
    "    suite: pachyderm\n",
    "  name: pachd\n",
    "  namespace: default\n",
    "  resourceVersion: \"38824\"\n",
    "  uid: 98bd124c-ae4f-4b32-9ba9-798861aee2c9\n",
    "spec:\n",
    "  progressDeadlineSeconds: 600\n",
    "  replicas: 1\n",
    "  revisionHistoryLimit: 10\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: pachd\n",
    "      suite: pachyderm\n",
    "  strategy:\n",
    "    rollingUpdate:\n",
    "      maxSurge: 25%\n",
    "      maxUnavailable: 25%\n",
    "    type: RollingUpdate\n",
    "  template:\n",
    "    metadata:\n",
    "      annotations:\n",
    "        checksum/helm-values: 96d4707a15cfe2139b4a2fed4b70dd84276812f4f292cdf18fdc1d3ea8d30486\n",
    "        checksum/storage-secret: 44d530a6561604c3aa1902c08580e6d18618f932f5199448520b26bc345bc88d\n",
    "      creationTimestamp: null\n",
    "      labels:\n",
    "        app: pachd\n",
    "        suite: pachyderm\n",
    "      name: pachd\n",
    "      namespace: default\n",
    "    spec:\n",
    "      automountServiceAccountToken: true\n",
    "      containers:\n",
    "      - args:\n",
    "        - --mode\n",
    "        - $(MODE)\n",
    "        command:\n",
    "        - /pachd\n",
    "        env:\n",
    "        - name: PACHW_IN_SIDECARS\n",
    "          value: \"true\"\n",
    "        - name: PACHW_MIN_REPLICAS\n",
    "          value: \"0\"\n",
    "        - name: PACHW_MAX_REPLICAS\n",
    "          value: \"1\"\n",
    "        - name: POSTGRES_HOST\n",
    "          value: postgres\n",
    "        - name: POSTGRES_PORT\n",
    "          value: \"5432\"\n",
    "        - name: POSTGRES_USER\n",
    "          value: pachyderm\n",
    "        - name: POSTGRES_DATABASE\n",
    "          value: pachyderm\n",
    "        - name: POSTGRES_PASSWORD\n",
    "          valueFrom:\n",
    "            secretKeyRef:\n",
    "              key: postgresql-password\n",
    "              name: postgres\n",
    "        - name: PG_BOUNCER_HOST\n",
    "          value: pg-bouncer\n",
    "        - name: PG_BOUNCER_PORT\n",
    "          value: \"5432\"\n",
    "        - name: LOKI_LOGGING\n",
    "          value: \"true\"\n",
    "        - name: LOKI_SERVICE_HOST\n",
    "          value: $(PACHD_LOKI_SERVICE_HOST)\n",
    "        - name: LOKI_SERVICE_PORT\n",
    "          value: $(PACHD_LOKI_SERVICE_PORT)\n",
    "        - name: PACH_ROOT\n",
    "          value: /pach\n",
    "        - name: ETCD_PREFIX\n",
    "        - name: STORAGE_BACKEND\n",
    "          value: LOCAL\n",
    "        - name: STORAGE_HOST_PATH\n",
    "          value: /var/pachyderm-gApqb/pachd\n",
    "        - name: WORKER_IMAGE\n",
    "          value: pachyderm/worker:2.5.5\n",
    "        - name: WORKER_USES_ROOT\n",
    "          value: \"True\"\n",
    "        - name: WORKER_SIDECAR_IMAGE\n",
    "          value: pachyderm/pachd:2.5.5\n",
    "        - name: WORKER_IMAGE_PULL_POLICY\n",
    "          value: IfNotPresent\n",
    "        - name: WORKER_SERVICE_ACCOUNT\n",
    "          value: pachyderm-worker\n",
    "        - name: METRICS\n",
    "          value: \"true\"\n",
    "        - name: PACHYDERM_LOG_LEVEL\n",
    "          value: info\n",
    "        - name: PACH_NAMESPACE\n",
    "          valueFrom:\n",
    "            fieldRef:\n",
    "              apiVersion: v1\n",
    "              fieldPath: metadata.namespace\n",
    "        - name: REQUIRE_CRITICAL_SERVERS_ONLY\n",
    "          value: \"false\"\n",
    "        - name: PACHD_POD_NAME\n",
    "          valueFrom:\n",
    "            fieldRef:\n",
    "              apiVersion: v1\n",
    "              fieldPath: metadata.name\n",
    "        - name: PPS_WORKER_GRPC_PORT\n",
    "          value: \"1080\"\n",
    "        - name: STORAGE_UPLOAD_CONCURRENCY_LIMIT\n",
    "          value: \"100\"\n",
    "        - name: STORAGE_PUT_FILE_CONCURRENCY_LIMIT\n",
    "          value: \"100\"\n",
    "        - name: STORAGE_COMPACTION_SHARD_SIZE_THRESHOLD\n",
    "          value: \"0\"\n",
    "        - name: STORAGE_COMPACTION_SHARD_COUNT_THRESHOLD\n",
    "          value: \"0\"\n",
    "        - name: STORAGE_COMPACTION_MAX_FANIN\n",
    "          value: \"10\"\n",
    "        - name: STORAGE_FILESETS_MAX_OPEN\n",
    "          value: \"50\"\n",
    "        - name: STORAGE_DISK_CACHE_SIZE\n",
    "          value: \"100\"\n",
    "        - name: STORAGE_MEMORY_CACHE_SIZE\n",
    "          value: \"100\"\n",
    "        - name: CONSOLE_OAUTH_ID\n",
    "          value: console\n",
    "        - name: CONSOLE_OAUTH_SECRET\n",
    "          valueFrom:\n",
    "            secretKeyRef:\n",
    "              key: OAUTH_CLIENT_SECRET\n",
    "              name: pachyderm-console-secret\n",
    "        - name: ENABLE_WORKER_SECURITY_CONTEXTS\n",
    "          value: \"true\"\n",
    "        - name: ENABLE_PREFLIGHT_CHECKS\n",
    "          value: \"true\"\n",
    "        - name: UNPAUSED_MODE\n",
    "          value: full\n",
    "        - name: K8S_MEMORY_REQUEST\n",
    "          valueFrom:\n",
    "            resourceFieldRef:\n",
    "              containerName: pachd\n",
    "              divisor: \"0\"\n",
    "              resource: requests.memory\n",
    "        - name: K8S_MEMORY_LIMIT\n",
    "          valueFrom:\n",
    "            resourceFieldRef:\n",
    "              containerName: pachd\n",
    "              divisor: \"0\"\n",
    "              resource: limits.memory\n",
    "        envFrom:\n",
    "        - secretRef:\n",
    "            name: pachyderm-storage-secret\n",
    "        - secretRef:\n",
    "            name: pachyderm-deployment-id-secret\n",
    "        - configMapRef:\n",
    "            name: pachd-config\n",
    "            optional: true\n",
    "        image: pachyderm/pachd:2.5.5\n",
    "        imagePullPolicy: IfNotPresent\n",
    "        livenessProbe:\n",
    "          exec:\n",
    "            command:\n",
    "            - /pachd\n",
    "            - --readiness\n",
    "          failureThreshold: 10\n",
    "          periodSeconds: 10\n",
    "          successThreshold: 1\n",
    "          timeoutSeconds: 30\n",
    "        name: pachd\n",
    "        ports:\n",
    "        - containerPort: 1600\n",
    "          name: s3gateway-port\n",
    "          protocol: TCP\n",
    "        - containerPort: 1650\n",
    "          name: api-grpc-port\n",
    "          protocol: TCP\n",
    "        - containerPort: 1653\n",
    "          name: peer-port\n",
    "          protocol: TCP\n",
    "        - containerPort: 1657\n",
    "          name: oidc-port\n",
    "          protocol: TCP\n",
    "        - containerPort: 1658\n",
    "          name: identity-port\n",
    "          protocol: TCP\n",
    "        - containerPort: 1656\n",
    "          name: prom-metrics\n",
    "          protocol: TCP\n",
    "        readinessProbe:\n",
    "          exec:\n",
    "            command:\n",
    "            - /pachd\n",
    "            - --readiness\n",
    "          failureThreshold: 3\n",
    "          periodSeconds: 10\n",
    "          successThreshold: 1\n",
    "          timeoutSeconds: 1\n",
    "        resources: {}\n",
    "        startupProbe:\n",
    "          exec:\n",
    "            command:\n",
    "            - /pachd\n",
    "            - --readiness\n",
    "          failureThreshold: 10\n",
    "          periodSeconds: 10\n",
    "          successThreshold: 1\n",
    "          timeoutSeconds: 30\n",
    "        terminationMessagePath: /dev/termination-log\n",
    "        terminationMessagePolicy: File\n",
    "        volumeMounts:\n",
    "        - mountPath: /tmp\n",
    "          name: tmp\n",
    "        - mountPath: /pach\n",
    "          name: pach-disk\n",
    "        - mountPath: /pachyderm-storage-secret\n",
    "          name: pachyderm-storage-secret\n",
    "      dnsPolicy: ClusterFirst\n",
    "      restartPolicy: Always\n",
    "      schedulerName: default-scheduler\n",
    "      securityContext:\n",
    "        runAsUser: 0\n",
    "      serviceAccount: pachyderm\n",
    "      serviceAccountName: pachyderm\n",
    "      terminationGracePeriodSeconds: 30\n",
    "      volumes:\n",
    "      - emptyDir: {}\n",
    "        name: tmp\n",
    "      - hostPath:\n",
    "          path: /var/pachyderm-gApqb/pachd\n",
    "          type: DirectoryOrCreate\n",
    "        name: pach-disk\n",
    "      - name: pachyderm-storage-secret\n",
    "        secret:\n",
    "          defaultMode: 420\n",
    "          secretName: pachyderm-storage-secret\n",
    "status:\n",
    "  conditions:\n",
    "  - lastTransitionTime: \"2023-05-01T16:59:52Z\"\n",
    "    lastUpdateTime: \"2023-05-01T16:59:52Z\"\n",
    "    message: Deployment does not have minimum availability.\n",
    "    reason: MinimumReplicasUnavailable\n",
    "    status: \"False\"\n",
    "    type: Available\n",
    "  - lastTransitionTime: \"2023-05-01T17:09:53Z\"\n",
    "    lastUpdateTime: \"2023-05-01T17:09:53Z\"\n",
    "    message: ReplicaSet \"pachd-7f5f57bd7d\" has timed out progressing.\n",
    "    reason: ProgressDeadlineExceeded\n",
    "    status: \"False\"\n",
    "    type: Progressing\n",
    "  observedGeneration: 1\n",
    "  replicas: 1\n",
    "  unavailableReplicas: 1\n",
    "  updatedReplicas: 1\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ee23e8-71b8-4b88-9c3a-9db7ddea08d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ae87a3-bbb1-4e1d-9c12-1d48438f1413",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bee5bf-dfb6-4dd8-8598-44ca5c5111c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f40ea3f-7db6-4e5d-bf21-59622241138f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
