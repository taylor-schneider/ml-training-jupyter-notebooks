{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "632cb7b8-17b0-4fab-a839-112f5dc470a0",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "The field of version control has been evolving since the early 60's. Recently, as data science, machine learning, and artificial inteligence (all data based applciations), take off, the need to maintain control over data sets is beginning to surface and grow. Additionally, the expectation that [databases evolve](https://martinfowler.com/articles/evodb.html) is also merging with the devops ideology.\n",
    "\n",
    "Currently, the application of version control to data is relatively new. While there are several options on the market, many are not fully featured yet.\n",
    "\n",
    "In this article we will\n",
    "1. Review the use cases for data version control systems (DVCS)\n",
    "2. Review the types of data that need versioning\n",
    "3. Define the full set of features one would expect from a VCS for data \n",
    "4. Review the current offerings on the market"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c16fef-fa4e-4abd-bf6d-cce912ca1663",
   "metadata": {},
   "source": [
    "# 1. Use cases for DVCS\n",
    "\n",
    "Tracking Temporal Data\n",
    "\n",
    "- Time series data\n",
    "\n",
    "Tracking evolution of the data set\n",
    "\n",
    "- Adhoc corrections need to be made to the data\n",
    "- New or experimental code that performs modifications is being run against a data set\n",
    "\n",
    "Unlocking rapid / agile development\n",
    "\n",
    "- Parallel development that introduces breaking changes\n",
    "\n",
    "Protection\n",
    "\n",
    "- Bug in an application causes corruption\n",
    "- Malicious actor makes unauthorized modifications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9468b7f6-716b-481b-91c8-f95d3a2f9c15",
   "metadata": {},
   "source": [
    "# 2. Types of Data\n",
    "Before talking about the various VCS options for data we need to understand that data comes in different formats and there likely isn't going to be a single robust solution that accomodates all data types.\n",
    "\n",
    "Data typically comes in the following flavors:\n",
    "1. Files / Filesystems\n",
    "    1. Tables\n",
    "    2. JSON\n",
    "2. Objects\n",
    "3. Raw Block Devices\n",
    "4. Databases\n",
    "\n",
    "Now that we understand the basic types of data, lets understand the typical features that version control systems offer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0eec932-eeac-4eb2-84a5-309905b75893",
   "metadata": {},
   "source": [
    "# 3. Features Of A DVCS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622cddd4-d5dd-4663-97bb-1ccdc69721f0",
   "metadata": {},
   "source": [
    "## 3.1. Core Features\n",
    "\n",
    "- **History** - An immutable sequential set of point-in-time representations (often referred to as commits) of a data set. I.e. A representation of the historical evolution of a data set.\n",
    "\n",
    "- **Lineage (provenance)** - An imutable ancenstral representation of the inputs and processes that produced a given commit, commit history, or change. \n",
    "\n",
    "    For example, if a pipeline is responsible for adding a new row or if a user is responsible for modifying a file, this information would be captured and reviewable.\n",
    "\n",
    "- **Braching** - An ability to represent and reference discrete parallel states of a data set for a single point in time.\n",
    "\n",
    "- **Merging** The ability to combine parallel commit histories into a single commit history by accepting changes from both version histories and accepting and combining changes to individual files; the abiity to define and impliment merge strategies.\n",
    "\n",
    "- **Conflict Detection and Resolution** - The ability to detect and remediate istances of merge conflicts (when changes in two branches are incompatible or contradict eachother). This functionality, like with diffing, may be offloaded in part to the underlying storage layer or an established third party tool.\n",
    "\n",
    "- **Data Efficiency** - The ability to minimize the number of redundant copies of data.\n",
    "\n",
    "    Storing data costs money, and with big data you will see big costs. The DVSC should be able to allow multiple branches or commits to reference the same copy of data without requiring a separate copy for each observer.\n",
    "\n",
    "- **Atomicity** - The version history is composed of atomic (invivisible and irreducible) units of change. The operations performed on the version history are also atomic guaranteeing that updates to are either successful or in innert in the case of failure. Additionally no two operations can be performed at the same time; only one update to a version history can occur at a given time.\n",
    "\n",
    "- **Persistance**: The ability for the data managed by the VCS to survive system power loss. The data should be stored in a persistant data store.\n",
    "\n",
    "- **Durability**: The ability for version history and linage to survive corruption events (such as transaction failures or bit-rot). The VCS should impliment or inherit mechanisms to prevent against corruption and possible unintended data loss.\n",
    "\n",
    "- **Retention**: The ability to define which set or subset of version history events to \"forget\" or erase based on some predefined policy. While the VCS must be able to persist all records of data changes, in some cases, the user may prefer only to retain a rolling window of changes. It's also possible that the user may want to specify an alternate retention strategy. The VCS should support the ability to prune its records of versions that are no longer needed or wanted. \n",
    "\n",
    "    Note: The underlying storage layer may impliment it's own retention policy. If the VCS is built on top of a 3rd party datastore this may interfere with the retention policy of the VCS depending on the implimentation.\n",
    "\n",
    "- **Working Set** - A mutable copy of a commit or an unstracked data set intended to be comitted at a later date.\n",
    "\n",
    "- **Reversion**: The ability to restore a working set or branch to a historical version. This reversion should support reverting the data in whole or in part (i.e. a single file or an entire directory).\n",
    "\n",
    "- **Metadata Support** - The VCS should allow the user to attach meaningful information to the changes being recorded in the system (i.e. what were the changes, why where they made, etc.). With traditional VCS we are able to see the user who comitted the, a description of the change, and a change list associated with the commit. Data pipelines however are a bit more complicated and robust than human driven edits to plain text files. For example, data may be transformed through a DAG expressed through a pipeline or by a transformation as part of a stream. In both of these instances the transformation is implimented via an instance of code. The VCS should be aware of the process which is orchestrating and/or applying a particular change to a data set. For example if there is a pipeline run the VCS should be able to create a link between the verson of code and the instance of the pipeline which performed the transaction. This additional \n",
    "\n",
    "- **Diffing**: The ability to show the differences between commits, between a working set and a commit, or individual files rather than the entire data set. \n",
    "\n",
    "   In practice, Some of this functionality can be offloaded in part to the underlying storage implimentation. For example, if a JSON file changed, there are already diff tools which can compare plain text files; if a database file changed, one may need to leverage the underlying database tooling to do a comparison.\n",
    "\n",
    "- **Reviewability** - The ability to review and query metadata attached to the revisions in the history. \n",
    "\n",
    "- **Format Aignostic** - Able to support multiple formats or types of data (eg. text files, binary files, database files).\n",
    "\n",
    "- **Blame Support** - Similar to traditional VCS the DVCS should allow a user to identify who is responsible for a particular change in the data.\n",
    "\n",
    "- **Role Based Access Control** - The VCS needs to be able to define users, roles, and associated permissions. While the underlying storage layer will have it's own RBAC, which should be respected by the VCS implimentation, the VCS maintains its own objects and data which also must be protected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d003d449-ee08-4bf5-9620-a5c35099a618",
   "metadata": {},
   "source": [
    "## 3.2. Operational Features\n",
    "\n",
    "- **Backup Support**: The VCS system needs to enable system admins to perform backups. As such a copy of the entire system can be snapshotted and archived for disaster recovery purposes.\n",
    "\n",
    "- **Resilliance** - As with a datastore, the VCS should be able to recover from failures and provide an uninterrupted service to the users.\n",
    "\n",
    "- **Scalability** - The VCS performance should not be impacted with changes to the size of the underlying data set or the length of the revision history."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b5349e-8c6d-4040-8471-5cb33f464298",
   "metadata": {},
   "source": [
    "## 3.3. Architectural Design\n",
    "\n",
    "- **Datastore Integration**: The VCS should be built on top of an established traditional storage layers. In doing so, the VCS will minimize the complexity of it's design and maximize the utility provided to users and admins.\n",
    "\n",
    "- **Highest Level Abstraction**: speaking in the abstract, data is stored in containers (sometimes in a nested or hierachical structure). For example, sql data is stored in a table in a database; json files are stored in an S3 bucket in an AWS account. The VCS should be aware of the abstractions provided by the underlying data stores. It should be able to understand and register changes to any multiple of lower abstraction within the data store. As such, the DVCS should provide an abstraction that can sit above any other \"layer\" representing a data source."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267428e7-ec59-4235-b241-6ef9836b1e87",
   "metadata": {},
   "source": [
    "# 4. Current Market Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2038173a-87fd-4258-bcbd-02c4696f5801",
   "metadata": {},
   "source": [
    "## 1.1. The Classic DIY Approach\n",
    "The classical approach has been for DBA's and system admins to maintain their own mechanisms for managing the various versions of a data set as it evolves over time. These solutions are generally built on top of the datastore, consists of a patchwork of code and 3rd party solutions, and impliment only a subset of features expected of a DVCS.\n",
    "\n",
    "Generally speaking there are two methods used in parallel:\n",
    "1. Taking system \"snapshots\" or backups (of the entire system or enough to rebuild the system manually) periodically via automation or manual efforts\n",
    "2. Integrating versioning metadata into the underlying schema of the data. This can be done by injecting information into file names or table names (eg. my_file_20203_v2 or my_table becomes my_table_2022 or mytable_v1).\n",
    "\n",
    "But these approaches don't provide all the functionality that a robust VCS would provide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bb467f-4f8e-4ac1-97ee-70609e006408",
   "metadata": {},
   "source": [
    "### 1.1.1. Example Using MS SQL Server\n",
    "\n",
    "https://blog.devart.com/database-versioning-with-examples.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64cdeb3-6109-4907-84ec-9d5079ec4567",
   "metadata": {},
   "source": [
    "### 1.1.2. \"As Of\" Syntax\n",
    "We have SQL \"as of\" syntax. This is in the SQL standard as of 2011 (pun intended) and supported by Oracle, Microsoft SQL Server, and MariaDB. With this syntax you can configure a table to be versioned or a \"temporal table\" and then query a version using as of <timestamp>. You don't get diffs and merges but you do get instant rollbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23347dc-33bf-4f49-b490-bff1a61cfc8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e3fb2a5-e08a-4cfe-bc1b-c6768defdd13",
   "metadata": {},
   "source": [
    "## 1.2. Solutions For Popular Databases\n",
    "\n",
    "Martin Fowler wrote this [article on Evolutionary Database Design](https://martinfowler.com/articles/evodb.html) outlining some of the problems and features one would expect. Some of the solutions below address the issues and designs being proposed.\n",
    "\n",
    "In general, when we talk about applying version control to a database, holistically we are talkign about all the elements of a database including:\n",
    "- Database Schema\n",
    "- Database Object Definitions\n",
    "- Database Data\n",
    "- Database RBAC (roles, privileges, etc.)\n",
    "\n",
    "As we may have infered from the custom built solutions section, we may need to apply a different tool / technique to each component of the database. Below we look at examples that try to capture many/all of these elements in one solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36eb993-e97e-43c0-bb9c-57c7bfbedc54",
   "metadata": {},
   "source": [
    "### 1.2.1. Dolt (MySql Databases)\n",
    "\n",
    "Dolt is a MySQL database that impliments a git-like api which provides version control for tables and table rows.\n",
    "\n",
    "According to it's homepage: \n",
    "> Dolt is the first  and only SQL database that you can fork, clone, branch, merge, push and pull just like a Git repository. Dolt is a version controlled database. Dolt is Git for Data. Dolt is a Versioned MySQL Replica.\n",
    "> \n",
    "> Dolt implements the Git command line and associated operations on table rows instead of files. Data and schema are modified in the working set using SQL. When you want to permanently store a version of the working set, you make a commit. In SQL, Dolt implements Git read operations (ie. diff, log) as system tables and write operations (ie. commit, merge) as stored procedures. Dolt produces cell-wise diffs and merges, making data debugging between versions tractable. Dolt is the only SQL database that has branches and merges.\n",
    ">\n",
    ">You can run Dolt online, like you would PostgreSQL or MySQL. Or you can run Dolt offline, treating data and schema like source code.\n",
    ">\n",
    "> https://docs.dolthub.com/introduction/what-is-dolt\n",
    "\n",
    "With the addition of its support for MySql/MariaDB Binlog replication, Dolt can be attached to a primary database and can provider versioning for this primary replica.\n",
    "\n",
    "Dolt handles write operations through stored procedures and will present the difference between data versions cell-wise. For more information we can see the [following article](https://cult.honeypot.io/reads/dolt-a-sql-database-that-works-like-git)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066b620a-7586-4ec6-9e1d-33dec513f3b3",
   "metadata": {},
   "source": [
    "### 1.2.2. VersionSQL (MS SQL Server)\n",
    "\n",
    "VersionSQL is a plugin for the SQL Server Management Studio IDE. It allows DBA's to track and version changes to database code files from the SSMS interface. The plugin connects to remote VCS systems like Github.\n",
    "\n",
    "https://www.versionsql.com/sql-server-database/\n",
    "\n",
    "This may work for other databases like MySQL as SSMS is able to connect to multiple servers. https://docs.devart.com/odbc/mysql/microsoft_sql_server_manager_s.htm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e9f94e-026f-4e8d-83f8-2e35134cdbbc",
   "metadata": {},
   "source": [
    "## 2.3. Solutions For File Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a331ca1-5677-49f9-b458-9dbabfb4c6e8",
   "metadata": {},
   "source": [
    "### Git-LFS\n",
    "Git Large File Storage (LFS) replaces large files such as audio samples, videos, datasets, and graphics with text pointers inside Git, while storing the file contents on a remote server implimenting the git-lfs protocol (like GitHub.com or GitHub Enterprise).\n",
    "\n",
    "This issues with git-lfs is that like git, it uses the client server model. This means there is a local copy of the data and a remote copy, and these need to be synchronized. This will not scale well when the data store starts reaching a massive size or a massive file count. Copying down terrabytes of information or even monitoring / managing that footprint will not scale well on a single machine. Additionally, this model also creates a potential bottleneck and it requires the user to operate on top of a POSIX api. \n",
    "\n",
    "To mitigate the issue of having to sync to and from the server, the user could elect to use a networked filesystem (like ceph) and mount it rather than working off of a local filesystem. This would provide a scalabile mechanism for sync'ing the data. But it would not resolve the issue of concurrency: multiple users would be pointing at the same branch. Some networked file systems have the ability to create subvolumes or atomic layers or branches. These subvolumes can be independently mounted. In this way we could provide the ability for multiple users to work independently, in parallel, on their own forked copies of the data.\n",
    "\n",
    "https://git-lfs.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de92e4ef-f65f-48f4-8144-30b2f40c4f16",
   "metadata": {},
   "source": [
    "### Git-annex\n",
    "\n",
    "[Git-annex](https://git-annex.branchable.com/) is similar to git-lfs; allows managing large files with git, without storing the file contents in git. It is an older solution than git-lfs and supports a wide number of backends as opposed to git-lfs which only supports a git-lfs backend. That being said, it has some quarks between windows and linux use cases. More information on the differences can be found [here](https://stackoverflow.com/questions/39337586/how-do-git-lfs-and-git-annex-differ) and [here](https://www.perforce.com/manuals/gitswarm-ee/workflow/lfs/migrate_from_git_annex_to_git_lfs.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c6585a-2e2e-4c48-a137-3fa1d261e571",
   "metadata": {
    "tags": []
   },
   "source": [
    "### DVC\n",
    "\n",
    "DVC stands for (Not Just) Data Version Control. The reason for this is that DVC offers versioning for the entire data science and machine learning workflow. It's homepage can be found [here](https://dvc.org/). It bosts that DVC can version source code, data, machine learning experiemtns, and machine learning models. These later features will be left for another article discussing those topics. \n",
    "\n",
    "With respect to versioning data, DVC is alot like git-lfs and it integrates directly with git. The basic function is to tell git to ignore a data directory and to tell dvc to track that data directory. DVC will add files tothe git repository that act as pointer to the actual data. The DVC command line is very similar to git meaining the concepts of branches, staging, commits, push, and pull still apply.\n",
    "\n",
    "One of the advantages of git-lfs is that DVC is able to store data in a larger array of backen storage repositories. DVC supports Amazon S3, Microsoft Azure Blob Storage, Google Drive, Google Cloud Storage, Aliyun OSS, SSH/SFTP, HDFS, HTTP, network-attached storage, or disc to store file contents.\n",
    "\n",
    "A helpful walkthrough can be found [here](https://towardsdatascience.com/introduction-to-dvc-data-version-control-tool-for-machine-learning-projects-7cb49c229fe0).\n",
    "\n",
    "\n",
    "Note: DVC will face similar issue to git-lfs and thus can use a similar workaround as the one mentioned in that section.\n",
    "\n",
    "Another scalability limitation is that users note that the system slows down as the number of files being tracked grows. This is because the solution is a program that runs on a single compute footprint. It is not distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00e5952-f061-401c-a0a8-e50562250815",
   "metadata": {},
   "source": [
    "### Hangar\n",
    "A python package which impliments the DVC functionality. More information can be found [here](https://opendatascience.com/introduction-to-data-version-control/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa0d09e-25aa-47f6-b679-2cab30a24337",
   "metadata": {},
   "source": [
    "### LakeFS\n",
    "LakeFS is a git-like solution that sits on top of S3 (azure support is on the roadmap). It works by providing a metadata layer on top of the \"real files\". It keeps track of which \"real files\" are part of which version as well as what operations were performed between versions. This is very nice because there is no data duplication and no client/server that you might see with git-lfs or DVC.\n",
    "\n",
    "For more information on LakeFS see [this article](./LakeFS/Intro%20To%20LakeFS.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859b47b1-5bca-455b-a695-70fae3e12d27",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Pachyderm\n",
    "Pachyderm is provides a DVCS implimentaion that provides functionality analogous to a minimal git implimentation; users can commit and branch the data repositories. The solution itself is hosted on kubernetes and can be deployed on cloud or on-prem. The solution has a community and enterprise edition available.\n",
    "\n",
    "The official documentation can be found [here](https://docs.pachyderm.com/latest/overview/). I explore this solution in the following [notebook](Pachyderm/Intro%20To%20Pachyderm.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e318bb-42f6-46c6-9925-76fb234d319f",
   "metadata": {},
   "source": [
    "### Deep Lake (Formerly Activeloop Hub)\n",
    "\n",
    "Deep Lake is a data lake implimentation designed for deep learning. According to the documentation:\n",
    "\n",
    "> Deep Lake (formerly known as Activeloop Hub) is a data lake for deep learning applications. Our open-source dataset format is optimized for rapid streaming and querying of data while training models at scale, and it includes a simple API for creating, storing, and collaborating on AI datasets of any size. It can be deployed locally or in the cloud, and it enables you to store all of your data in one place, ranging from simple annotations to large videos.\n",
    "> \n",
    "> https://github.com/activeloopai/deeplake\n",
    "\n",
    "Looking through the [documentation](https://github.com/activeloopai/deeplake) I see that Deep Lake is implimentated as a python package which is able to upload, download, and stream datasets to/from AWS S3/S3-compatible storage, GCP, Activeloop cloud, or local storage.\n",
    "\n",
    "Deep Lake [dataset version control](https://docs.activeloop.ai/getting-started/dataset-version-control#how-to-use-version-control-in-deep-lake) allows you to manage changes to datasets with commands very similar to Git. It's api impliments the familiar concepts of commits, branches, merges, and diffing.\n",
    "\n",
    "A Shortcoming of this technology is that the APIs for loading data from the data store into models is only available for Pytorch and Tensorflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3cc417-d7ee-4e64-947e-7f3ba960f27c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Kamu\n",
    "\n",
    "Kemu is a solution for managing, transforming, and collaborating on structured data.\n",
    "\n",
    "In a larger context, kamu is a reference implementation of Open Data Fabric - a Web 3.0 protocol for providing timely, high-quality, and verifiable data for data science, smart contracts, web and applications.\n",
    "\n",
    "Kemu meintains unbreakable lineage and provenance trail in tamper-proof metadata, which lets you assess the trustworthyness of data, no matter how many hands and transformation steps it went through.\n",
    "\n",
    "It is currently in an [experimental state](https://docs.kamu.dev/odf/).\n",
    "\n",
    "According to the documentation:\n",
    "\n",
    "> Kamu achieves for data what Version Control Systems did for Software , but does so without diffs, versioning, or snapshotting.\n",
    ">\n",
    "> Our new paradigm streamlines collaboration on data within your company, and enables the effect similar to Open Source Software revolution for data globally.\n",
    "> \n",
    "> How it works\n",
    ">\n",
    "> 1. We turn data into a ledger\n",
    ">\n",
    "> Data preserves complete history, and never updated destructively. Trust is anchored at the publisher, so they can be always held accountable for data they provide.\n",
    ">\n",
    "> 2. Datasets are registered on the network\n",
    ">\n",
    "> As a publisher you don’t need to move data into any central point. You maintain complete ownership and control.\n",
    ">\n",
    "> 3. People process data using special SQL code\n",
    "> \n",
    "> Our decentralized ETL pipelines can span across teams, organizations, and even continents. People can collaborate on cleaning and enriching data and confidently reuse data from any step.\n",
    ">\n",
    "> 4. Data flows in near real-time\n",
    "> \n",
    "> Our streaming SQL engines process data within seconds, continuously and autonomously. All of your science projects, dashboards, and automation get the fidelity of stock tickers data.\n",
    ">\n",
    "> 5. Accountability, verifiability, and provenance built-in\n",
    ">\n",
    "> Our SQL has the properties of Smart Contracts, so you can trace every single data cell to its source, and easily tell who processed it and how.\n",
    ">\n",
    "> https://www.kamu.dev/\n",
    "\n",
    "kamu is a single-binary utility that comes bundled with most of its dependencies. The binary runs on linux, windows, and mac.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9774c11e-8ce4-4093-82a1-04952e983f74",
   "metadata": {},
   "source": [
    "### Quilt\n",
    "\n",
    "Quilt is an AWS based commercial solution for creating and managing data packages. It uses a python api and [AWS infrastructure](https://docs.quiltdata.com/architecture) (like lambda, S3, cloud trail, elastic search, athena, etc.) to host the backend data store. Quilt manages data like code by creating a metadata layer on top of S3.\n",
    "\n",
    "According to the documentation:\n",
    "\n",
    "> All package metadata and data are stored in your S3 buckets. A slice of the package-level metadata, as well as S3 object contents, are sent to an ElasticSearch cluster managed by Quilt. All Quilt package manifests are accessible via SQL using AWS Athena.\n",
    ">\n",
    "> https://docs.quiltdata.com/architecture\n",
    ">\n",
    "> Quilt packages are one level of abstraction above S3 object versions. Object versions track mutations to a single file, whereas a quilt package references a collection files and assigns this collection a unique version.\n",
    ">\n",
    "> https://docs.quiltdata.com/more/faq#how-does-quilt-versioning-relate-to-s3-object-versioning\n",
    ">\n",
    "> In Quilt, S3 buckets are analogous to branches in git. Each bucket is a self-contained registry for one or more packages. As package data and schemas are refined, you can promote a package to a new bucket to signify its increased data quality.\n",
    ">\n",
    "> https://docs.quiltdata.com/mentalmodel#buckets-are-branches\n",
    "\n",
    "Quilt does provide the ability to look at revision history (commits) and diff various commits to show the changes. This functionality however is very different from git as it is provided via python api. Additionally, branch merging does not apprear to be fully featured; merges are implimented as an update, but the history of the source of the merge is not included in the new repo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a4ffef-8298-4de1-9a77-bf7b0747778f",
   "metadata": {},
   "source": [
    "### Project Nessie\n",
    "\n",
    "Nessie enables you to maintain multiple versions of your data tables and leverage a Git-like workflow (using branches, commits, tags, etc.).\n",
    "\n",
    "Nessie is an OSS service (rest api and browser ui) and set of libraries. The service is built on Java, leverages Quarkus, and is compiled to a GraalVM.\n",
    "\n",
    "Nessie extends existing table formats to provide a single serial view of transaction history. This is enabled across an unlimited number of tables, meaning that a transaction affecting multiple tables is able to be catalogued.\n",
    "\n",
    "Nessie enhances the following table formats with version control techniques:\n",
    "\n",
    "- Apache Iceberg (tables and views)\n",
    "- Delta Lake\n",
    "\n",
    "**Note**: Delta Lake support in Nessie requires some minor modifications to the core Delta libraries. This patch is still ongoing, in the meantime Nessie will not work on Databricks and must be used with the open source Delta. Nessie is able to interact with Delta Lake by implementing a custom version of Delta’s LogStore interface. This ensures that all filesystem changes are recorded by Nessie as commits. The benefit of this approach is the core ACID primitives are handled by Nessie. The limitations around concurrency that Delta would normally have are removed, any number of readers and writers can simultaneously interact with a Nessie managed Delta Lake table.\n",
    "https://projectnessie.org/tools/deltalake/\n",
    "\n",
    "Changes to the contents of the data lake are recorded in Nessie as commits without copying the actual data.\n",
    "\n",
    "Nessie offers a CLI that is installed through a pip package but it is [not currently fully featured](https://projectnessie.org/tools/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e6c5b2-d85c-4863-8828-3920a8baae23",
   "metadata": {},
   "source": [
    "### DataLad\n",
    "\n",
    "DataLad is a free and open source distributed data management system that keeps track of your data, creates structure, ensures reproducibility, supports collaboration, and integrates with widely used data infrastructure.\n",
    "https://www.datalad.org/\n",
    "\n",
    "A DataLad dataset is a directory with files, managed by DataLad. You can link other datasets, known as subdatasets, and perform commands recursively across an arbitrarily deep hierarchy of datasets. This helps you to create structure while maintaining advanced provenance capture abilities, versioning, and actionable file retrieval.\n",
    "\n",
    "Building on top of Git and git-annex, DataLad allows you to version control arbitrarily large files in datasets, without the need for custom data structures, central infrastructure, or third party services.\n",
    "\n",
    "DataLad is a free and open source command line tool with a Python API and is compatible with all major operating systems. Use DataLad to:\n",
    "\n",
    "-  create new datasets locally\n",
    "-  clone other datasets\n",
    "-  get content on-demand\n",
    "-  save changes to datasets\n",
    "-  drop content as needed\n",
    "-  push changes to a remote location\n",
    "\n",
    "https://www.datalad.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548197b5-743d-4f76-b237-71c0e873d2ab",
   "metadata": {},
   "source": [
    "## 2.4. Solutions Providing A Subset Of Functionality\n",
    "\n",
    "The following are a list of data science platforms that have versioning components but do not provide true version control for data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96936abd-c88c-4f1b-9464-bf8d5b995f28",
   "metadata": {},
   "source": [
    "### Neptune\n",
    "\n",
    "Neptune is a metadata store that offers experiment tracking and model registry for machine learning researchers and engineers. With Neptune, you can log, query, manage, display, and compare all your model metadata in a single place. \n",
    "\n",
    "Some users may elect to include the training and test data sets as artifacts stored in the metadata repository. While this is like version control, it is not version control for data.\n",
    "\n",
    "https://docs.neptune.ai/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4574e00-fcd9-4082-b45c-b1f0f410d4a5",
   "metadata": {},
   "source": [
    "### DagsHub\n",
    "\n",
    "DagsHub is a cloud based data science and machine learning platform.\n",
    "\n",
    "Behind the scenes, DagsHub uses DVC for versioning data. More information can be found [here](https://dagshub.com/docs/experiment_tutorial/2_data_versioning/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8872042-72b6-4c57-af2c-c4d3f58bdb5c",
   "metadata": {},
   "source": [
    "### Delta Lake\n",
    "Deltal lake is a data lakehouse implimentation. It provides time-travel, a form of version control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18b2bcc-1936-46ef-9c08-e1608c602e7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b0d793-c4e7-424c-af1e-96cf605de98c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
