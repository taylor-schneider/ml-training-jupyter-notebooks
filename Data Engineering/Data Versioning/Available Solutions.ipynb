{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "267428e7-ec59-4235-b241-6ef9836b1e87",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "In this notebook we review the various solutions and design patterns that exist on the market. For a few solutions we will also compare their architecture with the reference architecture we proposed in [this notebook](DVCS%20Reference%20Architecture.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9911d1-3edc-4d4a-850d-1a23661c35de",
   "metadata": {},
   "source": [
    "# Reccomendation\n",
    "\n",
    "As you can see below in the comparison matrix, there is not currently a \"one size fits all solution\". Every solution I have come accross has it's own limitations and use cases. Thus making a recommendation would depend on the circomstances. That being said, I would generally recommend sticking with a big data solution. LakeFS and Pachyderm have had great support thus far.\n",
    "\n",
    "Some of these solutions can however fit into the Reference Architecture outlined in [this notebook](DVCS%20Reference%20Architecture.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210a1445-c1d9-454b-952b-a7f4a2e70d0f",
   "metadata": {},
   "source": [
    "## Solution Feature Comparison\n",
    "\n",
    "<center><img src=\"images/dvcs-feature-matrix.png\"></center>\n",
    "\n",
    "See the [accompanying excel file](dvcs-feature-comparison.xlsx) to contribute new information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9fe512-19aa-46bc-ba3a-23303d913c8b",
   "metadata": {},
   "source": [
    "# Solutions Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2038173a-87fd-4258-bcbd-02c4696f5801",
   "metadata": {},
   "source": [
    "## 1.1. The Classic DIY Approach\n",
    "The classical approach has been for DBA's and system admins to maintain their own mechanisms for managing the various versions of a data set as it evolves over time. These solutions are generally built on top of the datastore, consists of a patchwork of code and 3rd party solutions, and impliment only a subset of features expected of a DVCS.\n",
    "\n",
    "Generally speaking there are two methods used in parallel:\n",
    "1. Taking system \"snapshots\" or backups (of the entire system or enough to rebuild the system manually) periodically via automation or manual efforts\n",
    "2. Integrating versioning metadata into the underlying schema of the data. This can be done by injecting information into file names or table names (eg. my_file_20203_v2 or my_table becomes my_table_2022 or mytable_v1).\n",
    "\n",
    "But these approaches don't provide all the functionality that a robust VCS would provide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bb467f-4f8e-4ac1-97ee-70609e006408",
   "metadata": {},
   "source": [
    "### 1.1.1. Example Using MS SQL Server\n",
    "\n",
    "https://blog.devart.com/database-versioning-with-examples.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64cdeb3-6109-4907-84ec-9d5079ec4567",
   "metadata": {},
   "source": [
    "### 1.1.2. \"As Of\" Syntax\n",
    "We have SQL \"as of\" syntax. This is in the SQL standard as of 2011 (pun intended) and supported by Oracle, Microsoft SQL Server, and MariaDB. With this syntax you can configure a table to be versioned or a \"temporal table\" and then query a version using as of <timestamp>. You don't get diffs and merges but you do get instant rollbacks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3fb2a5-e08a-4cfe-bc1b-c6768defdd13",
   "metadata": {},
   "source": [
    "## 1.2. Solutions For Popular Databases\n",
    "\n",
    "Martin Fowler wrote this [article on Evolutionary Database Design](https://martinfowler.com/articles/evodb.html) outlining some of the problems and features one would expect. Some of the solutions below address the issues and designs being proposed.\n",
    "\n",
    "In general, when we talk about applying version control to a database, holistically we are talkign about all the elements of a database including:\n",
    "- Database Schema\n",
    "- Database Object Definitions\n",
    "- Database Data\n",
    "- Database RBAC (roles, privileges, etc.)\n",
    "\n",
    "As we may have infered from the custom built solutions section, we may need to apply a different tool / technique to each component of the database. Below we look at examples that try to capture many/all of these elements in one solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36eb993-e97e-43c0-bb9c-57c7bfbedc54",
   "metadata": {},
   "source": [
    "### 1.2.1. Dolt (MySql Databases)\n",
    "\n",
    "Dolt is a MySQL database that impliments a git-like api which provides version control for tables and table rows.\n",
    "\n",
    "According to it's homepage: \n",
    "> Dolt is the first  and only SQL database that you can fork, clone, branch, merge, push and pull just like a Git repository. Dolt is a version controlled database. Dolt is Git for Data. Dolt is a Versioned MySQL Replica.\n",
    "> \n",
    "> Dolt implements the Git command line and associated operations on table rows instead of files. Data and schema are modified in the working set using SQL. When you want to permanently store a version of the working set, you make a commit. In SQL, Dolt implements Git read operations (ie. diff, log) as system tables and write operations (ie. commit, merge) as stored procedures. Dolt produces cell-wise diffs and merges, making data debugging between versions tractable. Dolt is the only SQL database that has branches and merges.\n",
    ">\n",
    ">You can run Dolt online, like you would PostgreSQL or MySQL. Or you can run Dolt offline, treating data and schema like source code.\n",
    ">\n",
    "> https://docs.dolthub.com/introduction/what-is-dolt\n",
    "\n",
    "With the addition of its support for MySql/MariaDB Binlog replication, Dolt can be attached to a primary database and can provider versioning for this primary replica.\n",
    "\n",
    "Dolt handles write operations through stored procedures and will present the difference between data versions cell-wise. For more information we can see the [following article](https://cult.honeypot.io/reads/dolt-a-sql-database-that-works-like-git)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066b620a-7586-4ec6-9e1d-33dec513f3b3",
   "metadata": {},
   "source": [
    "### 1.2.2. VersionSQL (MS SQL Server)\n",
    "\n",
    "VersionSQL is a plugin for the SQL Server Management Studio IDE. It allows DBA's to track and version changes to database code files from the SSMS interface. The plugin connects to remote VCS systems like Github.\n",
    "\n",
    "https://www.versionsql.com/sql-server-database/\n",
    "\n",
    "This may work for other databases like MySQL as SSMS is able to connect to multiple servers. https://docs.devart.com/odbc/mysql/microsoft_sql_server_manager_s.htm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e9f94e-026f-4e8d-83f8-2e35134cdbbc",
   "metadata": {},
   "source": [
    "## 1.3. Solutions For File Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b591a981-20be-4a44-af03-43afc4839990",
   "metadata": {},
   "source": [
    "### 1.3.1. Small Data Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae90ad85-9f93-4402-b5ee-77dc3e522339",
   "metadata": {},
   "source": [
    "#### DataLad\n",
    "\n",
    "DataLad is a free and open source distributed data management system that keeps track of your data, creates structure, ensures reproducibility, supports collaboration, and integrates with widely used data infrastructure.\n",
    "https://www.datalad.org/\n",
    "\n",
    "A DataLad dataset is a directory with files, managed by DataLad. You can link other datasets, known as subdatasets, and perform commands recursively across an arbitrarily deep hierarchy of datasets. This helps you to create structure while maintaining advanced provenance capture abilities, versioning, and actionable file retrieval.\n",
    "\n",
    "Building on top of Git and git-annex, DataLad allows you to version control arbitrarily large files in datasets, without the need for custom data structures, central infrastructure, or third party services.\n",
    "\n",
    "DataLad is a free and open source command line tool with a Python API and is compatible with all major operating systems. Use DataLad to:\n",
    "\n",
    "-  create new datasets locally\n",
    "-  clone other datasets\n",
    "-  get content on-demand\n",
    "-  save changes to datasets\n",
    "-  drop content as needed\n",
    "-  push changes to a remote location\n",
    "\n",
    "https://www.datalad.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f466a765-8abf-4b80-b966-b91b50503aef",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### DVC\n",
    "\n",
    "DVC stands for (Not Just) Data Version Control. The reason for this is that DVC offers versioning for the entire data science and machine learning workflow. It's homepage can be found [here](https://dvc.org/). The documentation bosts that DVC can version source code, data, machine learning experiemtns, and machine learning models. These later features will be left for another article discussing those topics. \n",
    "\n",
    "With respect to versioning data, DVC is alot like git-lfs and it integrates directly with git. The basic function is to tell git to ignore a data directory and to tell dvc to track that data directory. DVC will add files to the git repository that act as pointer to the actual data. The DVC command line is very similar to git meaining the concepts of branches, staging, commits, push, and pull still apply.\n",
    "\n",
    "One of the advantages over git-lfs is that DVC is able to store data in a larger array of backend storage repositories. DVC supports Amazon S3, Microsoft Azure Blob Storage, Google Drive, Google Cloud Storage, Aliyun OSS, SSH/SFTP, HDFS, HTTP, network-attached storage, or disc to store file contents.\n",
    "\n",
    "A helpful walkthrough can be found [here](https://towardsdatascience.com/introduction-to-dvc-data-version-control-tool-for-machine-learning-projects-7cb49c229fe0).\n",
    "\n",
    "DVC will face similar scalability issues like git-lfs. It also needs to pull data to a local machine and many users note that the system slows down as the number of files being tracked grows. This is likely because the solution is a program that runs on a single compute footprint and is not distributed or parallelized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcae6a4-1fd6-43b5-b6f9-043fde963fa9",
   "metadata": {},
   "source": [
    "#### Git-annex\n",
    "\n",
    "[Git-annex](https://git-annex.branchable.com/) is similar to git-lfs; allows managing large files with git, without storing the file contents in git. It is an older solution than git-lfs and supports a wide number of backends as opposed to git-lfs which only supports a git-lfs backend. That being said, it has some quarks between windows and linux use cases. It also suffers from the same scalability problem. More information on the differences can be found [here](https://stackoverflow.com/questions/39337586/how-do-git-lfs-and-git-annex-differ) and [here](https://www.perforce.com/manuals/gitswarm-ee/workflow/lfs/migrate_from_git_annex_to_git_lfs.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a331ca1-5677-49f9-b458-9dbabfb4c6e8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Git-LFS\n",
    "Git Large File Storage (LFS) replaces large files such as audio samples, videos, datasets, and graphics with text pointers inside Git, while storing the file contents on a remote server implimenting the git-lfs protocol (like GitHub.com or GitHub Enterprise).\n",
    "\n",
    "Git-lfs was designed to allow code project to version small set of data and binaries, not to version a data lake. Git-lfs suffers from a scalability problem. This is because the solution manages operations that should be implimented by the storage layer. For example, in order to read data from the repository, the user must sync their local copy of the data with the remote copy on the server. This, in effect, creates two copies of data. Additionally the time to checkout a particular version is dependent on the network connection. In some cases one might have to wait several minutes or hours to checkout a data set.\n",
    "\n",
    "https://git-lfs.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a0bc66-10a3-45d5-bde5-e66778ecd9a2",
   "metadata": {},
   "source": [
    "### 1.3.2. Big Data Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa27203-4ce5-4399-8eaf-68b0a3476416",
   "metadata": {},
   "source": [
    "#### Deep Lake (Formerly Activeloop Hub)\n",
    "\n",
    "Deep Lake is a data lake implimentation designed for deep learning. According to the documentation:\n",
    "\n",
    "> Deep Lake (formerly known as Activeloop Hub) is a data lake for deep learning applications. Our open-source dataset format is optimized for rapid streaming and querying of data while training models at scale, and it includes a simple API for creating, storing, and collaborating on AI datasets of any size. It can be deployed locally or in the cloud, and it enables you to store all of your data in one place, ranging from simple annotations to large videos.\n",
    "> \n",
    "> https://github.com/activeloopai/deeplake\n",
    "\n",
    "Looking through the [documentation](https://github.com/activeloopai/deeplake) I see that Deep Lake is implimentated as a python package which is able to upload, download, and stream datasets to/from AWS S3/S3-compatible storage, GCP, Activeloop cloud, or local storage.\n",
    "\n",
    "Deep Lake [dataset version control](https://docs.activeloop.ai/getting-started/dataset-version-control#how-to-use-version-control-in-deep-lake) allows you to manage changes to datasets with commands very similar to Git. It's api impliments the familiar concepts of commits, branches, merges, and diffing.\n",
    "\n",
    "A Shortcoming of this technology is that the APIs for loading data from the data store into models is only available for Pytorch and Tensorflow. Additionally, the technology is built around tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00e5952-f061-401c-a0a8-e50562250815",
   "metadata": {},
   "source": [
    "#### Hangar\n",
    "A python package which impliments the DVC functionality for tabular tensor (n-dimensional matrix) data. More information can be found [here](https://opendatascience.com/introduction-to-data-version-control/).\n",
    "\n",
    "The repository consists of data sets, collections of data samples. A data set consists of columns. Columns can be numeric, string, or byte strings. When interracting with the data contained in the respository, the python SDK will natively allow the storage and reference of columns in the repository. The CLI will allow the repository to be managed and for columns to be defined but not populated.\n",
    "\n",
    "For backends, Hangar supports HDF5, Local NP Memmap, LMDB, or a custom user defined plugin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f1aa7c-5281-446d-a1e1-77f5a78e3dbb",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Kamu\n",
    "\n",
    "Kemu is a solution for managing, transforming, and collaborating on structured data.\n",
    "\n",
    "In a larger context, kamu is a reference implementation of Open Data Fabric - a Web 3.0 protocol for providing timely, high-quality, and verifiable data for data science, smart contracts, web and applications.\n",
    "\n",
    "Kemu meintains unbreakable lineage and provenance trail in tamper-proof metadata, which lets you assess the trustworthyness of data, no matter how many hands and transformation steps it went through.\n",
    "\n",
    "It is currently in an [experimental state](https://docs.kamu.dev/odf/).\n",
    "\n",
    "According to the documentation:\n",
    "\n",
    "> Kamu achieves for data what Version Control Systems did for Software , but does so without diffs, versioning, or snapshotting.\n",
    ">\n",
    "> Our new paradigm streamlines collaboration on data within your company, and enables the effect similar to Open Source Software revolution for data globally.\n",
    "> \n",
    "> How it works\n",
    ">\n",
    "> 1. We turn data into a ledger\n",
    ">\n",
    "> Data preserves complete history, and never updated destructively. Trust is anchored at the publisher, so they can be always held accountable for data they provide.\n",
    ">\n",
    "> 2. Datasets are registered on the network\n",
    ">\n",
    "> As a publisher you don’t need to move data into any central point. You maintain complete ownership and control.\n",
    ">\n",
    "> 3. People process data using special SQL code\n",
    "> \n",
    "> Our decentralized ETL pipelines can span across teams, organizations, and even continents. People can collaborate on cleaning and enriching data and confidently reuse data from any step.\n",
    ">\n",
    "> 4. Data flows in near real-time\n",
    "> \n",
    "> Our streaming SQL engines process data within seconds, continuously and autonomously. All of your science projects, dashboards, and automation get the fidelity of stock tickers data.\n",
    ">\n",
    "> 5. Accountability, verifiability, and provenance built-in\n",
    ">\n",
    "> Our SQL has the properties of Smart Contracts, so you can trace every single data cell to its source, and easily tell who processed it and how.\n",
    ">\n",
    "> https://www.kamu.dev/\n",
    "\n",
    "kamu is a single-binary utility that comes bundled with most of its dependencies. The binary runs on linux, windows, and mac.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa0d09e-25aa-47f6-b679-2cab30a24337",
   "metadata": {},
   "source": [
    "#### LakeFS\n",
    "LakeFS is a git-like solution that sits on top of S3 (azure support is on the roadmap). It works by providing a metadata layer on top of the \"real files\". It keeps track of which \"real files\" are part of which version as well as what operations were performed between versions. This is very nice because there is no data duplication and no client/server that you might see with git-lfs or DVC.\n",
    "\n",
    "For more information on LakeFS see the [official documentation](https://lakefs.io/). I explore this solution in detail in [this article](./LakeFS/Intro%20To%20LakeFS.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859b47b1-5bca-455b-a695-70fae3e12d27",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Pachyderm\n",
    "Pachyderm is provides a DVCS implimentaion that provides functionality analogous to a minimal git implimentation; users can commit and branch the data repositories. The solution itself is hosted on kubernetes and can be deployed on cloud or on-prem. The solution has a community and enterprise edition available.\n",
    "\n",
    "The official documentation can be found [here](https://docs.pachyderm.com/latest/overview/). I explore this solution in the following [notebook](Pachyderm/Intro%20To%20Pachyderm.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a0faec-268e-47b7-9455-96e05cef33e6",
   "metadata": {},
   "source": [
    "#### Project Nessie\n",
    "\n",
    "Nessie enables you to maintain multiple versions of your data tables and leverage a Git-like workflow (using branches, commits, tags, etc.).\n",
    "\n",
    "It's [homepage](https://projectnessie.org/) lists the following:\n",
    "> Transactional Catalog for Data Lakes\n",
    "> - Git-inspired data version control\n",
    "> - Cross-table transactions and visibility\n",
    "> - Open data lake approach, supporting Hive, Spark, Dremio, AWS Athena, etc.\n",
    "> - Works with Apache Iceberg and Delta Lake tables\n",
    "> - Run as a docker image, AWS Lambda or fork it on GitHub\n",
    "\n",
    "Nessie is an OSS service (rest api and browser ui) and set of libraries. The service is built on Java, leverages Quarkus, and is compiled to a GraalVM.\n",
    "\n",
    "Nessie extends existing table formats to provide a single serial view of transaction history. This is enabled across an unlimited number of tables, meaning that a transaction affecting multiple tables is able to be catalogued.\n",
    "\n",
    "Nessie enhances the following table formats with version control techniques:\n",
    "\n",
    "- Apache Iceberg (tables and views)\n",
    "- Delta Lake\n",
    "\n",
    "**Note**: Delta Lake support in Nessie requires some minor modifications to the core Delta libraries. This patch is still ongoing, in the meantime Nessie will not work on Databricks and must be used with the open source Delta. Nessie is able to interact with Delta Lake by implementing a custom version of Delta’s LogStore interface. This ensures that all filesystem changes are recorded by Nessie as commits. The benefit of this approach is the core ACID primitives are handled by Nessie. The limitations around concurrency that Delta would normally have are removed, any number of readers and writers can simultaneously interact with a Nessie managed Delta Lake table.\n",
    "https://projectnessie.org/tools/deltalake/\n",
    "\n",
    "Changes to the contents of the data lake are recorded in Nessie as commits without copying the actual data.\n",
    "\n",
    "Nessie offers a CLI that is installed through a pip package but it is [not currently fully featured](https://projectnessie.org/tools/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9774c11e-8ce4-4093-82a1-04952e983f74",
   "metadata": {},
   "source": [
    "#### Quilt\n",
    "\n",
    "Quilt describes itself as a self-organizing data hub. \n",
    "\n",
    "> Quilt consists of a Python client, web catalog, lambda functions—all of which are open source—plus a suite of backend services and Docker containers orchestrated by CloudFormation.\n",
    ">\n",
    "> The backend services are available under a paid license on quiltdata.com.\n",
    "> https://github.com/quiltdata/quilt\n",
    "\n",
    "Quilt manages data like code by creating a metadata layer on top of S3.\n",
    "\n",
    "According to the documentation:\n",
    "\n",
    "> All package metadata and data are stored in your S3 buckets. A slice of the package-level metadata, as well as S3 object contents, are sent to an ElasticSearch cluster managed by Quilt. All Quilt package manifests are accessible via SQL using AWS Athena.\n",
    ">\n",
    "> https://docs.quiltdata.com/architecture\n",
    ">\n",
    "> Quilt packages are one level of abstraction above S3 object versions. Object versions track mutations to a single file, whereas a quilt package references a collection files and assigns this collection a unique version.\n",
    ">\n",
    "> https://docs.quiltdata.com/more/faq#how-does-quilt-versioning-relate-to-s3-object-versioning\n",
    ">\n",
    "> In Quilt, S3 buckets are analogous to branches in git. Each bucket is a self-contained registry for one or more packages. As package data and schemas are refined, you can promote a package to a new bucket to signify its increased data quality.\n",
    ">\n",
    "> https://docs.quiltdata.com/mentalmodel#buckets-are-branches\n",
    "\n",
    "Quilt does provide the ability to look at revision history (commits) and diff various commits to show the changes. This functionality however is very different from git as it is provided via python api. Additionally, branch merging does not apprear to be fully featured; merges are implimented as an update, but the history of the source of the merge is not included in the new repo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548197b5-743d-4f76-b237-71c0e873d2ab",
   "metadata": {},
   "source": [
    "## 2.4. Non-DVCS Solutions Providing A Subset Of DVCS Functionality\n",
    "\n",
    "The following are a list of data science platforms that have versioning components but do not provide true version control for data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96936abd-c88c-4f1b-9464-bf8d5b995f28",
   "metadata": {},
   "source": [
    "### Neptune\n",
    "\n",
    "Neptune is a metadata store that offers experiment tracking and model registry for machine learning researchers and engineers. With Neptune, you can log, query, manage, display, and compare all your model metadata in a single place. \n",
    "\n",
    "Some users may elect to include the training and test data sets as artifacts stored in the metadata repository. While this is like version control, it is not version control for data.\n",
    "\n",
    "https://docs.neptune.ai/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4574e00-fcd9-4082-b45c-b1f0f410d4a5",
   "metadata": {},
   "source": [
    "### DagsHub\n",
    "\n",
    "DagsHub is a cloud based data science and machine learning platform.\n",
    "\n",
    "Behind the scenes, DagsHub uses DVC for versioning data. More information can be found [here](https://dagshub.com/docs/experiment_tutorial/2_data_versioning/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8872042-72b6-4c57-af2c-c4d3f58bdb5c",
   "metadata": {},
   "source": [
    "### Delta Lake\n",
    "Deltal lake is a data lakehouse implimentation. It provides time-travel, a form of version control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18b2bcc-1936-46ef-9c08-e1608c602e7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b0d793-c4e7-424c-af1e-96cf605de98c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
