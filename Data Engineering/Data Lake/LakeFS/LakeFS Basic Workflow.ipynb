{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bccb6892-9e48-480b-b07d-9c5b17b0900d",
   "metadata": {},
   "source": [
    "# Overview\n",
    "In this notebook we are going to work with LakeFS to setup and manage our first repository. \n",
    "We will use several clients to interact with this repository. I will be using the local storage backend."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f4593e-c300-4b9f-ad7f-bdb3e3416779",
   "metadata": {},
   "source": [
    "# 1. Prerequisites\n",
    "This assumes we have a working installation and followed the setps in the [Basic Initial Setup](Basic%20Initial%20Setup.ipynb) notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcab060-8a29-45b7-9592-9f9b8e246458",
   "metadata": {},
   "source": [
    "# 2. Create A Repository\n",
    "Once logged in, we see that there are no repositories.\n",
    "\n",
    "<center><img src=\"images/lakefs-create-repo.png\" style=\"width:800px\"></center>\n",
    "\n",
    "As we see, there are two options to select from when creating a repository. \n",
    "\n",
    "<center><img src=\"images/lakefs-create-repo-choice.png\" style=\"width:800px\"></center>\n",
    "\n",
    "At the end of the day, both these options lead to the same end; there is only one type of repo in lakefs. \n",
    "\n",
    "<center><img src=\"images/lakefs-repos-view.png\" style=\"width:800px\"></center>\n",
    "\n",
    "But for completeness we will walk through both options in the create repo wizard."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0a84f3-f97a-4022-8ffc-2e580402e114",
   "metadata": {},
   "source": [
    "## 2.1. Creating a blank repository\n",
    "\n",
    "The [official documentation](https://docs.lakefs.io/quickstart/repository.html#create-the-repository) covers this scenario. After selecting the repo type, the wiard takes us to the following screen\n",
    "\n",
    "<center><img src=\"images/lakefs-create-blank-repo.png\" style=\"width:800px\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950c2ef4-af21-46fe-b42f-48d11703d97c",
   "metadata": {},
   "source": [
    "I had a look through the documentation to understand the various fields.\n",
    "\n",
    "The **Repository ID** is a unique identifier, similar to the repo name one would choose in a git repository.\n",
    "\n",
    "The **Storge Namespace** refers to the [underlying storage](https://docs.lakefs.io/understand/model.html#concepts-unique-to-lakefs). According to the documentation:\n",
    "\n",
    "The underlying storage is a location in an object store where lakeFS keeps your objects and some immutable metadata.\n",
    "The repository’s storage namespace is a location in the underlying storage where data for this repository will be stored.\n",
    "\n",
    "Additionally, LakeFS sometimes refers to underlying storage as physical (storage). The path used to store the contents of an object is then termed a physical path. \n",
    "\n",
    "The value for Storage Namespace has one of two prefixes to denote the two supported storage types. The local prefix allows us to reference a path on our local system while the s3 prefix allows us to reference a storage location in an s3 compliant backend. For example:\n",
    "- local://path \n",
    "- s3://example-bucket/prefix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b31c3f0-7c7d-4f6c-9d03-80e7dcec97ce",
   "metadata": {},
   "source": [
    "<center><img src=\"images/lakefs-create-repo-values.png\" style=\"width:800px\"></center>\n",
    "\n",
    "Once created we see the following page\n",
    "\n",
    "<center><img src=\"images/lakefs-created-blank-repo-page.png\" style=\"width:800px\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368f7a6b-0e1d-4077-9243-160d8896d321",
   "metadata": {},
   "source": [
    "I was curious about where the files were on the local filesystem. I looked at the configured root directory and saw that a directory was created. I also saw that a dummy file was placed into the directory as a sanity check.\n",
    "\n",
    "```\n",
    "[root@localhost lakefs]# pwd\n",
    "/data/datalake/lakefs\n",
    "\n",
    "[root@localhost lakefs]# ls -la\n",
    "total 0\n",
    "drwx------ 1 root root 1 Sep  8 17:21 .\n",
    "drwxr-xr-x 1 root root 2 Sep  8 16:57 ..\n",
    "drwxr-x--- 1 root root 1 Sep  8 17:21 test-blank-repo-namespace\n",
    "\n",
    "[root@localhost lakefs]# ls -la test-blank-repo-namespace/\n",
    "total 1\n",
    "drwxr-x--- 1 root root  1 Sep  8 17:21 .\n",
    "drwx------ 1 root root  1 Sep  8 17:21 ..\n",
    "-rw-r--r-- 1 root root 70 Sep  8 17:21 dummy\n",
    "\n",
    "[root@localhost lakefs]# ls -la test-blank-repo-namespace/dummy\n",
    "-rw-r--r-- 1 root root 70 Sep  8 17:21 test-blank-repo-namespace/dummy\n",
    "\n",
    "[root@localhost lakefs]# cat test-blank-repo-namespace/dummy\n",
    "this is dummy data - created by lakeFS in order to check accessibility\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ca381e-fe70-496d-9dde-96ead5566bd8",
   "metadata": {},
   "source": [
    "## 2.2. Creating a Spark Quickstart Repo\n",
    "\n",
    "**Note**: Because we are using the local storage backend we wont be able to explore all the functionality of this option. Not to worry, we will revisit later with a more complex configuration.\n",
    "\n",
    "We now select the  \"Spark Quickstart\" option from the drop down to create a repository.\n",
    "\n",
    "<center><img src=\"images/lakefs-create-spark-repo.png\" style=\"width:800px\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe016d3-a18e-42dc-a597-d47ed0ceb80d",
   "metadata": {},
   "source": [
    "**Note**: This feature was not particularely well documented. There is a [design document](https://github.com/treeverse/lakeFS/blob/e7f9f9de4711c1518ba959243fc38e45af7900d1/design/open/ttv-wizard-mvp.md) in the github repo that outlines how this wizard works at a high level. I also found [this github issue](https://github.com/treeverse/lakeFS/issues/3411) that lays out the rationale behind adding this component to the wizrard being related to creature comfort and convenience:\n",
    "\n",
    "> To get started using a new lakeFS installation with Spark, the following manual steps are typically required:\n",
    ">\n",
    "> 1. Create a repository and configure it with a storage namespace\n",
    "> 2. Bring in existing data (whether using import, copy, ingest, upload(?), etc)\n",
    "> 3. Connect Spark using the installation's S3 Gateway or configure Spark to use lakeFSFS with this installation\n",
    "> 4. Understand how to use lakeFS structured URIs in the existing code base or more likely configure metastore for this indirection to have existing code run on a separate branch/commit (i.e. I want SELECT * FROM events; to use the events table that exists in my experiment-oz-temp branch, not main or production!)\n",
    "> 5. Now, integrate Hive Metastore/Glue Data Catalog with lakeFS to be able to have a schema for my branch that contains all the same tables that I have in prod.\n",
    ">\n",
    "\n",
    "\n",
    "\n",
    "I had to look through the source code to understand error messages I was getting. I will add nuggets of info as we go along.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f37f64-5671-45fc-8fb3-68815abf15b6",
   "metadata": {},
   "source": [
    "We then see the wizard activate and we see a UI similar to that of the blank repo.\n",
    "\n",
    "<center><img src=\"images/lakefs-create-spark-repo-2.png\" style=\"width:800px\"></center>\n",
    "\n",
    "<center><img src=\"images/lakefs-create-spark-repo-3.png\" style=\"width:400px\"></center>\n",
    "\n",
    "Clicking next we then have the ability to import data. As noted in the UI:\n",
    "\n",
    "> Import doesn't copy objects. It only creates links to the objects in the lakeFS metadata layer. Don't worry, we will never change objects in the import source. [Learn more](https://docs.lakefs.io/setup/import.html).\n",
    "\n",
    "<center><img src=\"images/lakefs-create-spark-repo-4.png\" style=\"width:400px\"></center>\n",
    "\n",
    "I created some dummy data so we could see what the import looked actually did.\n",
    "\n",
    "```\n",
    "[root@localhost lakefs]# mkdir /data/datalake/dummy-data\n",
    "[root@localhost lakefs]# echo \"Hello, World\" >  /data/datalake/dummy-data/my_file.txt\n",
    "```\n",
    "\n",
    "I plugged in the values and clicked next\n",
    "\n",
    "<center><img src=\"images/lakefs-create-spark-repo-5.png\" style=\"width:400px\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9471d06-c31b-47ec-8380-572f4df34c21",
   "metadata": {},
   "source": [
    "But this raised an error. On my screen I saw a red error message warning me that there was a problem.\n",
    "\n",
    "<center><img src=\"images/lakefs-create-spark-repo-6.png\" style=\"width:400px\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78614483-905c-400e-b8b5-d738229b856b",
   "metadata": {},
   "source": [
    "I searched through the source code to find instances where this phrasing is used. I saw that the WalkerFactory's GetWalker function does not support a local storage backend. Looking at the [source code](https://github.com/treeverse/lakeFS/blob/71db8f37656ad021f3178a376651519ed24f4cf7/pkg/ingest/store/factory.go#L158) it looks like the only supported URI schemes at this point are s3://, gs:// for GCP, or http:// and https:// for Azure.\n",
    "\n",
    "I was a bit confused by this because I was able to create a repo and upload files to the local storage backend. After looking at the code and consulting blogs and documentation I realized that the issue is simply that the \"walker\" (the thing that walks the file system, enumerates file paths, and reads files) was not programmed to work on the local filesystem. After chatting in the official [slack channel](https://go.lakefs.io/JoinSlack) that this is because it is assumed that spark cannot access the files on the local filesystem of the lakefs server. The developers had not considered the case where I had mounted a remote filesystem to all the nodes.\n",
    "\n",
    "As a work around we have a few options:\n",
    "1. Skip the wizard and upload manually as we do with the basic repo\n",
    "2. Spin up an http file server to serve our local files\n",
    "3. Spin up MiniO (an S3 compliant api)\n",
    "4. Switch to an S3 data store\n",
    "\n",
    "For now I will skip this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2e74c7-7082-4c7c-bd41-9a8017eeaf08",
   "metadata": {},
   "source": [
    "Lastly we see the spark configurations page which gives us the spark configurations required to connect to our datastore.\n",
    "\n",
    "<center><img src=\"images/lakefs-create-spark-repo-7.png\" style=\"width:400px\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082e3f91-b498-4a65-b106-5811f329d721",
   "metadata": {},
   "source": [
    "# 3. Branching And Merging\n",
    "\n",
    "In this next section we look at how branching and merging works. We will not go through any complex branching strategy. We will just explore the basics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0a3bc0-bce0-42ad-87d9-c57b31f0b354",
   "metadata": {},
   "source": [
    "## 3.1. Uploading a document to main branch\n",
    "\n",
    "In this example we upload a simple text file from our local filesystem to the main branch on the lakefs server through the gui.\n",
    "\n",
    "<center><img src=\"images/lakefs-upload-1.png\" style=\"width:800px\"></center>\n",
    "<center><img src=\"images/lakefs-upload-2.png\" style=\"width:400px\"></center>\n",
    "<center><img src=\"images/lakefs-upload-3.png\" style=\"width:800px\"></center>\n",
    "<center><img src=\"images/lakefs-upload-4.png\" style=\"width:400px\"></center>\n",
    "\n",
    "**Note**: This file simply contains the text \"Hello, World!\"\n",
    "\n",
    "<center><img src=\"images/lakefs-upload-5.png\" style=\"width:800px\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43642990-aa4b-417e-b2bc-086ce5a896a6",
   "metadata": {},
   "source": [
    "## 3.2. Making a commit\n",
    "\n",
    "Looking at the uncommitted changes tab we see that the file is recognized as being changed and we have the option of making a commit.\n",
    "\n",
    "<center><img src=\"images/lakefs-changes-1.png\" style=\"width:800px\"></center>\n",
    "<center><img src=\"images/lakefs-changes-2.png\" style=\"width:400px\"></center>\n",
    "<center><img src=\"images/lakefs-changes-3.png\" style=\"width:800px\"></center>\n",
    "<center><img src=\"images/lakefs-changes-4.png\" style=\"width:800px\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfb96be-1cf6-4083-bebc-029dbe4225b3",
   "metadata": {},
   "source": [
    "## 3.3. Understand changes in filesystem\n",
    "\n",
    "I now wanted to understand how stages changes and commits were represented on the underlying file system. I created a second file with the name test2 and the text \"Hellow, world! 2\". I then had a look at the underlying filesystem.\n",
    "\n",
    "We can see that two files appear in the repositoy. Although renamed, the fils contain the contents we uploaded.\n",
    "\n",
    "```\n",
    "[root@localhost test-blank-repo-namespace]# ls -la\n",
    "total 2\n",
    "drwxr-x--- 1 root root  4 Sep  9 03:50 .\n",
    "drwx------ 1 root root  2 Sep  9 03:47 ..\n",
    "-rw-r--r-- 1 root root 13 Sep  9 03:47 4b7c506d1e0449bfbd5ec165d93d3f25\n",
    "-rw-r--r-- 1 root root 15 Sep  9 03:50 637a87413c314022b23af24590d8f60f\n",
    "-rw-r--r-- 1 root root 70 Sep  9 03:47 dummy\n",
    "drwxr-x--- 1 root root  2 Sep  9 03:50 _lakefs\n",
    "\n",
    "[root@localhost test-blank-repo-namespace]# cat 4b7c506d1e0449bfbd5ec165d93d3f25\n",
    "hello, world![root@localhost test-blank-repo-namespace]#\n",
    "\n",
    "[root@localhost test-blank-repo-namespace]# cat 637a87413c314022b23af24590d8f60f\n",
    "hello, world! 2\n",
    "```\n",
    "\n",
    "Additionally we see a _lakefs directory\n",
    "\n",
    "```\n",
    "[root@localhost test-blank-repo-namespace]# ls -la _lakefs/\n",
    "total 3\n",
    "drwxr-x--- 1 root root    2 Sep  8 23:51 .\n",
    "drwxr-x--- 1 root root    4 Sep  8 23:56 ..\n",
    "-rw-r--r-- 1 root root 1058 Sep  8 23:51 8565a8d7cf787aaf005af5b5e6e145f8c62cce6544eb3b0e87acd17c10\n",
    "e666c6\n",
    "-rw-r--r-- 1 root root 1018 Sep  8 23:51 b3f4878b4f146f36da8800fc12635176903622f9655673aad972c9af13\n",
    "208b83\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9307a818-6c99-4c85-bdfa-c967730a0f3b",
   "metadata": {},
   "source": [
    "It's a bit complicated to explain what exactly is in this directory. Long story short the directory contains commit metadata. Each file contains information about which objects are contained in which commit. By committing, the metadata that was in the Postgres database has been committed to the backend S3 system inside of the _lakefs prefix. Each of the files with the _lakefs prefix is a Graveler file, which is an “immutable” SSTable with metadata compatible with RocksDB. The name of the file itself is a function of its content (we say the file is “content-addressable”).\n",
    "\n",
    "More information can be found in the [versioning internals](https://docs.lakefs.io/understand/versioning-internals.html) documentation or [this article](https://blog.dataminded.com/what-is-lakefs-a-critical-survey-edce708a9b8e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e699a2fb-d97d-4ac5-899f-c0bda8d6e9e2",
   "metadata": {},
   "source": [
    "## 3.4. Creating a branch\n",
    "\n",
    "<center><img src=\"images/lakefs-create-branch.png\" style=\"width:800px\"></center>\n",
    "<center><img src=\"images/lakefs-create-branch-2.png\" style=\"width:400px\"></center>\n",
    "<center><img src=\"images/lakefs-create-branch-3.png\" style=\"width:800px\"></center>\n",
    "<center><img src=\"images/lakefs-create-branch-4.png\" style=\"width:800px\"></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b90b9e-7f10-4445-9719-daa2ec87766a",
   "metadata": {},
   "source": [
    "Looking at the underlying filesystem we see that data was not duplicated! We still see the original comitted file and the staged file from the main branch. we can see that lakefs is not moving files in the same way that git was. This is great for scalability!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292b701c-f2e6-43bb-b7a1-709e9a585606",
   "metadata": {},
   "source": [
    "I uploaded a test3 file to the branch and we see it was added to the filesystem\n",
    "\n",
    "```\n",
    "[root@localhost test-blank-repo-namespace]# ls -la\n",
    "total 2\n",
    "drwxr-x--- 1 root root  5 Sep  9 03:53 .\n",
    "drwx------ 1 root root  2 Sep  9 03:47 ..\n",
    "-rw-r--r-- 1 root root 13 Sep  9 03:47 4b7c506d1e0449bfbd5ec165d93d3f25\n",
    "-rw-r--r-- 1 root root 15 Sep  9 03:53 5b64d28e92ca43e5a285321b1d017405\n",
    "-rw-r--r-- 1 root root 15 Sep  9 03:50 637a87413c314022b23af24590d8f60f\n",
    "-rw-r--r-- 1 root root 70 Sep  9 03:47 dummy\n",
    "drwxr-x--- 1 root root  2 Sep  9 03:50 _lakefs\n",
    "\n",
    "[root@localhost test-blank-repo-namespace]# cat 5b64d28e92ca43e5a285321b1d017405\n",
    "hello, world! 3\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae52b8c-dffd-4ef4-b3a3-508926b74cd8",
   "metadata": {},
   "source": [
    "## 3.5. Modifying files\n",
    "like git, I have the ability to modify staged or comitted files on a branch. The way to do this in the gui is simply upload the new revision and specify an existing file name in the dialogue box. In doing so, we will see the ui is able to distinguish between additions and modifications.\n",
    "\n",
    "<center><img src=\"images/lakefs-modifications.png\" style=\"width:800px\"></center>\n",
    "\n",
    "I noticed we cannot compare the uncommitted changes no matter how I configure the comparison.\n",
    "\n",
    "<center><img src=\"images/lakefs-modify-2.png\" style=\"width:800px\"></center>\n",
    "\n",
    "We can only compare the comitted changes\n",
    "\n",
    "<center><img src=\"images/lakefs-modify-4.png\" style=\"width:800px\"></center>\n",
    "\n",
    "Making a commit we see the the information included in the commit.\n",
    "\n",
    "<center><img src=\"images/lakefs-modify-3.png\" style=\"width:800px\"></center>\n",
    "\n",
    "Looking at the filesystem we see that old files remain (we may want to revert to them in the future).\n",
    "\n",
    "```\n",
    "[root@localhost test-blank-repo-namespace]# ls -la\n",
    "total 3\n",
    "drwxr-x--- 1 root root  6 Sep  9 03:56 .\n",
    "drwx------ 1 root root  2 Sep  9 03:47 ..\n",
    "-rw-r--r-- 1 root root 13 Sep  9 03:47 4b7c506d1e0449bfbd5ec165d93d3f25\n",
    "-rw-r--r-- 1 root root 15 Sep  9 03:53 5b64d28e92ca43e5a285321b1d017405\n",
    "-rw-r--r-- 1 root root 15 Sep  9 03:50 637a87413c314022b23af24590d8f60f\n",
    "-rw-r--r-- 1 root root 15 Sep  9 03:56 daaf5fb4fde44d3e8a7e81553e3f7834\n",
    "-rw-r--r-- 1 root root 70 Sep  9 03:47 dummy\n",
    "drwxr-x--- 1 root root  4 Sep  9 04:01 _lakefs\n",
    "\n",
    "[root@localhost test-blank-repo-namespace]# cat 4b7c506d1e0449bfbd5ec165d93d3f25\n",
    "hello, world!\n",
    "\n",
    "[root@localhost test-blank-repo-namespace]# cat 637a87413c314022b23af24590d8f60f\n",
    "hello, world! 2\n",
    "\n",
    "[root@localhost test-blank-repo-namespace]# cat 5b64d28e92ca43e5a285321b1d017405\n",
    "hello, world! 3\n",
    "\n",
    "[root@localhost test-blank-repo-namespace]# cat daaf5fb4fde44d3e8a7e81553e3f7834\n",
    "hello, world! 4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76d44a4-f6db-4538-8fe7-75fab88bf1e3",
   "metadata": {},
   "source": [
    "## 3.6 Deleting Files\n",
    "When we delete files they remain on disk. According to [the documentaion](https://docs.lakefs.io/reference/garbage-collection.html):\n",
    "\n",
    "> By default, lakeFS keeps all your objects forever. This allows you to travel back in time to previous versions of your data. However, sometimes you may want to hard-delete your objects - namely, delete them from the underlying storage. Reasons for this include cost-reduction and privacy policies.\n",
    "\n",
    "> Garbage collection rules in lakeFS define for how long to retain objects after they have been deleted (see more information below). lakeFS provides a Spark program to hard-delete objects that have been deleted and whose retention period has ended according to the GC rules. The GC job does not remove any commits: you will still be able to use commits containing hard-deleted objects, but trying to read these objects from lakeFS will result in a 410 Gone HTTP status.\n",
    "\n",
    "> Note At this point, lakeFS supports Garbage Collection only on S3 and Azure. We have concrete plans to extend the support to GCP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc758b2c-d8e1-4241-bc04-da32aab14a92",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3.7. Merging Branches\n",
    "\n",
    "At this point would like to merge my feature branch back into my main branch.\n",
    "\n",
    "<center><img src=\"images/lakefs-merge-1.png\" style=\"width:800px\"></center>\n",
    "<center><img src=\"images/lakefs-merge-2.png\" style=\"width:400px\"></center>\n",
    "<center><img src=\"images/lakefs-merge-3.png\" style=\"width:400px\"></center>\n",
    "<center><img src=\"images/lakefs-merge-4.png\" style=\"width:400px\"></center>\n",
    "<center><img src=\"images/lakefs-merge-5.png\" style=\"width:800px\"></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e53fc3f-b06c-4b80-9eba-d7d7f74e2a69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1138f58f-c743-44a6-b4db-a25115c3cfe2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
