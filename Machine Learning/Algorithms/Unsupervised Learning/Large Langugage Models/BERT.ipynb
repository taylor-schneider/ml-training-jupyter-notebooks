{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f3e3bf8-a426-4c5d-a3df-7a945f43ffdc",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b018890-b33a-4fad-9dc3-4fad7abb6295",
   "metadata": {},
   "source": [
    "BERT was [published](https://arxiv.org/abs/1810.04805) in 2018 by a team of engineers within google's AI division.\n",
    "\n",
    "BERT provides a new definition for a Transformer's encoder offering better accuracy and the ability to provide transfer learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c86f236-33b6-485c-8a3a-29ba073786c7",
   "metadata": {},
   "source": [
    "# What is BERT?\n",
    "The acronymn BERT stands for Bidirectional Encoder Representations from Transformers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4170ebd-ac78-4d33-910b-412b55d4a4a8",
   "metadata": {},
   "source": [
    "## Language [R]epresentation Model\n",
    "\n",
    "This is a lot to unpack, lets start with the terms Representation:\n",
    "\n",
    "It's authors characterize BERT as a language representation model. This term was a bit confusing because I could not find an explicit explanation of the distinction between it, and a regular language model. I was however able to deduce the meaning from context. I found the following passage in the paper:\n",
    "\n",
    ">  they use ... language models to learn general language representations. ... BERT uses masked language models to enable pretrained deep bidirectional representations.\n",
    "\n",
    "From this context, I believe the language model itself is the stateless architecture while the langauge representation is the weights (and other internal state) associated with the data that the model was trained on. This would be consistent with the idea that the language model represnetations are used by downstream models:\n",
    "\n",
    "> There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo (Peters et al., 2018a), uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (OpenAI GPT) (Radford et al., 2018), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.\n",
    "\n",
    "So in this case, colloquially, when one talkes about a pre-trained model or a fine-tuned model, they are liekly referring to the representation and not the model when discussing situations where training has already been preformed and we are simply making predictions using the model and the pre-existing weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bcbd04-deba-444c-8b84-0ce0bdd1bd5c",
   "metadata": {},
   "source": [
    "## From [T]ransformers\n",
    "\n",
    "So now that we know what a representation is, the phrase \"Representation From Transformer\" starts to make sense. The representations are produced by a model, and in this case, that model is a transformer. BERT is a thing produced from a type of Transformer. We will continue to clarify what that stuff is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65770505-f5c0-47b1-b90d-32c2643e4223",
   "metadata": {},
   "source": [
    "## [B]idirectional Context\n",
    "\n",
    "Now lets talk about the term bidirectional. \n",
    "\n",
    "In this case bidirectionality qualifies the contex objets (i.e. the word embeddings gereated by the Transformer's encoder) used by BERT's attention mechanism. Recall that the context is what gives tokens (i.e. words or fragments) their meaning and what is used to generate the probabilities used to predict the next word in a sequence. Bith BERT, the language representation is jointly conditioned on both left and right context. This means the model considers the tokens to the left (i.e. before) of a given token and the tokens to the right (i.e. the tokens that empirically occur after). Additionally, BERT considers the bidirectional context in all layer of the neural network. \n",
    "\n",
    "The authors state that bidirectionality represents a majority of the emperical improvements in BERT's model performance. They claim that prior to this, the deep representations were sub-optimal because they were trained unidirectional (left-to-right in the case of OpenAI GPT) or they are trained on labled data, or both."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c619df1-95fc-44b8-ac19-cc2f35c9f3c0",
   "metadata": {},
   "source": [
    "## Sequence Pair [E]ncoder\n",
    "\n",
    "Next lets unpack the term Encoder.\n",
    "\n",
    "The paper reviews the history of pre-training general language represnetaions. Around 2015, it notes that sentence or document encoders started being used to produce contextual token representations which are then presumably fine-tuned and consumed by downstream processes or tasks (eg. decoders).\n",
    "\n",
    "The paper spells out that BERT is an advancement of the original transformer encoder design: \n",
    "\n",
    "> BERTâ€™s model architecture is a multi-layer bidirectional Transformer encoder based on the original implementation described in Vaswani et al. (2017) and released in the tensor2tensor library\n",
    ">\n",
    "> use of Transformers has become common and our implementation is almost identical to the original ... (Vaswani et al. (2017)).\n",
    "\n",
    "The difference here is that BERT's encoder was deigned to \"unambiguously\" represent either a single sequence or a pair of sequences (e.g. question and answer) as an input. In this way BERT is generic and able to be applied to a number of downstream tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44a0dc0-7482-40c8-a02e-3fefd35769dd",
   "metadata": {},
   "source": [
    "# Use Case: Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b841361-c35b-485d-bb92-695a7103807c",
   "metadata": {},
   "source": [
    "Asside from all the other advancements made by BERT, [transfer learning](./Transfer%20Learning%20And%20Pre-Trained%20Models.ipynb) sits close to the top. The term transfer learning was a bit confusing, I found it simpler to think of it as conducting knowledge transfers, or transfering knowledge, between machine learning models.\n",
    "\n",
    "The paper describes BERT as a framework, stating:\n",
    "\n",
    "> There are two steps in our framework: pre-training and fine-tuning. During pre-training, the model is trained on unlabeled data over different pre-training tasks. For finetuning, the BERT model is first initialized with the pre-trained parameters, and all of the parameters are fine-tuned using labeled data from the downstream tasks. Each downstream task has separate fine-tuned models, even though they are initialized with the same pre trained parameters.\n",
    "\n",
    "Digging a bit deeper, we can infer that the basic premise of the BERT framework is to provide a blueprint for transfer learning implimentations which is made possible through the architectural requirements of the framework:\n",
    "\n",
    "> A distinctive feature of BERT is its unified architecture across different tasks. There is minimal difference between the pre-trained architecture and the final downstream architecture.\n",
    "\n",
    "The authors propose that the BERT can be used as a pre-trained model and that it can be fine tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task specific architecture modifications. This adds huge operational efficiencies to the model tuning process as it's assumed that one would fine-tune a pretrained model rather than start from scratch saving a lot of time by cutting a substantial amount of training iterations out of the process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49c1dfc-85c8-4121-aa2f-92b4dbf17843",
   "metadata": {},
   "source": [
    "# Implimentations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319d1c1c-e8fd-4d56-b358-43cc4234205b",
   "metadata": {},
   "source": [
    "## BERT and Smaller BERT Models\n",
    "In 2018, the original BERT model was uploaded to [github](https://github.com/google-research/bert). Since then multiple additional releases have taken place including the release in ~ March 2020, a set of \"smaller\" BERT Models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f67620-1502-4c3d-95e8-8f09df842aac",
   "metadata": {},
   "source": [
    "## tensor2tensor\n",
    "The paper cites the [tensor2tensor](https://github.com/tensorflow/tensor2tensor) library as the multi-layer bidirectional Transformer encoder implimentation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
