{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0c5463c-59f4-4fb6-b35b-e2d72b7dde9a",
   "metadata": {},
   "source": [
    "# Overview\n",
    "I wanted to understand the various ways we can test LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0035bc-b222-4dbe-95f2-281afca53bc2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "01741a10-7b71-4654-8f29-7df268d46499",
   "metadata": {},
   "source": [
    "# LLM Use Cases "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefdd135-0091-4fd6-b998-0d305a5b14df",
   "metadata": {},
   "source": [
    "LLMs have a number of use cases. Generally speaking those largely fall into the following categories:\n",
    "- Translation\n",
    "- Sequence Generation\n",
    "    - Summarization\n",
    "    - Prompt/Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21170c22-c82b-4971-80ef-75bf77a6609b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3540b0-7d41-47d0-9508-05deb29d4e2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ccb377ed-1655-4485-870b-9c4708551e44",
   "metadata": {},
   "source": [
    "# Test Objetives\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d8b692-c59a-4cf8-a269-1f4af767d753",
   "metadata": {},
   "source": [
    "The intuition for evaluating generated text is the same as that for evaluating labels. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952d649f-1c21-4e84-9640-eed24c21aa90",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3a261f-1bae-4b70-87e4-bf0f45bedaa0",
   "metadata": {},
   "source": [
    "## Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682969e3-27a9-49ef-9203-b57ab1862286",
   "metadata": {},
   "source": [
    "## Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6754b134-8e7a-42dd-9d8a-069bcf704311",
   "metadata": {},
   "source": [
    "## Fluency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7601f0e2-6251-4c75-9281-d94dbdb364fc",
   "metadata": {},
   "source": [
    "> Fluency measures how well the model's responses are structured, grammatically correct, and linguistically coherent. It assesses the model's ability to generate smooth and natural-sounding language. To measure Fluency: Fluency is measured by the perplexity metric. Perplexity = normalized inverse probability of the test set normalized by number of words.\n",
    ">\n",
    "> [source](https://www.linkedin.com/pulse/key-metrics-consider-llm-based-ai-products-ganeshwaran-jayachandran/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928d3de9-7f52-4e2d-ac4e-6e944b8b8c72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "daab45ca-f50a-4435-8357-53c6bf7668b2",
   "metadata": {},
   "source": [
    "## Engagement and Interactivity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628975a8-b878-4858-9adb-bb704d78cb18",
   "metadata": {},
   "source": [
    "> This metric evaluates the model's ability to engage users in a conversation and promote interactivity. It examines whether the model asks relevant follow-up questions, seeks clarification when needed, and maintains an engaging dialogue flow. To measure Engagement: Well known usage metrics obtained through surveys or by other means can be used (e.x Avg number of queries, Avg size of queries, Response feedback rating, Avg session length, etc).\n",
    ">\n",
    "> [source](https://www.linkedin.com/pulse/key-metrics-consider-llm-based-ai-products-ganeshwaran-jayachandran/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0884e0-5f2a-45f5-b64c-1c2983187a3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8e7c94-314d-46c6-85f9-9e10280529c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f521ce-2d49-4051-be3d-62547c9b9e28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d39c63e2-3d3a-4c0f-a285-3a66cd38f3fc",
   "metadata": {},
   "source": [
    "# Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c083d31-93ac-4772-be2e-427a27ababcf",
   "metadata": {},
   "source": [
    "# Timeline & Source Materials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1812e2e1-0ac7-4004-b251-be63c4340ee9",
   "metadata": {},
   "source": [
    "## Reeder (2001) - Human Evaluation Techniques\n",
    "\n",
    "In 2021 Reeder [published](https://aclanthology.org/www.mt-archive.info/HLT-2001-Reeder.pdf) *Is That Your Final Answer?*.\n",
    "\n",
    "The paper is concerned with conducting an experiment to determine whether people are able to distinguish between machine translations and human translations. Ultimately, this discovery is liekly intended to feed into reasearch as to why that is the case.\n",
    "\n",
    "> The purpose of this research is to test the efficacy of applying automated evaluation techniques ... to the output of machine translation (MT) systems. \n",
    "> ...\n",
    "> Subjects were given a set of up to six extracts of translated newswire text. Some of the extracts were expert human translations, others were machine translation outputs. The subjects were given three minutes per extract to determine whether they believed the sample output to be an expert human translation or a machine translation. Additionally, they were asked to mark the word at which they made this decision.\n",
    "\n",
    "According to Papineni et. al. (2002), Reeder (2001) provides \"A comprehensive catalog of MT evaluation techniques\". Reading through the paper, there is a discussion about the various techniques used to determine the level of proficiency of a human translator. The notable technique involves counting the average number of words in a translation to determine if the translation was generated by a human or a machine.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd7d9f8-eab4-4ea2-bcb1-3586e1c83d30",
   "metadata": {},
   "source": [
    "## Reeder (2001) - list of SMT evaluation metrics\n",
    "\n",
    "I cannot find this paper but from the papers that reference it, Reeder catalogues the current state of statistical machine translation evaluation techniques.\n",
    "\n",
    "Additional mt-eval references.\n",
    "Technical report, International Standards for, 2001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3123929f-2dca-4881-b1b0-141c253ea7eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715914f9-ccac-4608-824c-1083dbb6979b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca0bdfb8-6b28-4aa5-8119-279d6320dba7",
   "metadata": {},
   "source": [
    "## Papineni et. al. (2002) - BLEU - Automated Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362b3b28-0d95-4ae8-8014-70a8978d313e",
   "metadata": {},
   "source": [
    "In July 2002, Papineni et. al. [published](https://aclanthology.org/P02-1040.pdf) *BLEU: a Method for Automatic Evaluation of Machine Translation* with the help of the IBM T. J. Watson Research Center. \n",
    "\n",
    "The paper is framed in the context of machine translation and proposes an automated method for evaluating the quality of a machine translation. Interestingly, they refer to the method as an \"understudy\" implying that it can fill in for a human when needed.\n",
    "\n",
    "> We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.\n",
    "\n",
    "They discuss the philosophy of the approach as\n",
    "\n",
    "> How does one measure translation performance? The closer a machine translation is to a professional human translation, the better it is. This is the central idea behind our proposal. To judge the quality of a machine translation, one measures its closeness to one or more reference human translations according to a numerical metric. Thus, our MT evaluation system requires two ingredients:\n",
    ">\n",
    "> 1. a numerical “translation closeness” metric\n",
    "> 2. a corpus of good quality human reference translations\n",
    "\n",
    "The authors also repeatedly note that they belive there is a possibility of multiple good translations. So their algorithm is designed to compare a given translation with a set of reference translations.\n",
    "\n",
    "And note that the method can be though of as an extension to the methods used in speech recognition\n",
    "\n",
    "> We fashion our closeness metric after the highly successful word error rate metric used by the speech recognition community, appropriately modified for multiple reference translations and allowing for legitimate differences in word choice and word order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45793979-efe1-4090-817d-2e637b2e9e8a",
   "metadata": {},
   "source": [
    "The paper assumes familiarity with n-grams. An n-gram is a segment of a text corpus that is n characters long. For exmaple, the sentence \"the brown fox\" would have thw following trigrams (n=3): \"the\", \"he \", \"e b\".\n",
    "\n",
    "The paper then goes on to discuss its method of computing precision using n-grams. It attributes a brevity penalty which penalized translations longer then the reference. The method also mentions a recall penalty which aims to prevent the translation from containing redundant information found separately accross the reference materials.\n",
    "\n",
    "> The BLEU metric ranges from 0 to 1. Few translations will attain a score of 1 unless they are identical to a reference translation. For this reason, even a human translator will not necessarily score 1. It is important to note that the more reference translations per sentence there are, the higher the score is. Thus, one must be cautious making even “rough” comparisons on evaluations with different numbers of reference translations: on a test corpus of about 500 sentences (40 general news stories), a human translator scored 0.3468 against four references and scored 0.2571 against two references"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2c5bc4-29d4-49e5-8d69-d89cd026df06",
   "metadata": {},
   "source": [
    "A conceise summary of BLEU is also provided by Przybocki et. al. (2010) and  Wołk and Marasek (2015)\n",
    "\n",
    "> BLEU (1) is a precision-based metric that counts the number of n-grams (sequences of n consecutive\n",
    "tokens) that a candidate translation and a corresponding reference translation have in common. The\n",
    "different precision scores (one per n-gram length) are combined using the geometric mean. Once the\n",
    "overall precision score is computed, a brevity penalty is computed over the entire corpus. The purpose\n",
    "of this brevity penalty is to penalize candidate translations that are shorter (overall) than the reference\n",
    "translations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bad75c-0d26-4854-8636-2e11ca95ed38",
   "metadata": {},
   "source": [
    "Looking back, Przyboki et. al. (2010) note the monumental impact of BLEU\n",
    "\n",
    "> It is not inconceivable to claim that IBM’s introduction of BLEU (1) in 2001 has had a greater impact on\n",
    "the advancement of statistical machine translation (MT) technology than any other single contribution\n",
    "to the field over the succeeding five years. BLEU was the first automated, and more importantly\n",
    "repeatable, metric to demonstrate general correlation with human judgments of translation quality (1),\n",
    "(2). As such, BLEU provided a means for instituting large-scale MT technology evaluations.\n",
    "1\n",
    "As the\n",
    "popularity of these evaluations grew, BLEU quickly became the de facto standard metric for MT\n",
    "evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5f5ca8-4bd8-45a2-abed-02f9966660ff",
   "metadata": {},
   "source": [
    "## DoD/DARPA (2001) - MT Evaluation Series Launches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a4c930-4f84-4565-a44b-2b31ec2ad4d7",
   "metadata": {},
   "source": [
    "The U.S. Department Of Defence originally lead an effort to centralize and push forward machine translation related research with the Machine Translation (MT) evaluation series.\n",
    "\n",
    "> The MT evaluation series started in 2001 as part of the DARPA TIDES (Translingual Information Detection, Extraction) program. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649aa4cf-8bd8-4de8-b485-fd7a029dad5c",
   "metadata": {},
   "source": [
    "## NIST (2006) - OpenMT Takes Over MT Evaluation Series\n",
    "\n",
    "The National Institute of Standards and Technology (NIST) is an agency of the United States Department of Commerce whose mission is to promote American innovation and industrial competitiveness. According to their website they facilitate a series of experiments called \"challenge events\" which test and observe various machine translation evaluation techniques.\n",
    "\n",
    "The MT evaluation series was then handed over to NIST around 2006\n",
    "\n",
    "> Beginning with the 2006 evaluation, the evaluations have been driven and coordinated by NIST as NIST OpenMT. These evaluations provide an important contribution to the direction of research efforts and the calibration of technical capabilities in MT.\n",
    ">\n",
    "> [NIST](https://catalog.ldc.upenn.edu/LDC2010T21)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fc51d9-9404-4fa4-a7e5-a5f382391f70",
   "metadata": {},
   "source": [
    "According to their website\n",
    "\n",
    "> The objective of the NIST Open Machine Translation (OpenMT) evaluation series is to support research in, and help advance the state of the art of, machine translation (MT) technologies - technologies that translate text between human languages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f8ebe8-c97b-4f66-bead-619c27a09519",
   "metadata": {},
   "source": [
    "The source data, reference translations and scoring software used in the NIST OpenMT evaluations is cataloged and made available by the Linguistic Data Consortium (LDC). The NIST 2008 Open Machine Translation (OpenMT) Evaluation for example is available [here](https://catalog.ldc.upenn.edu/LDC2010T21)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe3dd4e-0dd4-4494-a7de-0b684770816e",
   "metadata": {},
   "source": [
    "## NIST (2008) - NIST Launches Sister Evaluation Series re. Metrology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628940b4-80dd-41d3-b49a-d0aba7d75716",
   "metadata": {},
   "source": [
    "Around 2008, NIST launched MetricsMaTr, a complementary MT Evaluation Series that deat with Metrology rather than the actual MT technology itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01cdebb-4923-4a6d-9e83-ad3f72b9df4a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "> NIST coordinates Metrics for Machine Translation Evaluation (MetricsMaTr), a series of research challenge events for machine translation (MT) metrology.\n",
    ">\n",
    "> [MetricsMaTr](https://www.nist.gov/itl/iad/mig/metrics-machine-translation-evaluation)\n",
    "\n",
    "> Metrology is the science of measurement and its application. NIST's work in metrology focuses on advancing measurement science to enhance economic security and improve quality of life. Almost all of NIST's research has a metrology component to it.\n",
    ">\n",
    "> [NIST > Metrology](https://www.nist.gov/metrology)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc804237-0c5a-406d-af56-e5955e41b7d1",
   "metadata": {},
   "source": [
    "In 2008 a MetricsMaTr challenge was conducted where various automated metric scores were compared with those rendered by human judges. A general discussion of the usefulness of automated metrics is offered. And suggestions regarding improvements that should be incorporated into future evaluations of metrics for MT evaluation are put forward. \n",
    "\n",
    "This challenge is documented in detail by [Przybocki et. al. (2010)](https://www.nist.gov/publications/nist-2008-metrics-machine-translation-challenge-overview-methodology-metrics-and) and NIST provides a high level [summary](https://www.nist.gov/itl/iad/mig/metrics-machine-translation-evaluation) of the challeng results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6e6fbf-b4f4-4292-ad4d-d630b696995d",
   "metadata": {},
   "source": [
    "## Przybocki et. al. (2010) - Metrics For Machine Translation Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0d2fe3-ca45-4af6-97d7-bbce1275347f",
   "metadata": {},
   "source": [
    "In the paper titled *The NIST 2008 Metrics for Machine Translation Challenge - Overview, Methodology, Metrics, and Results* Przybocki et. al. (2010) [published](https://www.nist.gov/publications/nist-2008-metrics-machine-translation-challenge-overview-methodology-metrics-and) the a paper describing the 2008 MetricsMaTr challenge results and reccomendations for future research."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4376900b-ba36-4b38-a777-ee632a721e53",
   "metadata": {},
   "source": [
    "In section 3, the paper notes that 39 metrics evaluated, seven of which were preexisting\n",
    "baseline metrics, that is, metrics that have been used prominently in past evaluations. The remaining 32\n",
    "metrics were submitted by the participants.\n",
    "\n",
    "It then broke down the metrics into the following categories: baseline metrics which were the metrics from past evaluations, and submitted metrics, which are the newly submitted ones.\n",
    "\n",
    "The baseline metrics were as follows:\n",
    "\n",
    "> 3.1. Baseline metrics\n",
    ">\n",
    "> 3.1.1. Variants of the BLEU metric\n",
    ">\n",
    "> MetricsMATR evaluated four baseline variants of the BLUE metric using case-sensitive scoring:\n",
    "> - BLEU-1: (IBM version 1.04) limited to unigram precision\n",
    "> - BLEU-4: (IBM version 1.04) precision scores for n-grams of size between 1 and 4 tokens\n",
    "> - BLEU-v11b: (NIST mteval-v11b) similar to BLEU-4, but with a modified brevity penalty\n",
    "> - BLEU-v12: (NIST mtevale-v12) similar to BLEU-v11b, but with a modified tokenization scheme\n",
    ">\n",
    "> 3.1.2. NIST score\n",
    ">\n",
    "> NIST-v11b: (NIST mteval-v11b) scores case-sensitive n-grams of size varying between 1 and 5\n",
    ">\n",
    "> 3.1.3. TER\n",
    ">\n",
    "> TER (16) is a measure of edit distance which captures the number of edits required to make a candidate\n",
    "translation identical to a reference translation, counting block moves as a single error. Scoring was case\n",
    "sensitive and uses similar text normalization as the variants of BLEU.\n",
    ">\n",
    "> TER-v0.7.25: (BBN/UMD version 0.7.25) TERCOM scoring software5\n",
    ">\n",
    "> 3.1.4. METEOR\n",
    ">\n",
    "> METEOR-v0.6: (CMU version 0.66) modules used: exact, porter, wn_stem and wn_synonymy\n",
    ">\n",
    "> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd91a2b-6b5a-49fd-abb4-cbb24960d9e4",
   "metadata": {},
   "source": [
    "The submitted metrics were as follows:\n",
    "\n",
    "> |Affiliation | Metric name(s)|\n",
    "  |------------|---------------|\n",
    "  |BabbleQuest                                                              |Badger, BadgerLite\n",
    "  |Carnegie Mellon University                                               |METEOR-v0.7, METEOR-ranking, mBLEU, mTER |\n",
    "  |City University of Hong Kong, Department of Chinese, Translation and Linguistics | ATEC1, ATEC2, ATEC3, ATEC4 |\n",
    "  |Columbia University                                                      |SEPIA1, SEPIA2\n",
    "  |Harbin Institute of Technology, School of Computer Scienceand Technology | SVM-Rank, SNR, LET\n",
    "  |National University of Singapore MaxSimRWTH Aachen University            | BleuSP, invWer, CDer\n",
    "  |Stanford University                                                      | RTE, RTE-MT|\n",
    "  |University of Maryland / BBN Technologies                                |TERp |\n",
    "  |Universitat Politècnica de Catalunya, LSI                                |ULCh, ULCopt, DP-Or, SR-Or, DR-Or,DP-Orp |\n",
    "  |USC, Information Sciences Institute (Team 1)                             | BEwT-E |\n",
    "  |USC, Information Sciences Institute (Team 2)                             | Bleu-sbp, 4-GRR |\n",
    "  |University of Washington                                                 | EDPM |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0831520-393c-41ea-afd5-1d4d338de2be",
   "metadata": {},
   "source": [
    "The paper uses Spearman's rho statistic to show the degree of agreement between the automated metric scores and the scores of human judgments across three levels: segment, document, and system level.\n",
    "\n",
    "> We report correlations for the Spearman’s rho statistic (Spearman’s correlation coefficient for ranked\n",
    "data) as our primary correlation measure. Although we found that the correlations statistics for\n",
    "Spearman’s Rho, Kendall’s Tau, and Pearson’s R to closely track each other, Spearman’s provides the\n",
    "benefit of not showing sensitivity to outliers (as does Pearson’s R), and being based on ranks,\n",
    "Spearman’s does not assume samples from a bivariate normal distribution (21).\n",
    "\n",
    "Looking at the results table in section 5, METEOR is consistantly in the top two for each level and the only metric to be top two for all levels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7430299-810a-4b4b-8bc9-9b73a3c3dc0f",
   "metadata": {},
   "source": [
    "## NIST Metric ??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ec9dc9-8761-4500-9562-bc11247aab7c",
   "metadata": {},
   "source": [
    "\n",
    "> The NIST metric was designed to improve BLEU by\n",
    "rewarding the translation of infrequently used words.\n",
    "This was intended to further prevent inflation of SMT\n",
    "evaluation scores by focusing on common words and\n",
    "high confidence translations. As a result, the NIST metric\n",
    "uses heavier weights for rarer words. The final NIST\n",
    "score is calculated using the arithmetic mean of the ngram matches between SMT and reference translations.\n",
    "In addition, a smaller brevity penalty is used for smaller\n",
    "variations in phrase lengths. The reliability and quality of\n",
    "the NIST metric has been shown to be superior to the\n",
    "BLEU metric. \n",
    ">\n",
    "> Wołk and Marasek (2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b952ff44-9246-4304-9ca8-4d5643e98571",
   "metadata": {},
   "source": [
    "> The NIST score (3) was the official metric in early DARPA TIDES MT evaluations. It is based on\n",
    "information weighted n-gram co-occurrences. Some of the differences between BLEU and the NIST\n",
    "score include the method of co-occurrence measures (arithmetic mean replacing geometric mean), a\n",
    "modified brevity penalty, and a modified weighting of n-grams, depending on the frequency of specific\n",
    "n-grams.\n",
    ">\n",
    "> Przybocki et. al. (2010)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a610894b-798f-494b-aad3-15ab6e710caa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9e06f9-835a-4c2d-a140-4ab92d4fc8ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b830ab-9a41-4cf6-b680-609a67745a08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3b9f72-450b-4428-b3d9-837f78422d83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15fec484-6afe-423f-92f2-a67f64185827",
   "metadata": {},
   "source": [
    "Mentioned by Wołk and Marasek (2015):\n",
    "- National Institute of Standards and Technology (NIST) metric; \n",
    "- Translation Error Rate (TER), \n",
    "- the Metric for Evaluation of Translation with Explicit Ordering (METEOR); \n",
    "- Length Penalty, Precision, n-gram Position difference Penalty and Recall (LEPOR); \n",
    "- the Rankbased Intuitive Bilingual Evaluation Score (RIBES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287655ad-2b95-432f-a09e-a738f94162fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65b9f4a-7208-4b69-8c03-d614426fdbf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84baba2b-d994-4c5c-8e8c-d644d552b3b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ff25106-38c9-4996-aba4-3e1ff449b399",
   "metadata": {},
   "source": [
    "##  Wołk and Marasek (2015) - Enhanced BLEU\n",
    "On Sep 2015, Wołk and Marasek [published](https://arxiv.org/abs/1509.09088) paper titled *Enhanced Bilingual Evaluation Understudy*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
