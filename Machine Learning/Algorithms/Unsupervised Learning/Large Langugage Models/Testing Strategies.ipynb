{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0c5463c-59f4-4fb6-b35b-e2d72b7dde9a",
   "metadata": {},
   "source": [
    "# Overview\n",
    "I wanted to understand the various ways we can test LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0035bc-b222-4dbe-95f2-281afca53bc2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "01741a10-7b71-4654-8f29-7df268d46499",
   "metadata": {},
   "source": [
    "# LLM Use Cases "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefdd135-0091-4fd6-b998-0d305a5b14df",
   "metadata": {},
   "source": [
    "LLMs have a number of use cases. Generally speaking those largely fall into the following categories:\n",
    "- Translation\n",
    "- Sequence Generation\n",
    "    - Summarization\n",
    "    - Prompt/Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21170c22-c82b-4971-80ef-75bf77a6609b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3540b0-7d41-47d0-9508-05deb29d4e2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ccb377ed-1655-4485-870b-9c4708551e44",
   "metadata": {},
   "source": [
    "# Test Objetives\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d8b692-c59a-4cf8-a269-1f4af767d753",
   "metadata": {},
   "source": [
    "The intuition for evaluating generated text is the same as that for evaluating labels. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952d649f-1c21-4e84-9640-eed24c21aa90",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3a261f-1bae-4b70-87e4-bf0f45bedaa0",
   "metadata": {},
   "source": [
    "## Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682969e3-27a9-49ef-9203-b57ab1862286",
   "metadata": {},
   "source": [
    "## Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6754b134-8e7a-42dd-9d8a-069bcf704311",
   "metadata": {},
   "source": [
    "## Fluency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7601f0e2-6251-4c75-9281-d94dbdb364fc",
   "metadata": {},
   "source": [
    "> Fluency measures how well the model's responses are structured, grammatically correct, and linguistically coherent. It assesses the model's ability to generate smooth and natural-sounding language. To measure Fluency: Fluency is measured by the perplexity metric. Perplexity = normalized inverse probability of the test set normalized by number of words.\n",
    ">\n",
    "> [source](https://www.linkedin.com/pulse/key-metrics-consider-llm-based-ai-products-ganeshwaran-jayachandran/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928d3de9-7f52-4e2d-ac4e-6e944b8b8c72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "daab45ca-f50a-4435-8357-53c6bf7668b2",
   "metadata": {},
   "source": [
    "## Engagement and Interactivity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628975a8-b878-4858-9adb-bb704d78cb18",
   "metadata": {},
   "source": [
    "> This metric evaluates the model's ability to engage users in a conversation and promote interactivity. It examines whether the model asks relevant follow-up questions, seeks clarification when needed, and maintains an engaging dialogue flow. To measure Engagement: Well known usage metrics obtained through surveys or by other means can be used (e.x Avg number of queries, Avg size of queries, Response feedback rating, Avg session length, etc).\n",
    ">\n",
    "> [source](https://www.linkedin.com/pulse/key-metrics-consider-llm-based-ai-products-ganeshwaran-jayachandran/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0884e0-5f2a-45f5-b64c-1c2983187a3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8e7c94-314d-46c6-85f9-9e10280529c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f521ce-2d49-4051-be3d-62547c9b9e28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d39c63e2-3d3a-4c0f-a285-3a66cd38f3fc",
   "metadata": {},
   "source": [
    "# Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c083d31-93ac-4772-be2e-427a27ababcf",
   "metadata": {},
   "source": [
    "# Timeline & Source Materials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1812e2e1-0ac7-4004-b251-be63c4340ee9",
   "metadata": {},
   "source": [
    "## Reeder (2001) - Human Evaluation Techniques\n",
    "\n",
    "In 2021 Reeder [published](https://aclanthology.org/www.mt-archive.info/HLT-2001-Reeder.pdf) *Is That Your Final Answer?*.\n",
    "\n",
    "The paper is concerned with conducting an experiment to determine whether people are able to distinguish between machine translations and human translations. Ultimately, this discovery is liekly intended to feed into reasearch as to why that is the case.\n",
    "\n",
    "> The purpose of this research is to test the efficacy of applying automated evaluation techniques ... to the output of machine translation (MT) systems. \n",
    "> ...\n",
    "> Subjects were given a set of up to six extracts of translated newswire text. Some of the extracts were expert human translations, others were machine translation outputs. The subjects were given three minutes per extract to determine whether they believed the sample output to be an expert human translation or a machine translation. Additionally, they were asked to mark the word at which they made this decision.\n",
    "\n",
    "According to Papineni et. al. (2002), Reeder (2001) provides \"A comprehensive catalog of MT evaluation techniques\". Reading through the paper, there is a discussion about the various techniques used to determine the level of proficiency of a human translator. The notable technique involves counting the average number of words in a translation to determine if the translation was generated by a human or a machine.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd7d9f8-eab4-4ea2-bcb1-3586e1c83d30",
   "metadata": {},
   "source": [
    "## Reeder (2001) - list of SMT evaluation metrics\n",
    "\n",
    "I cannot find this paper but from the papers that reference it, Reeder catalogues the current state of statistical machine translation evaluation techniques.\n",
    "\n",
    "Additional mt-eval references.\n",
    "Technical report, International Standards for, 2001."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb68a31-6c9d-464d-b6b3-58f224075025",
   "metadata": {},
   "source": [
    "## IBM (2001) - Evaluation Understudy\n",
    "\n",
    "> the July 2001 TIDES PI meeting in\n",
    "Philadelphia, IBM described an automatic MT evaluation technique\n",
    "that can provide immediate feedback and guidance in MT research.\n",
    "Their idea, which they call an \"evaluation understudy\", compares\n",
    "MT output with expert reference translations in terms of the\n",
    "statistics of short sequences of words (word N-grams). The more\n",
    "of these N-grams that a translation shares with the reference\n",
    "translations, the better the translation is judged to be. The idea is\n",
    "elegant in its simplicity. But far more important, IBM showed a\n",
    "strong correlation between these automatically generated scores\n",
    "and human judgments of translation quality.' As a result, DARPA\n",
    "commissioned NIST to develop an MT evaluation facility based on\n",
    "the IBM work. This utility is now available from NIST and serves\n",
    "as the primary evaluation measure for TIDES MT research.\n",
    ">\n",
    "> Doddington (2002) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec54450-e7e1-4a8a-ad74-6613043b9572",
   "metadata": {},
   "source": [
    "## DoD/DARPA (2001) - MT Evaluation Series Launches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f58397b-887e-4041-9fc3-f67fd76face5",
   "metadata": {},
   "source": [
    "The U.S. Department Of Defence originally lead an effort to centralize and push forward machine translation related research with the Machine Translation (MT) evaluation series.\n",
    "\n",
    "> The MT evaluation series started in 2001 as part of the DARPA TIDES (Translingual Information Detection, Extraction) program. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0bdfb8-6b28-4aa5-8119-279d6320dba7",
   "metadata": {},
   "source": [
    "## Papineni et. al. (2002) - BLEU - Automated Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362b3b28-0d95-4ae8-8014-70a8978d313e",
   "metadata": {},
   "source": [
    "In July 2002, Papineni et. al. [published](https://aclanthology.org/P02-1040.pdf) *BLEU: a Method for Automatic Evaluation of Machine Translation* with the help of the IBM T. J. Watson Research Center. \n",
    "\n",
    "The paper is framed in the context of machine translation and proposes an automated method for evaluating the quality of a machine translation. Interestingly, they refer to the method as an \"understudy\" implying that it can fill in for a human when needed.\n",
    "\n",
    "> We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.\n",
    "\n",
    "They discuss the philosophy of the approach as\n",
    "\n",
    "> How does one measure translation performance? The closer a machine translation is to a professional human translation, the better it is. This is the central idea behind our proposal. To judge the quality of a machine translation, one measures its closeness to one or more reference human translations according to a numerical metric. Thus, our MT evaluation system requires two ingredients:\n",
    ">\n",
    "> 1. a numerical “translation closeness” metric\n",
    "> 2. a corpus of good quality human reference translations\n",
    "\n",
    "The authors also repeatedly note that they belive there is a possibility of multiple good translations. So their algorithm is designed to compare a given translation with a set of reference translations.\n",
    "\n",
    "And note that the method can be though of as an extension to the methods used in speech recognition\n",
    "\n",
    "> We fashion our closeness metric after the highly successful word error rate metric used by the speech recognition community, appropriately modified for multiple reference translations and allowing for legitimate differences in word choice and word order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45793979-efe1-4090-817d-2e637b2e9e8a",
   "metadata": {},
   "source": [
    "The paper assumes familiarity with n-grams. An n-gram is a segment of a text corpus that is n characters long. For exmaple, the sentence \"the brown fox\" would have thw following trigrams (n=3): \"the\", \"he \", \"e b\".\n",
    "\n",
    "The paper then goes on to discuss its method of computing precision using n-grams. It attributes a brevity penalty which penalized translations longer then the reference. The method also mentions a recall penalty which aims to prevent the translation from containing redundant information found separately accross the reference materials.\n",
    "\n",
    "> The BLEU metric ranges from 0 to 1. Few translations will attain a score of 1 unless they are identical to a reference translation. For this reason, even a human translator will not necessarily score 1. It is important to note that the more reference translations per sentence there are, the higher the score is. Thus, one must be cautious making even “rough” comparisons on evaluations with different numbers of reference translations: on a test corpus of about 500 sentences (40 general news stories), a human translator scored 0.3468 against four references and scored 0.2571 against two references"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2c5bc4-29d4-49e5-8d69-d89cd026df06",
   "metadata": {},
   "source": [
    "A conceise summary of BLEU is also provided by Przybocki et. al. (2010) and  Wołk and Marasek (2015)\n",
    "\n",
    "> BLEU (1) is a precision-based metric that counts the number of n-grams (sequences of n consecutive\n",
    "tokens) that a candidate translation and a corresponding reference translation have in common. The\n",
    "different precision scores (one per n-gram length) are combined using the geometric mean. Once the\n",
    "overall precision score is computed, a brevity penalty is computed over the entire corpus. The purpose\n",
    "of this brevity penalty is to penalize candidate translations that are shorter (overall) than the reference\n",
    "translations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bad75c-0d26-4854-8636-2e11ca95ed38",
   "metadata": {},
   "source": [
    "Looking back, Przyboki et. al. (2010) note the monumental impact of BLEU\n",
    "\n",
    "> It is not inconceivable to claim that IBM’s introduction of BLEU (1) in 2001 has had a greater impact on\n",
    "the advancement of statistical machine translation (MT) technology than any other single contribution\n",
    "to the field over the succeeding five years. BLEU was the first automated, and more importantly\n",
    "repeatable, metric to demonstrate general correlation with human judgments of translation quality (1),\n",
    "(2). As such, BLEU provided a means for instituting large-scale MT technology evaluations.\n",
    "1\n",
    "As the\n",
    "popularity of these evaluations grew, BLEU quickly became the de facto standard metric for MT\n",
    "evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30976d24-5cc2-42e8-b433-f6ce92ec4ad7",
   "metadata": {},
   "source": [
    "## Doddington (2003) - NIST Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce8d909-4fda-48ed-ac44-813cff23db69",
   "metadata": {},
   "source": [
    "In 2003, Doddington, from NIST, [published](https://aclanthology.org/www.mt-archive.info/HLT-2002-Doddington.pdf) *Automatic Evaluation of Machine Translation Quality Using N-gram Co-Occurrence Statistics*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63efd1a-6fe3-4ed8-afbb-b95fcaeb246b",
   "metadata": {},
   "source": [
    "> The NIST score (Doddington 2002) was the official metric in early DARPA TIDES MT evaluations. It is based on\n",
    "information weighted n-gram co-occurrences. Some of the differences between BLEU and the NIST\n",
    "score include the method of co-occurrence measures (arithmetic mean replacing geometric mean), a\n",
    "modified brevity penalty, and a modified weighting of n-grams, depending on the frequency of specific\n",
    "n-grams.\n",
    ">\n",
    "> Przybocki et. al. (2010)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20de5a07-8115-46e9-a941-ff8930f2ec84",
   "metadata": {},
   "source": [
    "The NIST Score, like BLEU is based on n-gram scoring. However it makes several enhancements which are discussed in the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450c9bde-7e2b-45f3-8de8-6c261f53647a",
   "metadata": {},
   "source": [
    "\n",
    "> The NIST metric was designed to improve BLEU by\n",
    "rewarding the translation of infrequently used words.\n",
    "This was intended to further prevent inflation of SMT\n",
    "evaluation scores by focusing on common words and\n",
    "high confidence translations. As a result, the NIST metric\n",
    "uses heavier weights for rarer words. The final NIST\n",
    "score is calculated using the arithmetic mean of the ngram matches between SMT and reference translations.\n",
    "In addition, a smaller brevity penalty is used for smaller\n",
    "variations in phrase lengths. The reliability and quality of\n",
    "the NIST metric has been shown to be superior to the\n",
    "BLEU metric. \n",
    ">\n",
    "> Wołk and Marasek (2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99243c8-d660-475b-8335-4c088667f465",
   "metadata": {},
   "source": [
    "Ultimately the paper provides a discussion of the perfromance of NIST compared to the original BLEU. It looks at the correlation between BLEU scores and human judge scores. It then looks at a set of possible n-gram scoring variations and evaluates the F-scores of those variations. Based on the results, NIST elected the best method as the official NIST formula given in equation 3. Additionally, NIST makes a change to the brevity penalty.\n",
    "\n",
    "> This change\n",
    "was made to minimize the impact on the score of small variations\n",
    "in the length of a translation. This preserves the original motivation\n",
    "of including a brevity penalty (which is to help prevent gaming the\n",
    "evaluation measure) while reducing the contributions of length\n",
    "variations to the score for small variations. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865b3454-7a43-49dd-b704-0125bc0617a9",
   "metadata": {},
   "source": [
    "> The NIST evaluation score is compared with IBM's original BLEU\n",
    "score in Figure 5 and Figure 6. Figure 5 demonstrates that the\n",
    "NIST score provides significant improvement in score stability and\n",
    "reliability for all four of the corpora studied. Figure 6 demonstrates\n",
    "that, for human judgments of Adequacy, the NIST score correlates\n",
    "better than the BLEU score on all of the corpora. For Fluency\n",
    "judgments, however, the NIST score correlates better than the\n",
    "BLEU score only on the Chinese corpus. This may be a mere\n",
    "141\n",
    "7\n",
    " Large amounts of data are required to estimate N-gram statistics\n",
    "for N > 2. In the current implementation, however, the N-gram\n",
    "statistics are computed only from the reference translations for the\n",
    "evaluation corpus.\n",
    "random statistical difference between corpora. Or alternatively, this\n",
    "may be a consequence of different human judgment criteria or\n",
    "procedures. (The Chinese-to-English translations were judged at\n",
    "LDC using a different procedure than that used by John White at\n",
    "PRC for the 1994 corpora.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e194c2b-9d22-4c7c-8f4c-35309b1c3c35",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>\n",
    "                <img src=\"images/NIST_score_graph_1.png\" >\n",
    "            </td>\n",
    "            <td>\n",
    "                <img src=\"images/NIST_score_graph_2.png\" >\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649aa4cf-8bd8-4de8-b485-fd7a029dad5c",
   "metadata": {},
   "source": [
    "## NIST (2006) - OpenMT Takes Over MT Evaluation Series\n",
    "\n",
    "The National Institute of Standards and Technology (NIST) is an agency of the United States Department of Commerce whose mission is to promote American innovation and industrial competitiveness. According to their website they facilitate a series of experiments called \"challenge events\" which test and observe various machine translation evaluation techniques.\n",
    "\n",
    "The MT evaluation series was then handed over to NIST around 2006\n",
    "\n",
    "> Beginning with the 2006 evaluation, the evaluations have been driven and coordinated by NIST as NIST OpenMT. These evaluations provide an important contribution to the direction of research efforts and the calibration of technical capabilities in MT.\n",
    ">\n",
    "> [NIST](https://catalog.ldc.upenn.edu/LDC2010T21)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fc51d9-9404-4fa4-a7e5-a5f382391f70",
   "metadata": {},
   "source": [
    "According to their website\n",
    "\n",
    "> The objective of the NIST Open Machine Translation (OpenMT) evaluation series is to support research in, and help advance the state of the art of, machine translation (MT) technologies - technologies that translate text between human languages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f8ebe8-c97b-4f66-bead-619c27a09519",
   "metadata": {},
   "source": [
    "The source data, reference translations and scoring software used in the NIST OpenMT evaluations is cataloged and made available by the Linguistic Data Consortium (LDC). The NIST 2008 Open Machine Translation (OpenMT) Evaluation for example is available [here](https://catalog.ldc.upenn.edu/LDC2010T21)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe3dd4e-0dd4-4494-a7de-0b684770816e",
   "metadata": {},
   "source": [
    "## NIST (2008) - NIST Launches Sister Evaluation Series re. Metrology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628940b4-80dd-41d3-b49a-d0aba7d75716",
   "metadata": {},
   "source": [
    "Around 2008, NIST launched MetricsMaTr, a complementary MT Evaluation Series that deat with Metrology rather than the actual MT technology itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01cdebb-4923-4a6d-9e83-ad3f72b9df4a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "> NIST coordinates Metrics for Machine Translation Evaluation (MetricsMaTr), a series of research challenge events for machine translation (MT) metrology.\n",
    ">\n",
    "> [MetricsMaTr](https://www.nist.gov/itl/iad/mig/metrics-machine-translation-evaluation)\n",
    "\n",
    "> Metrology is the science of measurement and its application. NIST's work in metrology focuses on advancing measurement science to enhance economic security and improve quality of life. Almost all of NIST's research has a metrology component to it.\n",
    ">\n",
    "> [NIST > Metrology](https://www.nist.gov/metrology)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc804237-0c5a-406d-af56-e5955e41b7d1",
   "metadata": {},
   "source": [
    "In 2008 a MetricsMaTr challenge was conducted where various automated metric scores were compared with those rendered by human judges. A general discussion of the usefulness of automated metrics is offered. And suggestions regarding improvements that should be incorporated into future evaluations of metrics for MT evaluation are put forward. \n",
    "\n",
    "This challenge is documented in detail by [Przybocki et. al. (2010)](https://www.nist.gov/publications/nist-2008-metrics-machine-translation-challenge-overview-methodology-metrics-and) and NIST provides a high level [summary](https://www.nist.gov/itl/iad/mig/metrics-machine-translation-evaluation) of the challeng results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6e6fbf-b4f4-4292-ad4d-d630b696995d",
   "metadata": {},
   "source": [
    "## Przybocki et. al. (2010) - Metrics For Machine Translation Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0d2fe3-ca45-4af6-97d7-bbce1275347f",
   "metadata": {},
   "source": [
    "In March 2010, Przybocki et. al. [published](https://www.nist.gov/publications/nist-2008-metrics-machine-translation-challenge-overview-methodology-metrics-and) the paper titled *The NIST 2008 Metrics for Machine Translation Challenge - Overview, Methodology, Metrics, and Results* describing the 2008 MetricsMaTr challenge results and reccomendations for future research."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4376900b-ba36-4b38-a777-ee632a721e53",
   "metadata": {},
   "source": [
    "In section 3, the paper notes that 39 metrics evaluated, seven of which were preexisting\n",
    "baseline metrics, that is, metrics that have been used prominently in past evaluations. The remaining 32\n",
    "metrics were submitted by the participants.\n",
    "\n",
    "It then broke down the metrics into the following categories: baseline metrics which were the metrics from past evaluations, and submitted metrics, which are the newly submitted ones.\n",
    "\n",
    "The baseline metrics were as follows:\n",
    "\n",
    "> 3.1. Baseline metrics\n",
    ">\n",
    "> 3.1.1. Variants of the BLEU metric\n",
    ">\n",
    "> MetricsMATR evaluated four baseline variants of the BLUE metric using case-sensitive scoring:\n",
    "> - BLEU-1: (IBM version 1.04) limited to unigram precision\n",
    "> - BLEU-4: (IBM version 1.04) precision scores for n-grams of size between 1 and 4 tokens\n",
    "> - BLEU-v11b: (NIST mteval-v11b) similar to BLEU-4, but with a modified brevity penalty\n",
    "> - BLEU-v12: (NIST mtevale-v12) similar to BLEU-v11b, but with a modified tokenization scheme\n",
    ">\n",
    "> 3.1.2. NIST score\n",
    ">\n",
    "> NIST-v11b: (NIST mteval-v11b) scores case-sensitive n-grams of size varying between 1 and 5\n",
    ">\n",
    "> 3.1.3. TER\n",
    ">\n",
    "> TER (16) is a measure of edit distance which captures the number of edits required to make a candidate\n",
    "translation identical to a reference translation, counting block moves as a single error. Scoring was case\n",
    "sensitive and uses similar text normalization as the variants of BLEU.\n",
    ">\n",
    "> TER-v0.7.25: (BBN/UMD version 0.7.25) TERCOM scoring software5\n",
    ">\n",
    "> 3.1.4. METEOR\n",
    ">\n",
    "> METEOR-v0.6: (CMU version 0.66) modules used: exact, porter, wn_stem and wn_synonymy\n",
    ">\n",
    "> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd91a2b-6b5a-49fd-abb4-cbb24960d9e4",
   "metadata": {},
   "source": [
    "The submitted metrics were as follows:\n",
    "\n",
    "> |Affiliation | Metric name(s)|\n",
    "  |------------|---------------|\n",
    "  |BabbleQuest                                                              |Badger, BadgerLite\n",
    "  |Carnegie Mellon University                                               |METEOR-v0.7, METEOR-ranking, mBLEU, mTER |\n",
    "  |City University of Hong Kong, Department of Chinese, Translation and Linguistics | ATEC1, ATEC2, ATEC3, ATEC4 |\n",
    "  |Columbia University                                                      |SEPIA1, SEPIA2\n",
    "  |Harbin Institute of Technology, School of Computer Scienceand Technology | SVM-Rank, SNR, LET\n",
    "  |National University of Singapore MaxSimRWTH Aachen University            | BleuSP, invWer, CDer\n",
    "  |Stanford University                                                      | RTE, RTE-MT|\n",
    "  |University of Maryland / BBN Technologies                                |TERp |\n",
    "  |Universitat Politècnica de Catalunya, LSI                                |ULCh, ULCopt, DP-Or, SR-Or, DR-Or,DP-Orp |\n",
    "  |USC, Information Sciences Institute (Team 1)                             | BEwT-E |\n",
    "  |USC, Information Sciences Institute (Team 2)                             | Bleu-sbp, 4-GRR |\n",
    "  |University of Washington                                                 | EDPM |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0831520-393c-41ea-afd5-1d4d338de2be",
   "metadata": {},
   "source": [
    "The paper uses Spearman's rho statistic to show the degree of agreement between the automated metric scores and the scores of human judgments across three levels: segment, document, and system level.\n",
    "\n",
    "> We report correlations for the Spearman’s rho statistic (Spearman’s correlation coefficient for ranked\n",
    "data) as our primary correlation measure. Although we found that the correlations statistics for\n",
    "Spearman’s Rho, Kendall’s Tau, and Pearson’s R to closely track each other, Spearman’s provides the\n",
    "benefit of not showing sensitivity to outliers (as does Pearson’s R), and being based on ranks,\n",
    "Spearman’s does not assume samples from a bivariate normal distribution (21).\n",
    "\n",
    "Looking at the results table in section 5, METEOR is consistantly in the top two for each level and the only metric to be top two for all levels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d40b77-7aca-44e1-93c3-2aa8981768c8",
   "metadata": {},
   "source": [
    "## NIST (2010) - MetricsMaTr 2010 Challenge\n",
    "\n",
    "[link](https://www.nist.gov/system/files/documents/itl/iad/mig/NISTMetricsMaTr10EvalPlan.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464b115d-a276-4ccf-8598-7ac040575cfa",
   "metadata": {},
   "source": [
    "## Przybocki et. al. (2010) - MetricsMaTr 2010 Challenge Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7985464-e108-45bc-a075-1fa937f7bede",
   "metadata": {},
   "source": [
    "On July 2010, Przybocki et. al. [published](https://www.nist.gov/publications/findings-2010-joint-workshop-statistical-machine-translation-and-metrics-machine) to outline the challenger results. The abstract reads:\n",
    "\n",
    "> This paper presents the results of the WMT10 and MetricsMATR10 shared tasks, which included a translation task, a system combination task, and an evaluation task. We conducted a large-scale manual evaluation of 104 machine translation systems and 41 system combination entries. We used the ranking of these systems to measure how strongly auto- matic metrics correlate with human judgments of translation quality for 26 metrics. This year we also investigated increasing the number of human judgments by hiring non-expert annotators through Amazon's Mechanical Turk.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ee6757-13b9-4513-96ad-058166b3da66",
   "metadata": {},
   "source": [
    "An interesting note from the authors was that in some cases, they prefer rule based machine translation (RBMT) systems. Recall that RBMT systems have hard coded, language specific, rules which perform the translations.\n",
    "\n",
    "> Unfortunately, fewer rule-based systems participated in this year’s edition of WMT, compared\n",
    "to previous editions. We hope to attract more\n",
    "rule-based systems in future editions as they increase the variation of translation output and for\n",
    "some language pairs, such as German-English,\n",
    "tend to outperform statistical machine translation\n",
    "systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58e9128-d038-4f60-a61c-1bd0fe4d10d3",
   "metadata": {},
   "source": [
    "Looking at the system level results of the study we see that there is not a clear consistent winner. Instead we see competition between a number of metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dba47a-0075-4510-b6ec-91fb35f0fef7",
   "metadata": {},
   "source": [
    "\n",
    "<table>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>\n",
    "                <img src=\"images/metricsmatr_2010_system_results.png\">\n",
    "            </td>\n",
    "            <td>\n",
    "                <img src=\"images/metricsmatr_2010_system_results_2.png\">\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff039b84-87f9-490b-85e9-4bd263fef0e9",
   "metadata": {},
   "source": [
    "At the segment level we see that SVM-rank is outperforming in every category with Bkars being a close second."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5118980b-7742-4a47-a18f-3b28b563cc8a",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>\n",
    "                <img src=\"images/metricsmatr_2010_segment_results.png\">\n",
    "            </td>\n",
    "            <td>\n",
    "                <img src=\"images/metricsmatr_2010_segment_results_2.png\">\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d3a084-5dd5-434a-bfe0-a8b49766a64c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ff25106-38c9-4996-aba4-3e1ff449b399",
   "metadata": {},
   "source": [
    "##  Wołk and Marasek (2015) - Enhanced BLEU\n",
    "On Sep 2015, Wołk and Marasek [published](https://arxiv.org/abs/1509.09088) paper titled *Enhanced Bilingual Evaluation Understudy*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fec484-6afe-423f-92f2-a67f64185827",
   "metadata": {},
   "source": [
    "Mentioned by Wołk and Marasek (2015):\n",
    "- National Institute of Standards and Technology (NIST) metric; \n",
    "- Translation Error Rate (TER), \n",
    "- the Metric for Evaluation of Translation with Explicit Ordering (METEOR); \n",
    "- Length Penalty, Precision, n-gram Position difference Penalty and Recall (LEPOR); \n",
    "- the Rankbased Intuitive Bilingual Evaluation Score (RIBES)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
