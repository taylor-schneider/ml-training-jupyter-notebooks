{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdafd5b1-e8e9-4dc5-9f62-ee199af23464",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ded8804-7f0d-41e7-8d2c-d79110a6b159",
   "metadata": {},
   "source": [
    "# History"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcba346-2361-4e3b-b3fa-7e1e149e9a23",
   "metadata": {},
   "source": [
    "I started reading the book \"Modern Approaches in Natural Language Processing\" by Becker et. al. (2020)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab36829-e5e0-439f-9075-6b16fb31585d",
   "metadata": {},
   "source": [
    "## Problem: Representing Words as Numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ac2d8e-f868-4c6d-8316-9127b0431e53",
   "metadata": {},
   "source": [
    "From the beginning of NLP and related fields, creating numeric representations of language has been a question at the forefront."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9aaa50f-4cd1-4f9a-afcd-5d4f70cc4232",
   "metadata": {},
   "source": [
    "## One Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8d804d-7a69-42b0-a2e8-759ed9b3927e",
   "metadata": {},
   "source": [
    "### Origin and Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8066fe68-3a30-44b0-b1dc-fb9aa7d106ee",
   "metadata": {},
   "source": [
    "From what I [gather](https://stats.stackexchange.com/questions/308916/what-is-one-hot-encoding-called-in-scientific-literature), the term originated from field of electrical engineering and I was unable to find a precise publication of origin. In this context, one typically thinks of a state machine, ie. a circuit, is used to maintain or track the \"state\" of some process. Abstractly, the process can take on a set of mutually exclisive states over it's lifetime. The state machine observes the process and records the current state of the underlying process. This record however is typically an encoding: a number used to represent each unique state. And because we are dealing with electrical circuits, that number is represented in binary. There are many schemed for such an encoding, One Hot Encoding being one of them.\n",
    "\n",
    "With one hot encoding, each state is represented by a unique position in the binary bit stream. The rules for the stream are that a bit can only be \"hot\" (i.e. have a value of 1) when the rest of the bits in the stream are \"cold\" (i.e. have a value of 0); thus only one state can be realized at a given time. In other words, only one bit can be \"hot\" in the group of bits (sometimes called a one hot or a one hot encoding).\n",
    "\n",
    "We can see a book talking about this [here](https://www.sciencedirect.com/topics/computer-science/one-hot-encoding)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fa9dbf-2cc9-4f8b-8b89-5ec18aee3163",
   "metadata": {},
   "source": [
    "Although the application and implimentation are physically different, One Hot Encoding in the context of machine learning is exactly the same; we are using bits to represent states."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1610906-be0f-42f8-9deb-e6bbf3b2eb73",
   "metadata": {},
   "source": [
    "### Basic Concept\n",
    "In the context of machine learning, one-hot encodings are often used to encode words in a vocabulary (language) or text. Each word is indexed and represented by it's own number. Generally this is done using vectors; where each dimension in the vector coresponds to a specific word from the set of possible words. Thus each word will have a one-to-one mapping with a one-hot vector.\n",
    "\n",
    "The problem with this approach is that it creates \"sparse vectors\" i.e. vectors which high dimensionality but are mostly empty and only have one bit of information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0254a10d-4ba8-4474-907e-d1e963972190",
   "metadata": {},
   "source": [
    "<center><img src=\"images/one_hot_encoding.png\" style=\"width:50%\"><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f414ea8-ebd0-4054-868b-e1c749f307b3",
   "metadata": {},
   "source": [
    "## Bag Of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbbbc85-ea49-4268-bff3-cb1b2047166b",
   "metadata": {},
   "source": [
    "An early reference to \"bag of words\" in a linguistic context can be found in Zellig Harris's 1954 article on Distributional Structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31a1be3-d391-4156-b30e-fc66317f7501",
   "metadata": {},
   "source": [
    "With bag of words, we are again representing a word as a vector but this time, the vector's dimensions are encoded with counts of co-occurance. So in this encoding, any time a word occurs before or after another given word, that adjacent words co-occurance dimension is incremented. This can be done with words in the same sentence, different sentences, different chapters, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03081782-fcf3-40a4-aa26-35f52f42a042",
   "metadata": {},
   "source": [
    "<center><img src=\"images/bag_of_words.png\" style=\"width:50%\"><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270d477c-024d-470b-8312-23f4c6a4bc3f",
   "metadata": {},
   "source": [
    "While this is less sparse, it is still sparse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de6971a-413f-4ba6-99dc-df88c80a9f39",
   "metadata": {},
   "source": [
    "## Problem: Sparse Vectors & Word Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beefb7d2-979b-4174-96d1-4e5e60047bf6",
   "metadata": {},
   "source": [
    "> Many machine learning models won’t work well with high dimensional and sparse features (Goldberg (2016)). Neural networks in particular struggle with this type of data. And with growing vocabulary the feature size vectors also increases by the same length. So, the dimensionality of these approaches is the same as the number of different words in your text. That means estimating more parameters and therefore using exponentially more data is required to build a reasonably generalizable model. This is known as the curse of dimensionality. But these problems can be solved with dimensionality reduction methods such as Principal Component Analysis or feature selection models where less informative context words, such as the and a are dropped.\n",
    ">\n",
    "> The major drawback of these methods is that there is no notion of similarity between words. That means words like cat and tiger are represented as similar as cat and car. If the words cat and tiger would be represented as similar words one could use the information won from the more frequent word “cat” for sentences in which the less frequent word tiger appears. If the word embedding for tiger is similar to that of cat the network model can take a similar path instead of having to learn how to handle it completely anew.\n",
    ">\n",
    ">  Becker et. al. (2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fc2aee-0382-4b69-a543-106ae11f1c37",
   "metadata": {},
   "source": [
    "## TODO: Intermediary representations?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b47dd4-e892-4967-a63f-26debf96b328",
   "metadata": {},
   "source": [
    "> Vector space models have been used in distributional semantics since the 1990s. Since then, we have seen the development of a number models used for estimating continuous representations of words, Latent Dirichlet Allocation (LDA) and Latent Semantic Analysis (LSA) being two such examples.\n",
    ">\n",
    "> [source](https://aylien.com/blog/overview-word-embeddings-history-word2vec-cbow-glove#reference1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af118574-a7b1-4125-adea-cd0cf81c77ec",
   "metadata": {},
   "source": [
    "## Context Vectors ??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4f7d5d-04b0-46c7-9323-a1b62c824ba0",
   "metadata": {},
   "source": [
    "## Problem: Computational Power"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c209e40a-1cf2-47c4-bbcd-5e7efc63e076",
   "metadata": {},
   "source": [
    "> After Bengio et al.'s initial efforts in neural language models, research in word embeddings stalled as computational power and algorithms were not yet at a level that enabled the training of a large vocabulary.\n",
    ">\n",
    "> https://aylien.com/blog/overview-word-embeddings-history-word2vec-cbow-glove"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85543057-72d5-428a-aac0-b4348ac4aaba",
   "metadata": {},
   "source": [
    "## Solution: More Computing Power"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8fdb7d-e248-4a44-a80b-80fa937b6c80",
   "metadata": {},
   "source": [
    "With the first commercially available quad core processor hitting the market towards the end of 2006. Possibilities began to open as operating systemd were able to support more memory and larger file systems. Advancements continued through the 2010's pushing the computing power, memory, and storage space far beyond the levels of the early 2000's. This greatly enabled the continued development of neural networks and thus research into context vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e0c582-a014-4751-8084-5300f63c28e5",
   "metadata": {},
   "source": [
    "## Solution: More Efficient Algorithms and Less Expensive Loss Functions\n",
    "As we will see, while computing power increased and research continued, new methods were introduced which require less computing compared to their predecessors (e.g. Collobert and Weston)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105fd39a-0249-471a-a002-fb06a494cf31",
   "metadata": {},
   "source": [
    "## Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a37c9c4-e022-4c67-bdf1-b2840e3c54f3",
   "metadata": {},
   "source": [
    "To overcome the problem of sparse vectors and the problem of encoding a word's meaning word embeddings were introduced.\n",
    "\n",
    "Word embeddings also represent words as vectors in a high-dimensional space. The difference with this encoding scheme is that words with similar meanings or relationships are positioned closer to each other geometrically within the geometric space being represented by the vectors.\n",
    "\n",
    "Note: As we will see, the traditional word embeddings lacked context (i.e. sematic relationships at the phrase level)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c9e45a-22c9-4913-a659-0df68685021f",
   "metadata": {},
   "source": [
    "### Origin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f476239e-8365-498f-8745-51cb33886241",
   "metadata": {},
   "source": [
    "> Word embeddings are based on the idea that contextual information alone constitutes a viable representation of linguistic items, in stark contrast to formal linguistics and the Chomsky tradition. This idea has its theoretical roots in structuralist linguistics and ordinary language philosophy, and in particular in the works of Zellig Harris, John Firth, and Ludwig Wittgenstein, all publishing important works in the 1950s (in the case of Wittgenstein, posthumously). The earliest attempts at using feature representations to quantify (semantic) similarity used hand-crafted features. Charles Osgood’s semantic differentials in the 1960s is a good example, and similar representations were also used in early works on connectionism and artificial intelligence in the 1980s.\n",
    ">\n",
    "> Methods for using automatically generated contextual features were developed more or less simultaneously around 1990 in several different research areas...\n",
    ">\n",
    "> Later developments are basically only refinements of these early models...\n",
    ">\n",
    "> The main difference between these various models is the type of contextual information they use...\n",
    "> \n",
    "> These different contextual representations capture different types of semantic similarity; the document-based models capture semantic relatedness (e.g. “boat” – “water”) while the word-based models capture semantic similarity (e.g. “boat” – “ship”). This very basic difference is too often misunderstood.\n",
    ">\n",
    "> [*A Brief History of Word Embeddings*](https://www.gavagai.io/text-analytics/a-brief-history-of-word-embeddings/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a408490d-604f-44e4-a7f1-4437daaf47e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "073c3ff5-dd25-4e9a-a135-faf60c3119f6",
   "metadata": {},
   "source": [
    "#### Bengio et. al. (2003)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70316d4c-86b3-45a7-82d2-c78243305201",
   "metadata": {},
   "source": [
    "I have seen claims ([e.g.](https://aylien.com/blog/overview-word-embeddings-history-word2vec-cbow-glove)) that the term word embeddings was originally coined in a 2003 [paper](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf) titled \"A Neural Probabilistic Language Model\" by Bengio et al. But if you read the paper, the term embedding is not found.\n",
    "\n",
    "Instead we see them using the terms like \"distributed representations (of words) or distributed word feature vectos\".\n",
    "\n",
    "I think it's more acurate to say: Bengio was first to propose a neural network-based word embedding model and his work inspired several other researchers like Mikolev who, as we will see, makes a major publication in 2013. [ref](https://medium.com/co-learning-lounge/nlp-word-embedding-tfidf-bert-word2vec-d7f04340af7f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fda2c0-22f8-400e-9d15-adaca62fb9c6",
   "metadata": {},
   "source": [
    "Additionally, i believe Bengio et. al. were the first to train them in a neural language model jointly with the model's parameters. We will see this practice carry forward and influence the discovery of the Transformer model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c287eeb-e667-4e34-a5b6-04612dd52772",
   "metadata": {},
   "source": [
    "#### Collobert and Weston (2008)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630bc72b-4cdb-4733-8be1-4ce3ae1cceed",
   "metadata": {},
   "source": [
    "I have seen several articles ([e.g.](https://aylien.com/blog/overview-word-embeddings-history-word2vec-cbow-glove) [e.g.](https://www.ruder.io/word-embeddings-1)) agruing that it was Collbert and Weston to first show the value of pre-training word embeddings to that they can be used in downstream tasks. \n",
    "\n",
    "Essentially they showed how to apply word embeddings to [transfer learning](Transfer%20Learning%20And%20Pre-Trained%20Models.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30dcbc3-4539-43d5-b462-4440bd054dbb",
   "metadata": {},
   "source": [
    "Their [paper](http://machinelearning.org/archive/icml2008/papers/391.pdf) titled *A unified architecture for natural language processing* also introduces a neural network architecture that forms the foundation for many current approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dff6bd-d994-47f0-940c-f3a8e6954508",
   "metadata": {},
   "source": [
    "In their paper they also note that:\n",
    "\n",
    ">  (Bengio & Ducharme, 2001) and (Schwenk & Gauvain, 2002) already presented very similar language models. However, their goal was to give a probability of a word given previous ones in a sentence. Here, we only want to have a good representation of words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fffac9-b07b-4e1a-a0f2-59edcd113785",
   "metadata": {},
   "source": [
    ">In their 2011 paper, they further expand on this [8]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee3c321-2913-4d48-86a9-f6e99b4c8c0c",
   "metadata": {},
   "source": [
    "##### Language Representations vs. Word Predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4864393-ec50-4abd-8efc-44a96f846435",
   "metadata": {},
   "source": [
    "Rather than contrstructing devices explicitly for next word prediction, they began looking at vectors more generally as a means of soring meaning based on context: thus the term context vectors. This abstraction allowed a more generic approach that could then be applied downstream through transfer learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0376879-cd5a-44d3-8f13-bdcb0955ad48",
   "metadata": {},
   "source": [
    "Another big win was an optimization of the training process to use a less computationally demanding loss function:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a1350b-fd51-4484-93ec-c90a82639316",
   "metadata": {},
   "source": [
    "> In order to avoid computing the expensive softmax, their solution is to employ an alternative objective function: rather than the cross-entropy criterion of Bengio et al., which maximizes the probability of the next word given the previous words, Collobert and Weston train a network to output a higher score ... for a correct word sequence (a probable word sequence in Bengio's model) than for an incorrect one. For this purpose, they use a pairwise ranking criterion\n",
    ">\n",
    "> https://aylien.com/blog/overview-word-embeddings-history-word2vec-cbow-glove"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4b154a-6f2a-4c5a-a752-f7fe88f99385",
   "metadata": {},
   "source": [
    "#### Mikolov et al. (2013) - Word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa3efe8-cdd1-451c-92ff-029177b4b57d",
   "metadata": {},
   "source": [
    "A year later, Pennington et al. introduced us to GloVe, a competitive set of pre-trained embeddings, suggesting that word embeddings was suddenly among the mainstream.\n",
    ">\n",
    "> [source](https://aylien.com/blog/overview-word-embeddings-history-word2vec-cbow-glove#reference1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f202271-4b8d-4c65-a500-f5ec1fc32ad9",
   "metadata": {},
   "source": [
    "#### Pennington et al. (2014) - Glove"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ce9b5f-4cdd-49d6-b29f-7473bb1a6a02",
   "metadata": {},
   "source": [
    "It was Mikolov et al. (2013), however, who really brought word embedding to the fore through the creation of word2vec, a toolkit enabling the training and use of pre-trained embeddings.\n",
    ">\n",
    "> [source](https://aylien.com/blog/overview-word-embeddings-history-word2vec-cbow-glove#reference1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f51d88f-2e34-4491-9472-275c793e37a5",
   "metadata": {},
   "source": [
    "### Basic Concept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517e88a8-0a46-48f8-9bcb-f6371983a71f",
   "metadata": {},
   "source": [
    "Word embeddings are again, a type of encoding that uses a vector to store the encoding information. However, as opposed to some of the previous implimentations which use discrete vectors, Word embeddings use continuous vectors to represent each word in a vocabulary. \n",
    "\n",
    "The trick with word embeddings is that the dimensions corespond to a particular meaning of a word. Additionally meaning is not mutually exclusive, words can have similar meaning and then have non-zero values in that dimension, or negative values if they have the opposite meaning for example. \n",
    "\n",
    "Words that are similar to eachother in meaning have similar vector values and are thus geometrically oriented such that they are close to those words with similar meaning.\n",
    "\n",
    "This is an important design consideration; the overlap allows the vectors to be much more dense than the prior encodings which were all sparse.\n",
    "\n",
    "So the question becomes: what are the meanings associated with each dimension. This is a difficult question to answer because the dimensions are chosen dynamically based on a machine earning algorithm which selects the optimal dimensions based on some loss function. \n",
    "\n",
    "We will also see that there are many implimentation which use their own loss fucntions etc. to determine the dimensionality and the values of the vectors.\n",
    "\n",
    "But a common analogy is to think of the principal axes or principal components one discovers through [Principal Componen Analysis (PCA)](../../../../Data%20Science/Principal%20Component%20Analysis%20(PCA)/Principal%20Component%20Analysis.ipynb). According to Becker (2000) the dimensionality is usualy between 100 and 500 principle meanings of words.\n",
    "\n",
    "Using this analogy, the word embedding algorithm will seek to minimize the distance between similar words while minimizing the dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811e5ad7-717a-4d7d-b917-cf60e7a807b2",
   "metadata": {},
   "source": [
    "<center><img src=\"images/word_embeddings.png\"\"><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374029e8-1b47-4032-9fad-32089735130c",
   "metadata": {},
   "source": [
    "Another important feature of the Word Embeddings is that because they are continuous and encode meaning, they allow us to perform algebra:\n",
    "\n",
    "> With such word vectors even algebraic computations become possible as shown in Tomáš Mikolov, Yih, and Zweig (2013). For example, vector(King)−vector(Man)+vector(Woman) results in a vector that is closest to the vector representation of the word Queen. Another possibility to use word embeddings vectors is translation between languages. Tomas Mikolov, Le, and Sutskever (2013) showed that they can find word translations by comparing vectors generated from different languages. By searching for a translation one can use the word vector from the source language and search for the closest vector in the target language vector space, this word can then be used as a translation. The reason this works is that if a word vector from one language is similar to the word vector of the other language, this word is used in a similar context. This method can be used to infer missing dictionary entries. An example for this method depicted in figure 3.4. In figure 3.4 the vectors for numbers and animals are depicted on the left side and the same words are depicted on the right side. It can be seen that the vectors for the correct translation align in similar geometric spaces. Again, two-dimensional representation was achieved by using dimension reduction methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9157c2-24a2-4646-a00a-885a709cc1d1",
   "metadata": {},
   "source": [
    "<center><img src='images/word_embedding_algebra.png'></center>\n",
    "\n",
    "> FIGURE 3.4: Distributed word vector representations of numbers and animals in English (left) and Spanish (right). Source: Tomas Mikolov, Le, and Sutskever (2013)\n",
    ">\n",
    "> Becker et. al. (2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace1e75f-cba4-4f13-b485-757ba4c67adf",
   "metadata": {},
   "source": [
    "There are a number of implimentations for word embeddings as we will see."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfa83fb-5791-41bd-9530-182d30f6ef98",
   "metadata": {},
   "source": [
    "### Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f1f683-ca0b-4cc2-b5a4-2f7be14e9c10",
   "metadata": {},
   "source": [
    "I started wondering why word embeddings were given their name. I gogled and found the following answer on [quora](https://www.quora.com/Why-are-word-embeddings-called-word-embeddings):\n",
    "\n",
    "> \"Word\": This part of the term specifies that we are dealing with individual words in the vocabulary. In reality we have evolved the term to deal with \"tokens\" which may be only parts of words.\n",
    ">\n",
    "> The term \"embedding\" comes from the field of mathematics, where it refers to the process of mapping objects from one space into another, often with the goal of preserving certain relationships or properties. In the context of word embeddings: ... (the term) signifies the act of placing or mapping words into a continuous vector space. Just as embedding a physical object in a material might mean surrounding it or encapsulating it within that material, word embeddings encapsulate the semantic meaning and relationships of words within a vector space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9994edf7-0cba-43ee-ada0-7c96696a51f9",
   "metadata": {},
   "source": [
    "### Implimentations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0db94d-5e95-4395-a779-c16241b4a934",
   "metadata": {},
   "source": [
    "#### Word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b89329c-2894-48e7-9f54-02bc342141fa",
   "metadata": {},
   "source": [
    "##### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ddc95c-eb9d-4252-bc73-e6dc913d8519",
   "metadata": {},
   "source": [
    "From googling, I see that word2vec is not a singular algorithm, rather, it is a family of model architectures and optimizations for learning word embeddings.\n",
    "\n",
    "Within the word2vec umbrella, there are two implimentations of word embeddings:\n",
    "\n",
    "- **Continuous bag-of-words model**: predicts the middle word based on surrounding context words. The context consists of a few words before and after the current (middle) word. This architecture is called a bag-of-words model as the order of words in the context is not important.\n",
    "- **Continuous skip-gram model**: predicts words within a certain range before and after the current word in the same sentence. A worked example of this is given below.\n",
    "\n",
    "Generally speaking, the two algorithms rely on shallow neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee70abb-45ba-4c95-bc20-6791376bfb7c",
   "metadata": {},
   "source": [
    "A great articles can be found [here](https://jalammar.github.io/illustrated-word2vec/) and [here](https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f99bf7c-70bd-40d1-bb83-f04c32af3901",
   "metadata": {},
   "source": [
    "##### History"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac59d83-141d-452e-80a1-0a2b95df9c88",
   "metadata": {},
   "source": [
    "According to [wikipedia](https://en.wikipedia.org/wiki/Word2vec), Word2vec was published in 2013 by a team of researchers led by Mikolov at Google over two papers respectively titled [Efficient Estimation of Word Representations in Vector Space\n",
    "](https://arxiv.org/abs/1301.3781) and [Distributed Representations of Words and Phrases and their Compositionality](https://ui.adsabs.harvard.edu/abs/2013arXiv1310.4546M/abstract). Among the authors, Tomas Mikolov is the most widely cited. Additionally the algorithm was [patented](https://worldwide.espacenet.com/patent/search/family/053054725/publication/US9037464B1?q=pn%3DUS9037464) in 2015."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2336f2-eb8e-4764-9a1a-b73e4bed3244",
   "metadata": {},
   "source": [
    "Additionally, a follow up paper was [published](https://arxiv.org/abs/1402.3722) in 2014 by Goldberg et. al. explaining the math and rational behind it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fcbd2d-fd6a-4290-8cea-fc4482ccf0cc",
   "metadata": {},
   "source": [
    "With the invention of the [Transformer](./Transformers.ipynb), the word2vec algorithm is seen as being outdated as a means of producing word ebmeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6ba7b2-3942-4e9c-b3bf-e437519477fc",
   "metadata": {},
   "source": [
    "##### Tutorials\n",
    "\n",
    "[Using tensorflow](https://www.tensorflow.org/text/tutorials/word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa6c7a6-bc75-4462-a362-601bb5e8368c",
   "metadata": {},
   "source": [
    "#### Transformer ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c73c306-7ced-4f90-b4e2-df99f0edee64",
   "metadata": {},
   "source": [
    "More on how this works ^^"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56305a0-4798-4704-be48-496f203d46a5",
   "metadata": {},
   "source": [
    "Mikolov et al. [2] recommend two architectures for learning word embeddings that, when compared with previous models, are computationally less expensive.\n",
    "\n",
    "Here are two key benefits that these architectures have over Bengio's and the C&W model;\n",
    "\n",
    "- They forgo the costly hidden layer.\n",
    "\n",
    "- They allow the language model to take additional context into account.\n",
    "\n",
    "The success their model can not only be attributed to these differences, it importantly also comes from specific training strategies, both of which we will now look at;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be827c4-e86e-4179-a013-63a6615d3f7e",
   "metadata": {},
   "source": [
    "#### GloVe\n",
    "\n",
    "GloVe was [published](https://aclanthology.org/D14-1162/) in 2014 by a team of researcher at Stanford University (Pennington et. al.). The project is open source and the home page can be found [here](https://nlp.stanford.edu/projects/glove/). It hosts several iterations and versions of word vectors trained through various means.\n",
    "\n",
    "According to the home page:\n",
    "> Training is performed on aggregated global word-word co-occurrence statistics from a corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4bb244-3736-433f-acc0-b2869a01f355",
   "metadata": {},
   "source": [
    "And according to the [paper's](https://aclanthology.org/D14-1162.pdf) abstract:\n",
    "> Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1851e97a-35e2-477d-8450-d147ccf33e96",
   "metadata": {},
   "source": [
    "While algorithms like word2vec implicitly derive word meaning, GloVe tries to do this explicitly.\n",
    "\n",
    "> the creators of GloVe illustrate that the ratio of the co-occurrence probabilities of two words (rather than their co-occurrence probabilities themselves) is what contains information and so look to encode this information as vector differences.\n",
    ">\n",
    "> For this to be accomplished, they propose a weighted least squares objective (J) that directly aims to reduce the difference between the dot product of the vectors of two words and the logarithm of their number of co-occurrences\n",
    ">\n",
    "> ...\n",
    ">\n",
    "> With GloVe, we have already seen that the differences are not as obvious: While GloVe is considered a predict model by Levy et al. (2015) [10], it is clearly factorizing a word-context co-occurrence matrix, which brings it close to traditional methods such as PCA and LSA. Even more, Levy et al. [12] demonstrate that word2vec implicitly factorizes a word-context PMI matrix.\n",
    "> [source](https://aylien.com/blog/overview-word-embeddings-history-word2vec-cbow-glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba1100f-24ab-4bbd-9daa-4212b772113d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c68f3ad-e21d-4adc-b929-748c75f2b80b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24f1bbca-07de-4a3c-823e-990056d79894",
   "metadata": {},
   "source": [
    "## Context Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7f9f53-3eca-4170-abe3-c8545b0df13c",
   "metadata": {},
   "source": [
    "### Word Embeddings vs. Contextual Embeddings\n",
    "\n",
    "> Both embedding techniques, traditional word embedding (e.g. word2vec, Glove) and contextual embedding (e.g. ELMo, BERT), aim to learn a continuous (vector) representation for each word in the documents...\n",
    ">\n",
    "> Traditional word embedding techniques learn a global word embedding. They first build a global vocabulary using unique words in the documents by ignoring the meaning of words in different context. Then, similar representations are learnt for the words appeared more frequently close each other in the documents. The problem is that in such word representations the words' contextual meaning (the meaning derived from the words' surroundings), is ignored. For example, only one representation is learnt for \"left\" in sentence \"I left my phone on the left side of the table.\" However, \"left\" has two different meanings in the sentence, and needs to have two different representations in the embedding space.\n",
    ">\n",
    "> Word embeddings provided by word2vec or fastText has a vocabulary (dictionary) of words. The elements of this vocabulary (or dictionary) are words and its corresponding word embeddings. Hence, given a word, its embeddings is always the same in whichever sentence it occurs. Here, the pre-trained word embeddings are static.\n",
    ">\n",
    "> On the other hand, contextual embedding methods are used to learn sequence-level semantics by considering the sequence of all words in the documents. Thus, such techniques learn different representations for polysemous words (words with multiple meanings, i.e. homonyms), e.g. \"left\" in example above, based on their context.\n",
    ">\n",
    "> Contextual embeddings depend on the other words in a given sentence... Thus, given a word, it will not have a static embeddings, but the embeddings are dynamically generated.\n",
    ">\n",
    "> For example, consider the two sentences:\n",
    ">\n",
    "> I will show you a valid point of reference and talk to the point.\n",
    "Where have you placed the point.\n",
    "Now, the word embeddings from a pre-trained embeddings such as word2vec, the embeddings for the word 'point' is same for both of its occurrences in example 1 and also the same for the word 'point' in example 2. (all three occurrences has same embeddings).\n",
    ">\n",
    "> While, the embeddings from BERT or ELMO or any such transformer based models, the the two occurrences of the word 'point' in example 1 will have different embeddings. Also, the word 'point' occurring in example 2 will have different embeddings than the ones in example 1.\n",
    ">\n",
    "> [stackoverflow](https://stackoverflow.com/questions/62272056/what-are-the-differences-between-contextual-embedding-and-word-embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d1c344-d18b-4026-9f23-2bd1a8e70aac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
