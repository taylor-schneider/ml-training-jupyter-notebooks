{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdafd5b1-e8e9-4dc5-9f62-ee199af23464",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ded8804-7f0d-41e7-8d2c-d79110a6b159",
   "metadata": {},
   "source": [
    "# History"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcba346-2361-4e3b-b3fa-7e1e149e9a23",
   "metadata": {},
   "source": [
    "I started reading the book \"Modern Approaches in Natural Language Processing\" by Becker et. al. (2020)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d779e529-1053-4c53-9586-d0f1aa16b02c",
   "metadata": {},
   "source": [
    "The term work embedding has a broad meaning which has expanded with advancements in the field."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab36829-e5e0-439f-9075-6b16fb31585d",
   "metadata": {},
   "source": [
    "## Problem: Representing Words as Numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ac2d8e-f868-4c6d-8316-9127b0431e53",
   "metadata": {},
   "source": [
    "From the beginning of NLP and related fields, creating numeric representations of language has been a question at the forefront."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9aaa50f-4cd1-4f9a-afcd-5d4f70cc4232",
   "metadata": {},
   "source": [
    "## One Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8d804d-7a69-42b0-a2e8-759ed9b3927e",
   "metadata": {},
   "source": [
    "### Origin and Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8066fe68-3a30-44b0-b1dc-fb9aa7d106ee",
   "metadata": {},
   "source": [
    "From what I [gather](https://stats.stackexchange.com/questions/308916/what-is-one-hot-encoding-called-in-scientific-literature), the term originated from field of electrical engineering and I was unable to find a precise publication of origin. In this context, one typically thinks of a state machine, ie. a circuit, is used to maintain or track the \"state\" of some process. Abstractly, the process can take on a set of mutually exclisive states over it's lifetime. The state machine observes the process and records the current state of the underlying process. This record however is typically an encoding: a number used to represent each unique state. And because we are dealing with electrical circuits, that number is represented in binary. There are many schemed for such an encoding, One Hot Encoding being one of them.\n",
    "\n",
    "With one hot encoding, each state is represented by a unique position in the binary bit stream. The rules for the stream are that a bit can only be \"hot\" (i.e. have a value of 1) when the rest of the bits in the stream are \"cold\" (i.e. have a value of 0); thus only one state can be realized at a given time. In other words, only one bit can be \"hot\" in the group of bits (sometimes called a one hot or a one hot encoding).\n",
    "\n",
    "We can see a book talking about this [here](https://www.sciencedirect.com/topics/computer-science/one-hot-encoding)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fa9dbf-2cc9-4f8b-8b89-5ec18aee3163",
   "metadata": {},
   "source": [
    "Although the application and implimentation are physically different, One Hot Encoding in the context of machine learning is exactly the same; we are using bits to represent states."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1610906-be0f-42f8-9deb-e6bbf3b2eb73",
   "metadata": {},
   "source": [
    "### Basic Concept\n",
    "In the context of machine learning, one-hot encodings are often used to encode words in a vocabulary (language) or text. Each word is indexed and represented by it's own number. Generally this is done using vectors; where each dimension in the vector coresponds to a specific word from the set of possible words. Thus each word will have a one-to-one mapping with a one-hot vector.\n",
    "\n",
    "The problem with this approach is that it creates \"sparse vectors\" i.e. vectors which high dimensionality but are mostly empty and only have one bit of information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0254a10d-4ba8-4474-907e-d1e963972190",
   "metadata": {},
   "source": [
    "<center><img src=\"images/one_hot_encoding.png\" style=\"width:50%\"><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f414ea8-ebd0-4054-868b-e1c749f307b3",
   "metadata": {},
   "source": [
    "## Bag Of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbbbc85-ea49-4268-bff3-cb1b2047166b",
   "metadata": {},
   "source": [
    "An early reference to \"bag of words\" in a linguistic context can be found in Zellig Harris's 1954 article on Distributional Structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31a1be3-d391-4156-b30e-fc66317f7501",
   "metadata": {},
   "source": [
    "With bag of words, we are again representing a word as a vector but this time, the vector's dimensions are encoded with counts of co-occurance. So in this encoding, any time a word occurs before or after another given word, that adjacent words co-occurance dimension is incremented. This can be done with words in the same sentence, different sentences, different chapters, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03081782-fcf3-40a4-aa26-35f52f42a042",
   "metadata": {},
   "source": [
    "<center><img src=\"images/bag_of_words.png\" style=\"width:50%\"><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270d477c-024d-470b-8312-23f4c6a4bc3f",
   "metadata": {},
   "source": [
    "While this is less sparse, it is still sparse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de6971a-413f-4ba6-99dc-df88c80a9f39",
   "metadata": {},
   "source": [
    "## Problem: Sparse Vectors & Word Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beefb7d2-979b-4174-96d1-4e5e60047bf6",
   "metadata": {},
   "source": [
    "> Many machine learning models won’t work well with high dimensional and sparse features (Goldberg (2016)). Neural networks in particular struggle with this type of data. And with growing vocabulary the feature size vectors also increases by the same length. So, the dimensionality of these approaches is the same as the number of different words in your text. That means estimating more parameters and therefore using exponentially more data is required to build a reasonably generalizable model. This is known as the curse of dimensionality. But these problems can be solved with dimensionality reduction methods such as Principal Component Analysis or feature selection models where less informative context words, such as the and a are dropped.\n",
    ">\n",
    "> The major drawback of these methods is that there is no notion of similarity between words. That means words like cat and tiger are represented as similar as cat and car. If the words cat and tiger would be represented as similar words one could use the information won from the more frequent word “cat” for sentences in which the less frequent word tiger appears. If the word embedding for tiger is similar to that of cat the network model can take a similar path instead of having to learn how to handle it completely anew.\n",
    ">\n",
    ">  Becker et. al. (2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fc2aee-0382-4b69-a543-106ae11f1c37",
   "metadata": {},
   "source": [
    "## TODO: Intermediary representations?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b47dd4-e892-4967-a63f-26debf96b328",
   "metadata": {},
   "source": [
    "> Vector space models have been used in distributional semantics since the 1990s. Since then, we have seen the development of a number models used for estimating continuous representations of words, Latent Dirichlet Allocation (LDA) and Latent Semantic Analysis (LSA) being two such examples.\n",
    ">\n",
    "> [source](https://aylien.com/blog/overview-word-embeddings-history-word2vec-cbow-glove#reference1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc5e718-0357-4698-9587-eb2f63547b64",
   "metadata": {},
   "source": [
    "### Context Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6f13dd-dc1a-4673-8aa2-2cf8fb11e6e9",
   "metadata": {},
   "source": [
    "To overcome some of the problems with previous encodings that are sparse and/or do not convey word meaning context vectors were introduced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95e0a9f-6e49-43c8-8f3f-431f984a49e4",
   "metadata": {},
   "source": [
    "> Context Vectors are fixed-length vector representations useful for document retrieval and word sense disambiguation. Context vectors were motivated by four goals:\n",
    ">\n",
    ">1. Capture “similarity of use” among words (“car” is similar to “auto”, but not similar to “hippopotamus”).\n",
    ">2. Quickly find constituent objects (eg., documents that contain specified words).\n",
    ">3. Generate context vectors automatically from an unlabeled corpus.\n",
    ">4. Use context vectors as input to standard learning algorithms.\n",
    ">\n",
    "> Context Vectors lack, however, a natural way to represent syntax, discourse, or logic.\n",
    ">\n",
    "> [Hybrid Neural Systems 1998](https://link.springer.com/chapter/10.1007/10719871_14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa96f5c-4650-4a22-916c-0f66727991ed",
   "metadata": {},
   "source": [
    "**Note**: Colloially context vectors are used synonimousyly with word embeddings ([e.g.](https://www.baeldung.com/cs/word2vec-word-embeddings), [e.g.](https://medium.com/@RobinVetsch/nlp-from-word-embedding-to-transformers-76ae124e6281), [e.g.](https://towardsdatascience.com/what-in-the-corpus-is-a-word-embedding-2e1a4e2ef04d)). But there is a distinction between the two. Word embeddings are a particular class of context vectors that are typically trained using neural networks. While context vectors have been around since at least the 1980's word embeddings were not seen until the early 2000s."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4f7d5d-04b0-46c7-9323-a1b62c824ba0",
   "metadata": {},
   "source": [
    "## Problem: Computational Power"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c209e40a-1cf2-47c4-bbcd-5e7efc63e076",
   "metadata": {},
   "source": [
    "> After Bengio et al.'s initial efforts in neural language models, research in word embeddings stalled as computational power and algorithms were not yet at a level that enabled the training of a large vocabulary.\n",
    ">\n",
    "> https://aylien.com/blog/overview-word-embeddings-history-word2vec-cbow-glove"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85543057-72d5-428a-aac0-b4348ac4aaba",
   "metadata": {},
   "source": [
    "## Solution: More Computing Power"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8fdb7d-e248-4a44-a80b-80fa937b6c80",
   "metadata": {},
   "source": [
    "With the first commercially available quad core processor hitting the market towards the end of 2006. Possibilities began to open as operating systemd were able to support more memory and larger file systems. Advancements continued through the 2010's pushing the computing power, memory, and storage space far beyond the levels of the early 2000's. This greatly enabled the continued development of neural networks and thus research into context vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e0c582-a014-4751-8084-5300f63c28e5",
   "metadata": {},
   "source": [
    "## Solution: More Efficient Algorithms and Less Expensive Loss Functions\n",
    "As we will see, while computing power increased and research continued, new methods were introduced which require less computing compared to their predecessors (e.g. Collobert and Weston)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b39c4e-4212-41fc-8093-324d12e890aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbf8b05-cabf-4026-b799-fb59abb76dc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d85f36c-b3ff-49d5-b0b3-8b2615094468",
   "metadata": {},
   "source": [
    "## Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a37c9c4-e022-4c67-bdf1-b2840e3c54f3",
   "metadata": {},
   "source": [
    "To overcome the problem of sparse vectors and the problem of encoding a word's meaning word embeddings were introduced.\n",
    "\n",
    "Word embeddings also represent words as vectors in a high-dimensional space. The difference with this encoding scheme is that words with similar meanings are positioned closer to each other geometrically within the geometric space being represented by the vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e94d78-0cc3-43ac-bfe7-8bbeb324ba0d",
   "metadata": {},
   "source": [
    "**Note**: Sometimes word embeddings are referred to as \"distributed word representations\" or \"word representations\" (e.g. [ Liu et. al. (2020) - A Survey on Contextual Embeddings](https://arxiv.org/abs/2003.07278))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d827f059-0d11-4f19-8e55-5ef8e8405d09",
   "metadata": {},
   "source": [
    "**Note**: As we will see, the traditional word embeddings lacked context (i.e. information about sematic relationships at the phrase level was not collected and encoded). This gave rise to Contextual Word Embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f476239e-8365-498f-8745-51cb33886241",
   "metadata": {},
   "source": [
    "> Word embeddings are based on the idea that contextual information alone constitutes a viable representation of linguistic items, in stark contrast to formal linguistics and the Chomsky tradition. This idea has its theoretical roots in structuralist linguistics and ordinary language philosophy, and in particular in the works of Zellig Harris, John Firth, and Ludwig Wittgenstein, all publishing important works in the 1950s (in the case of Wittgenstein, posthumously). The earliest attempts at using feature representations to quantify (semantic) similarity used hand-crafted features. Charles Osgood’s semantic differentials in the 1960s is a good example, and similar representations were also used in early works on connectionism and artificial intelligence in the 1980s.\n",
    ">\n",
    "> Methods for using automatically generated contextual features were developed more or less simultaneously around 1990 in several different research areas...\n",
    ">\n",
    "> Later developments are basically only refinements of these early models...\n",
    ">\n",
    "> The main difference between these various models is the type of contextual information they use...\n",
    "> \n",
    "> These different contextual representations capture different types of semantic similarity; the document-based models capture semantic relatedness (e.g. “boat” – “water”) while the word-based models capture semantic similarity (e.g. “boat” – “ship”). This very basic difference is too often misunderstood.\n",
    ">\n",
    "> [*A Brief History of Word Embeddings*](https://www.gavagai.io/text-analytics/a-brief-history-of-word-embeddings/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073c3ff5-dd25-4e9a-a135-faf60c3119f6",
   "metadata": {},
   "source": [
    "### Bengio et. al. (2003)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70316d4c-86b3-45a7-82d2-c78243305201",
   "metadata": {},
   "source": [
    "I have seen claims ([e.g.](https://aylien.com/blog/overview-word-embeddings-history-word2vec-cbow-glove)) that the term word embeddings was originally coined in a 2003 [paper](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf) titled \"A Neural Probabilistic Language Model\" by Bengio et al. But if you read the paper, the term embedding is not found.\n",
    "\n",
    "Instead we see them using the terms like \"distributed representations (of words) or distributed word feature vectos\".\n",
    "\n",
    "I think it's more acurate to say: Bengio was first to propose a neural network-based word embedding model and his work inspired several other researchers like Mikolev who, as we will see, makes a major publication in 2013. [ref](https://medium.com/co-learning-lounge/nlp-word-embedding-tfidf-bert-word2vec-d7f04340af7f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fda2c0-22f8-400e-9d15-adaca62fb9c6",
   "metadata": {},
   "source": [
    "Additionally, i believe Bengio et. al. were the first to train them in a neural language model jointly with the model's parameters. We will see this practice carry forward and influence the discovery of the Transformer model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c287eeb-e667-4e34-a5b6-04612dd52772",
   "metadata": {},
   "source": [
    "### Collobert and Weston (2008)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630bc72b-4cdb-4733-8be1-4ce3ae1cceed",
   "metadata": {},
   "source": [
    "I have seen several articles ([e.g.](https://aylien.com/blog/overview-word-embeddings-history-word2vec-cbow-glove) [e.g.](https://www.ruder.io/word-embeddings-1)) agruing that it was Collbert and Weston to first show the value of pre-training word embeddings to that they can be used in downstream tasks. \n",
    "\n",
    "Essentially they showed how to apply word embeddings to [transfer learning](Transfer%20Learning%20And%20Pre-Trained%20Models.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30dcbc3-4539-43d5-b462-4440bd054dbb",
   "metadata": {},
   "source": [
    "Their [paper](http://machinelearning.org/archive/icml2008/papers/391.pdf) titled *A unified architecture for natural language processing* also introduces a neural network architecture that forms the foundation for many current approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dff6bd-d994-47f0-940c-f3a8e6954508",
   "metadata": {},
   "source": [
    "In their paper they also note that:\n",
    "\n",
    ">  (Bengio & Ducharme, 2001) and (Schwenk & Gauvain, 2002) already presented very similar language models. However, their goal was to give a probability of a word given previous ones in a sentence. Here, we only want to have a good representation of words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fffac9-b07b-4e1a-a0f2-59edcd113785",
   "metadata": {},
   "source": [
    ">In their 2011 paper, they further expand on this [8]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee3c321-2913-4d48-86a9-f6e99b4c8c0c",
   "metadata": {},
   "source": [
    "#### Language Representations vs. Word Predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4864393-ec50-4abd-8efc-44a96f846435",
   "metadata": {},
   "source": [
    "Rather than contrstructing devices explicitly for next word prediction, they began looking at vectors more generally as a means of soring meaning based on context: thus the term context vectors. This abstraction allowed a more generic approach that could then be applied downstream through transfer learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0376879-cd5a-44d3-8f13-bdcb0955ad48",
   "metadata": {},
   "source": [
    "Another big win was an optimization of the training process to use a less computationally demanding loss function:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a1350b-fd51-4484-93ec-c90a82639316",
   "metadata": {},
   "source": [
    "> In order to avoid computing the expensive softmax, their solution is to employ an alternative objective function: rather than the cross-entropy criterion of Bengio et al., which maximizes the probability of the next word given the previous words, Collobert and Weston train a network to output a higher score ... for a correct word sequence (a probable word sequence in Bengio's model) than for an incorrect one. For this purpose, they use a pairwise ranking criterion\n",
    ">\n",
    "> https://aylien.com/blog/overview-word-embeddings-history-word2vec-cbow-glove"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4b154a-6f2a-4c5a-a752-f7fe88f99385",
   "metadata": {},
   "source": [
    "### Mikolov et. al. (2013) - Word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a08736b-b517-4728-b7d0-2386a13e670f",
   "metadata": {},
   "source": [
    "In 2013, Mikolov et. al. [published](https://arxiv.org/abs/1301.3781) *Efficient Estimation of Word Representations in Vector Space* which proposes two novel model architectures for computing continuous vector representations of words from very large data sets (~1.6 billion words)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ce9b5f-4cdd-49d6-b29f-7473bb1a6a02",
   "metadata": {},
   "source": [
    "> It was Mikolov et al. (2013), however, who really brought word embedding to the fore through the creation of word2vec, a toolkit enabling the training and use of pre-trained embeddings.\n",
    ">\n",
    "> [aylien - blog](https://aylien.com/blog/overview-word-embeddings-history-word2vec-cbow-glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d92d47-b77d-45d9-b3c7-02e630c3071b",
   "metadata": {},
   "source": [
    "It is important to note that these new architectures are computationally less expensive when compared with previous models (like Bengio or Collobert) because of two feaures:\n",
    "\n",
    "- They forgo the costly hidden layer.\n",
    "- They allow the language model to take additional context into account.\n",
    "\n",
    "[aylien - blog](https://aylien.com/blog/overview-word-embeddings-history-word2vec-cbow-glove)\n",
    "\n",
    "In addition, Mikolov and team contribute to the discussion of how training time and accuracy depends\n",
    "on the dimensionality of the word vectors and on the amount of the training data. Building on previous Mikolov publications as well as the work of others they construct an equation to model the time complexity of the training process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f202271-4b8d-4c65-a500-f5ec1fc32ad9",
   "metadata": {},
   "source": [
    "### Pennington et al. (2014) - Glove"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa3efe8-cdd1-451c-92ff-029177b4b57d",
   "metadata": {},
   "source": [
    "A year later, Pennington et al. introduced us to GloVe, a competitive set of pre-trained embeddings, suggesting that word embeddings was suddenly among the mainstream.\n",
    ">\n",
    "> [source](https://aylien.com/blog/overview-word-embeddings-history-word2vec-cbow-glove#reference1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17036fa6-a534-403f-b05c-10ff4b379185",
   "metadata": {},
   "source": [
    "## Problem: Global Meaning vs. Contextual Meaning\n",
    "\n",
    "Traditional word embedding techniques learn meaning globally. In other words, they learn an embedding that is applicable to all instances of a given word within the training corpus. They do this by computing and encoding global co-occurance statistics for a given word into the coresponding vector. As such, regardless of the context, every instance of a word will map to the same vector, ragardless of it's context. \n",
    "\n",
    "In some writings, we use the term \"static\" to reflect the fact that the vectors are not changing based on the context.\n",
    "\n",
    "The problem with this approach is that while we do capture some contextual information, there is a \"blurring\" that \"mutes\" the nuance of a particular word. \n",
    "\n",
    "For example, only one representation is learnt for the word \"left\" in sentence \"I left my phone on the left side of the table.\" However, \"left\" has two different meanings which depend on the context of the word. A better model would be able to handle two different representations in the embedding space.\n",
    "\n",
    "[stackoverflow](https://stackoverflow.com/questions/62272056/what-are-the-differences-between-contextual-embedding-and-word-embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5853424e-ed84-4da5-bcec-a6f6e2aaf0f3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Solution: Context Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c01ca4d-9946-4b50-b22c-3093f759d26f",
   "metadata": {},
   "source": [
    "Contextual embedding methods however are designed to learn sequence-level semantics with context being expressed as a function over the entire input sequence. As such, Context Embeddings are able to transcend the limitations of traditional i.e. global word representations (word2vec, GloVe) which look at context as a fucntion of co-oocurance within the input sequence. \n",
    "They are able to learn different representations for polysemous words (words with multiple meanings, i.e. homonyms), e.g. \"left\" in example above, based on their context.\n",
    "\n",
    "Additionally, even if a word has a similar meaning and is used in a similar way, the context embedding will still be slightly different due to the difference in context.\n",
    "\n",
    "For example, consider the two sentences:\n",
    "- I will show you a valid point of reference and talk to the point.\n",
    "- Where have you placed the point.\n",
    "\n",
    "Now, the word embeddings from a pre-trained embeddings such as word2vec, the embeddings for the word 'point' is same for both of its occurrences in sentence one and also the same for the word 'point' in sentenct two. (all three occurrences has same embeddings). \n",
    "\n",
    "The context embeddings on the other hand will be different between all three occurrances of the word point.\n",
    "\n",
    "[stackoverflow](https://stackoverflow.com/questions/62272056/what-are-the-differences-between-contextual-embedding-and-word-embedding)\n",
    "\n",
    "This disctinguishes Context Embeddings as a new class with fundamentally different philosophies on the nature of context and thus the inherent meaning of words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756a5a17-6ad8-4be6-8630-03a9b89e9e20",
   "metadata": {},
   "source": [
    "An additional difference is that context embeddings are generally oriented at the token level rather than the word level. They assign each token a representation based on its context. As such, it is possible for the same token to have multiple representations based on it's context. \n",
    "\n",
    "[ Liu et. al. (2020) - A Survey on Contextual Embeddings](https://arxiv.org/abs/2003.07278)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1890b262-6008-4614-8aaf-c1fc88315ceb",
   "metadata": {},
   "source": [
    "Another distinction is that the ability to differentiate nuance between differering context words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb8384e-2b06-4cd7-a02e-912492f6fdec",
   "metadata": {},
   "source": [
    "> A limitation of CBOW is that it equally weights the context words when making a prediction, which is inefficient, since some words have higher predictive value than others.\n",
    ">\n",
    "> https://aclanthology.org/2020.coling-main.608.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381e7a65-0aab-43d1-adae-95eb44e1ba15",
   "metadata": {},
   "source": [
    "These advancements have lead to the discovery that Context Embeddings are transferrable between languanges. A great article on the subject can be found [here](https://www.ruder.io/cross-lingual-embeddings/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2f7c39-71c6-40fd-9a1f-bd02a0e301bf",
   "metadata": {},
   "source": [
    "> Further analyses (Liu et al., 2019a; Hewitt and Liang , 2019 ; Hewitt and Manning , 2019 ; Tenney et al. , 2019a) demonstrate that contextual embeddings are capable of learning useful and transferable representations across languages\n",
    "\n",
    "[ Liu et. al. (2020) - A Survey on Contextual Embeddings](https://arxiv.org/abs/2003.07278)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a624228-2ff1-46d6-92f4-599da1413ba3",
   "metadata": {},
   "source": [
    "**Note**: Despite significant differences, the two terms are used interchangably in a number of texts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a29f58-33b8-4b5d-9a76-7328b2e31226",
   "metadata": {},
   "source": [
    "Below we list a number of major developments within the field:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a8a4db-23dc-4f8d-8238-d02329f5da22",
   "metadata": {},
   "source": [
    "### Dai and Le (2015) - Precursor To Modern Context Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ed771e-2e05-4041-adbe-834f0e678385",
   "metadata": {},
   "source": [
    "> Dai and Le (2015) is the first work we are aware of that uses language modelling together with a sequence autoencoder to improve sequence learning with recurrent networks. Thus, it can be thought of as a precursor to modern contextual embedding methods.\n",
    ">\n",
    "> [ Liu et. al. (2020) - A Survey on Contextual Embeddings](https://arxiv.org/abs/2003.07278)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1d852d-6f49-49c6-9b44-2867a2646cf4",
   "metadata": {},
   "source": [
    "### (Rocktaschel et al. 2015) - Traversal-style Approaches\n",
    "> These approaches pre-process each text input as a single contiguous sequence of tokens through special tokens including [START] (the start of a sequence), [DELIM] (delimiting two sequences from the text input) and [EXTRACT] (the end of a sequence).\n",
    ">\n",
    "> [ Liu et. al. (2020) - A Survey on Contextual Embeddings](https://arxiv.org/abs/2003.07278)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921bf428-4e2b-4946-ad59-fcfe4d7ef13c",
   "metadata": {},
   "source": [
    "### (2015) - OpenAI Founded\n",
    "\n",
    "> OpenAI was founded in 2015 as a nonprofit research organization by Altman, Elon Musk, Peter Thiel, and LinkedIn cofounder Reid Hoffman, among other tech leaders.\n",
    ">\n",
    "> [vice](https://www.vice.com/en/article/5d3naz/openai-is-now-everything-it-promised-not-to-be-corporate-closed-source-and-for-profit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e9a7b6-099e-421c-b9ad-66fd7c0b9aa2",
   "metadata": {},
   "source": [
    "### Ramachandran et al. (2016) - Initializing With Pretrained Weights\n",
    "> Ramachandran et al. (2016) extends Dai and Le (2015) by proposing a pre-training method to improve the accuracy of sequence to sequence (seq2seq) models. The encoder and decoder of the seq2seq model is initialized with the pre-trained weights (as opposed to random weights, which are resolved based on the input and output sequence corpus).\n",
    ">\n",
    "> [ Liu et. al. (2020) - A Survey on Contextual Embeddings](https://arxiv.org/abs/2003.07278)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af480fc1-83f3-4166-8b4a-142de82297bb",
   "metadata": {},
   "source": [
    "### (Vaswani et al., 2017) - Transformer Architecture\n",
    ">  has been shown to better capture global dependencies from the inputscompared to its alternatives, e.g. recurrent networks, and perform strongly on a range of sequence learning tasks, such as machine translation (Vaswani et al., 2017) and document generation (Liu et al., 2018).\n",
    ">\n",
    "> [ Liu et. al. (2020) - A Survey on Contextual Embeddings](https://arxiv.org/abs/2003.07278)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87485530-c4f4-432b-9bb8-4dcf0807c7dd",
   "metadata": {},
   "source": [
    "This model is discussed in further detail in the [Transformer notebook](Transformers.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7d1ffc-79cf-4adb-b752-0cd9a1f402c5",
   "metadata": {},
   "source": [
    "### (Peters et al., 2018) - Bidirectional Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db02b9e0-086b-4374-9a94-d01755ac1ab4",
   "metadata": {},
   "source": [
    "> The ELMo model (Peters et al., 2018) generalizes traditional word embeddings by extracting context-dependent representations from a bidirectional language model.\n",
    ">\n",
    "> [ Liu et. al. (2020) - A Survey on Contextual Embeddings](https://arxiv.org/abs/2003.07278)\n",
    "\n",
    "The paper is titled *Deep contextualized word representations* and can be found [here](https://arxiv.org/abs/1802.05365)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f892c24-baab-46a7-9968-1953bd0c0d7e",
   "metadata": {},
   "source": [
    "### (Radford et al., 2018) - GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900e523c-b67b-452e-a2e7-6edb875c0c0b",
   "metadata": {},
   "source": [
    "> GPT adopts a two-stage learning paradigm: (a) nsupervised pre-training using a language modelling objective and (b) supervised fine-tuning.\n",
    ">\n",
    "> The goal is to learn universal representations transferable to a wide range of downstream tasks.\n",
    ">\n",
    "> [ Liu et. al. (2020) - A Survey on Contextual Embeddings](https://arxiv.org/abs/2003.07278)\n",
    "\n",
    "The Radford and the rest of the OpenAI team published *Improving Language Understanding\n",
    "by Generative Pre-Training* on June 11, 2018 which can be found [here](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)\n",
    "\n",
    "GPT is a proprietary model owned by OpenAi."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5bd8b0-7b8b-4c7e-932b-92f71d097564",
   "metadata": {},
   "source": [
    "### (Devlin et al., 2018) - BERT - Masked Language Modeling (MLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a764c3-f955-4bfc-b83e-574ec9e79022",
   "metadata": {},
   "source": [
    "ELMo concatenates representations from the forward and backward LSTMs without considering the interactions between the left and right contexts. GPT and GPT-2 use a left-to-right decoder, where every token can only attend to its left context. These architectures are sub-optimal for sentence-level tasks,\n",
    "e.g. named entity recognition and sentiment analysis, as it is crucial to incorporate contexts from\n",
    "both directions. \n",
    "\n",
    "BERT proposes a masked language modelling (MLM) objective, where some of the tokens of a input sequence are randomly masked, and the objective is to predict these masked positions taking the corrupted sequence as input. BERT applies a Transformer encoder to attend to bi-directional contexts during pre-training.\n",
    "\n",
    "[ Liu et. al. (2020) - A Survey on Contextual Embeddings](https://arxiv.org/abs/2003.07278)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656831d1-3f83-407a-a566-a6d108616677",
   "metadata": {},
   "source": [
    "### (Radford et al., 2019) GPT-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75696e5-036f-4746-b37e-ffe357552fbf",
   "metadata": {},
   "source": [
    "GPT2 follows a similar architecture to the original GPT.\n",
    "\n",
    "It trains on a significantly larger corpus named WebText which (scraped from reddit posts and coresponding outbound links) which inherantly exhibits instances of text formatted in a question and answer style structure as well as summarization information. This results in the model being ablt to solve a wide variety of NLP tasks without explicit supervision.\n",
    "\n",
    "Like GPT, GPT-2 uses a left-to-right decoder rather than a bi-directional encoder like ELmo.\n",
    "\n",
    "[ Liu et. al. (2020) - A Survey on Contextual Embeddings](https://arxiv.org/abs/2003.07278)\n",
    "\n",
    "In the original paper the authors compare GPT-2 to GPT and BERT highlighting that GPT-2 is a \"larger\" model meaning that the neural network is larger and thus more weights need to be trained. The GPT-2 model is said to have ~1.5 Billion parameters.\n",
    "\n",
    "GPt-2 was published February 2019 in a paper titled *Language Models are Unsupervised Multitask Learners* and can be found [here](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39029c3-627b-4b30-997a-815b3fd31161",
   "metadata": {},
   "source": [
    "### (2018 - 2019) - BERT Variants \n",
    "Variants of BERT including EARNIE, SpanBert, StructBert, RoBERTa, ALBERT further study and improve the objective and architecture of BERT\n",
    "\n",
    "[ Liu et. al. (2020) - A Survey on Contextual Embeddings](https://arxiv.org/abs/2003.07278)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b105a9-b257-47be-a2c1-b24e9a5f513f",
   "metadata": {},
   "source": [
    "### (Yang et al., 2019) - XLNet, Traversal Without Artificial Symbols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a0a3d8-41fb-4352-ab68-9cce544fc38b",
   "metadata": {},
   "source": [
    "The XLNet model identifies two weaknesses of BERT:\n",
    "1. BERT assumes conditional independence of corrupted tokens.\n",
    "2. The symbols such as [MASK] are introduced by BERT during pre-training, yet they never occur in real data, resulting in a discrepancy between pre-training and fine-tuning.\n",
    "\n",
    "XLNet proposes a new auto-regressive method based on permutation language modelling (PLM) (Uria et al., 2016) without introducing any new symbols. \n",
    "\n",
    "XLNet further adopts two-stream self-attention and Transformer-XL (Dai et al., 2019) to take into account the target positions and learn longrange dependencies, respectively\n",
    "\n",
    "[ Liu et. al. (2020) - A Survey on Contextual Embeddings](https://arxiv.org/abs/2003.07278)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4022a66a-f7e9-429e-a8eb-61e77d60dcd2",
   "metadata": {},
   "source": [
    "### (Clark et al., 2019) - ELECTRA - Traversing Via Imputation\n",
    "\n",
    "> Compared to BERT, ELECTRA (Clark et al., 2019) proposes a more effective pretraining method. Instead of corrupting some positions of inputs with [MASK], ELECTRA replaces some tokens of the inputs with their plausible alternatives sampled from a small generator network. ELECTRA trains a discriminator to predict whether each token in the corrupted input was replaced by the generator or not. The pre-trained discriminator can then be used in downstream tasks for fine-tuning, improving upon the pre-trained representation learned by the generator.\n",
    "\n",
    "[ Liu et. al. (2020) - A Survey on Contextual Embeddings](https://arxiv.org/abs/2003.07278)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489fab82-7591-4662-bc09-31bb9ab640af",
   "metadata": {},
   "source": [
    "### (Lewis et al., 2019) - BART - Advanced MLM\n",
    "\n",
    "For more information see the [BART notebook](BART.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1bfab8-6b3a-4be4-aa88-143d0577a13b",
   "metadata": {},
   "source": [
    "### (March 2019) - OpenAI goes Closed-source\n",
    "\n",
    "> The code behind both GPT-1 and GPT-2 has been officially released by OpenAI and is available on GitHub for any developer to utilize and make improvements on. The same cannot be said for GPT-3. Rather than deliver on their original promise listed in their mission statement claiming that “[our code] will be shared with the world,” (OpenAI) OpenAI instead decided to not release the source code for GPT-3 and instead release the model in the form of service.\n",
    ">\n",
    "> [source](https://sites.imsa.edu/hadron/2021/02/03/openai-was-the-shift-to-closed-source-justified/)\n",
    "\n",
    "> In (March) 2019, OpenAI became a for-profit company called OpenAI LP, controlled by a parent company called OpenAI Inc. The result was a “capped-profit” structure that would limit the return of investment at 100-fold the original sum. If you invested \\\\$10 million, at most you’d get \\\\$1 billion. Not exactly what I’d call capped.\n",
    ">\n",
    "> A few months after the change, Microsoft injected $1 billion. OpenAI’s partnership with Microsoft was sealed on the grounds of allowing the latter to commercialize part of the tech, as we’ve seen happening with GPT-3 and Codex.\n",
    ">\n",
    "> [source](https://onezero.medium.com/openai-sold-its-soul-for-1-billion-cf35ff9e8cd4)\n",
    ">\n",
    "> [source](https://www.technologyreview.com/2020/02/17/844721/ai-openai-moonshot-elon-musk-sam-altman-greg-brockman-messy-secretive-reality/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6186f0-5d0b-4dae-a818-b6dee7857c1f",
   "metadata": {},
   "source": [
    "### (Radford et al. 2020) - GPT-3 - 175 Billion Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dae2859-1b08-4637-b44f-143b91df8f6b",
   "metadata": {},
   "source": [
    "GPT-3 was announced in may and release in june 2020. \n",
    "\n",
    "In the [paper](https://arxiv.org/abs/2005.14165) the authors note that increasing the size of the language model improves is ability to perform downstream one-shot tasks:\n",
    "\n",
    "> Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96309cf8-4dce-41a2-9190-16bc3a8635f7",
   "metadata": {},
   "source": [
    "### (March 2022) - GPT-3.5 - Enhancements And Newer Data\n",
    "\n",
    "In March 2022, OpenAI made available new versions of GPT-3 which were trained on newer data sets (including text as well as code) spannign up to June 2021. Additionally, the public api added new features like edit and insert.\n",
    "\n",
    "There were several models/releases included in the 3.5 family:\n",
    "- gpt-3.5-turbo (chat)\n",
    "- text-davinci-002 (text completion)\n",
    "- text-davinci-003 (text completion)\n",
    "\n",
    "https://en.wikipedia.org/wiki/GPT-3\n",
    "\n",
    "> GPT-3.5 is an upgraded version of GPT-3 with fewer parameters that includes a fine-tuning process for machine learning algorithms. The fine-tuning process involves reinforcement learning with human feedback, which helps to improve the accuracy and effectiveness of the algorithms. Additionally, GPT-3.5 is designed to work within policies based on ethical human values, ensuring that the AI systems it powers are safe and reliable for human use.\n",
    ">\n",
    "> [source](https://www.iffort.com/blog/2023/03/31/gpt-3-vs-gpt-3-5)\n",
    "\n",
    "> Instead of releasing GPT-3.5 in its fully trained form, OpenAI utilized it to develop several systems specifically optimized for various tasks, all accessible via the OpenAI API. One of these, text-davinci-003, is said to handle more intricate commands than models constructed on GPT-3 and produce higher quality, longer-form writing.\n",
    ">\n",
    "> OpenAI data scientist Jan Leike stated that text-davinci-003 is comparable to InstructGPT, a series of GPT-3-based models that OpenAI introduced earlier this year. These models are designed to minimize the generation of problematic text, like toxic or highly biased content, while better adhering to a user’s intentions.\n",
    ">\n",
    "> https://blog.accubits.com/gpt-3-vs-gpt-3-5-whats-new-in-openais-latest-update/#What%E2%80%99s-different-in-GPT-3.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a902249-8ee6-4219-9af8-c001ff56896f",
   "metadata": {},
   "source": [
    "### (November 2022) - ChatGPT - User Facing Chat Bot\n",
    "\n",
    "The original release of ChatGPT was based on the GPT-3 family models but has since been updated to support newer version (GPT-4) as well.\n",
    "\n",
    "While the GPT family of models are geared towards researchers and developers, ChatGPT is geared towared uses. Through the Web UI or API, users can now interract with the GPT models in a much friendlier way.\n",
    "\n",
    "This marked the beginning of a monumental shift in the worlds perception of this technology."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b054172f-e957-4fee-bc49-1974f6f6a3b0",
   "metadata": {},
   "source": [
    "### (March 2023) - GPT-4 - 100 trillion parameters\n",
    "\n",
    "In addition to it's large size, GPT-4 bosts the following enhancements:\n",
    "\n",
    "- Improved model alignment — the ability to follow user intention\n",
    "- Lower likelihood of generating offensive or dangerous output\n",
    "- Increased factual accuracy\n",
    "- Better steerability — the ability to change behavior according to user requests\n",
    "- Internet connectivity – the latest feature includes the ability to search the Internet in real-time\n",
    "\n",
    "[source](https://www.forbes.com/sites/bernardmarr/2023/05/19/a-short-history-of-chatgpt-how-we-got-to-where-we-are-today/?sh=515ef82f674f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f51d88f-2e34-4491-9472-275c793e37a5",
   "metadata": {},
   "source": [
    "# Basic Concept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517e88a8-0a46-48f8-9bcb-f6371983a71f",
   "metadata": {},
   "source": [
    "Word embeddings are again, a type of encoding that uses a vector to store the encoding information. However, as opposed to some of the previous implimentations which use discrete vectors, Word embeddings use continuous vectors to represent each word in a vocabulary. \n",
    "\n",
    "The trick with word embeddings is that the dimensions corespond to a particular meaning of a word. Additionally meaning is not mutually exclusive, words can have similar meaning and then have non-zero values in that dimension, or negative values if they have the opposite meaning for example. \n",
    "\n",
    "Words that are similar to eachother in meaning have similar vector values and are thus geometrically oriented such that they are close to those words with similar meaning.\n",
    "\n",
    "This is an important design consideration; the overlap allows the vectors to be much more dense than the prior encodings which were all sparse.\n",
    "\n",
    "So the question becomes: what are the meanings associated with each dimension. This is a difficult question to answer because the dimensions are chosen dynamically based on a machine earning algorithm which selects the optimal dimensions based on some loss function. \n",
    "\n",
    "We will also see that there are many implimentation which use their own loss fucntions etc. to determine the dimensionality and the values of the vectors.\n",
    "\n",
    "But a common analogy is to think of the principal axes or principal components one discovers through [Principal Componen Analysis (PCA)](../../../../Data%20Science/Principal%20Component%20Analysis%20(PCA)/Principal%20Component%20Analysis.ipynb). According to Becker (2000) the dimensionality is usualy between 100 and 500 principle meanings of words.\n",
    "\n",
    "Using this analogy, the word embedding algorithm will seek to minimize the distance between similar words while minimizing the dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811e5ad7-717a-4d7d-b917-cf60e7a807b2",
   "metadata": {},
   "source": [
    "<center><img src=\"images/word_embeddings.png\"\"><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374029e8-1b47-4032-9fad-32089735130c",
   "metadata": {},
   "source": [
    "Another important feature of the Word Embeddings is that because they are continuous and encode meaning, they allow us to perform algebra:\n",
    "\n",
    "> With such word vectors even algebraic computations become possible as shown in Tomáš Mikolov, Yih, and Zweig (2013). For example, vector(King)−vector(Man)+vector(Woman) results in a vector that is closest to the vector representation of the word Queen. Another possibility to use word embeddings vectors is translation between languages. Tomas Mikolov, Le, and Sutskever (2013) showed that they can find word translations by comparing vectors generated from different languages. By searching for a translation one can use the word vector from the source language and search for the closest vector in the target language vector space, this word can then be used as a translation. The reason this works is that if a word vector from one language is similar to the word vector of the other language, this word is used in a similar context. This method can be used to infer missing dictionary entries. An example for this method depicted in figure 3.4. In figure 3.4 the vectors for numbers and animals are depicted on the left side and the same words are depicted on the right side. It can be seen that the vectors for the correct translation align in similar geometric spaces. Again, two-dimensional representation was achieved by using dimension reduction methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9157c2-24a2-4646-a00a-885a709cc1d1",
   "metadata": {},
   "source": [
    "<center><img src='images/word_embedding_algebra.png'></center>\n",
    "\n",
    "> FIGURE 3.4: Distributed word vector representations of numbers and animals in English (left) and Spanish (right). Source: Tomas Mikolov, Le, and Sutskever (2013)\n",
    ">\n",
    "> Becker et. al. (2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace1e75f-cba4-4f13-b485-757ba4c67adf",
   "metadata": {},
   "source": [
    "There are a number of implimentations for word embeddings as we will see."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfa83fb-5791-41bd-9530-182d30f6ef98",
   "metadata": {},
   "source": [
    "# Intuition Behind Terminology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f1f683-ca0b-4cc2-b5a4-2f7be14e9c10",
   "metadata": {},
   "source": [
    "I started wondering why word embeddings were given their name. I gogled and found the following answer on [quora](https://www.quora.com/Why-are-word-embeddings-called-word-embeddings):\n",
    "\n",
    "> \"Word\": This part of the term specifies that we are dealing with individual words in the vocabulary. In reality we have evolved the term to deal with \"tokens\" which may be only parts of words.\n",
    ">\n",
    "> The term \"embedding\" comes from the field of mathematics, where it refers to the process of mapping objects from one space into another, often with the goal of preserving certain relationships or properties. In the context of word embeddings: ... (the term) signifies the act of placing or mapping words into a continuous vector space. Just as embedding a physical object in a material might mean surrounding it or encapsulating it within that material, word embeddings encapsulate the semantic meaning and relationships of words within a vector space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9994edf7-0cba-43ee-ada0-7c96696a51f9",
   "metadata": {},
   "source": [
    "# Implimentations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0db94d-5e95-4395-a779-c16241b4a934",
   "metadata": {},
   "source": [
    "## Word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b89329c-2894-48e7-9f54-02bc342141fa",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ddc95c-eb9d-4252-bc73-e6dc913d8519",
   "metadata": {},
   "source": [
    "From googling, I see that word2vec is not a singular algorithm, rather, it is a family of model architectures and optimizations for learning word embeddings.\n",
    "\n",
    "Within the word2vec umbrella, there are two implimentations of word embeddings:\n",
    "\n",
    "- **Continuous bag-of-words model**: predicts the middle word based on surrounding context words. The context consists of a few words before and after the current (middle) word. This architecture is called a bag-of-words model as the order of words in the context is not important.\n",
    "- **Continuous skip-gram model**: predicts words within a certain range before and after the current word in the same sentence. A worked example of this is given below.\n",
    "\n",
    "Generally speaking, the two algorithms rely on shallow neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee70abb-45ba-4c95-bc20-6791376bfb7c",
   "metadata": {},
   "source": [
    "A great articles can be found [here](https://jalammar.github.io/illustrated-word2vec/), [here](https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa), and [here](https://towardsdatascience.com/what-in-the-corpus-is-a-word-embedding-2e1a4e2ef04d)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f99bf7c-70bd-40d1-bb83-f04c32af3901",
   "metadata": {},
   "source": [
    "### History"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac59d83-141d-452e-80a1-0a2b95df9c88",
   "metadata": {},
   "source": [
    "According to [wikipedia](https://en.wikipedia.org/wiki/Word2vec), Word2vec was published in 2013 by a team of researchers led by Mikolov at Google over two papers respectively titled [Efficient Estimation of Word Representations in Vector Space\n",
    "](https://arxiv.org/abs/1301.3781) and [Distributed Representations of Words and Phrases and their Compositionality](https://ui.adsabs.harvard.edu/abs/2013arXiv1310.4546M/abstract). Among the authors, Tomas Mikolov is the most widely cited. Additionally the algorithm was [patented](https://worldwide.espacenet.com/patent/search/family/053054725/publication/US9037464B1?q=pn%3DUS9037464) in 2015."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2336f2-eb8e-4764-9a1a-b73e4bed3244",
   "metadata": {},
   "source": [
    "Additionally, a follow up paper was [published](https://arxiv.org/abs/1402.3722) in 2014 by Goldberg et. al. explaining the math and rational behind it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fcbd2d-fd6a-4290-8cea-fc4482ccf0cc",
   "metadata": {},
   "source": [
    "With the invention of the [Transformer](./Transformers.ipynb), the word2vec algorithm is seen as being outdated as a means of producing word ebmeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6ba7b2-3942-4e9c-b3bf-e437519477fc",
   "metadata": {},
   "source": [
    "### Tutorials\n",
    "\n",
    "[Using tensorflow](https://www.tensorflow.org/text/tutorials/word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be827c4-e86e-4179-a013-63a6615d3f7e",
   "metadata": {},
   "source": [
    "## GloVe\n",
    "\n",
    "\n",
    "\n",
    "GloVe was [published](https://aclanthology.org/D14-1162/) in 2014 by a team of researcher at Stanford University (Pennington et. al.). The project is open source and the home page can be found [here](https://nlp.stanford.edu/projects/glove/). It hosts several iterations and versions of word vectors trained through various means.\n",
    "\n",
    "According to the paper:\n",
    "\n",
    "> (we introduce... ) a new model for word representation which we call GloVe, for Global Vectors, because the global corpus statistics are captured directly by the model.\n",
    "\n",
    "According to the home page:\n",
    "> Training is performed on aggregated global word-word co-occurrence statistics from a corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4bb244-3736-433f-acc0-b2869a01f355",
   "metadata": {},
   "source": [
    "And according to the [paper's](https://aclanthology.org/D14-1162.pdf) abstract:\n",
    "> Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1851e97a-35e2-477d-8450-d147ccf33e96",
   "metadata": {},
   "source": [
    "While algorithms like word2vec implicitly derive word meaning, GloVe tries to do this explicitly.\n",
    "\n",
    "> the creators of GloVe illustrate that the ratio of the co-occurrence probabilities of two words (rather than their co-occurrence probabilities themselves) is what contains information and so look to encode this information as vector differences.\n",
    ">\n",
    "> For this to be accomplished, they propose a weighted least squares objective (J) that directly aims to reduce the difference between the dot product of the vectors of two words and the logarithm of their number of co-occurrences\n",
    ">\n",
    "> ...\n",
    ">\n",
    "> With GloVe, we have already seen that the differences are not as obvious: While GloVe is considered a predict model by Levy et al. (2015) [10], it is clearly factorizing a word-context co-occurrence matrix, which brings it close to traditional methods such as PCA and LSA. Even more, Levy et al. [12] demonstrate that word2vec implicitly factorizes a word-context PMI matrix.\n",
    "> [source](https://aylien.com/blog/overview-word-embeddings-history-word2vec-cbow-glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba1100f-24ab-4bbd-9daa-4212b772113d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c68f3ad-e21d-4adc-b929-748c75f2b80b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d1c344-d18b-4026-9f23-2bd1a8e70aac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
