{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb665c3b-fb62-4070-a30d-21aae459f8db",
   "metadata": {},
   "source": [
    "# BART Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13a7b9c-4252-49bf-a1bf-7c6751f7501e",
   "metadata": {},
   "source": [
    "BART was [published](https://arxiv.org/abs/1910.13461) in October 2019. The abstract describes BART as a denoising autoencoder \n",
    "for pretraining sequence-to-sequence models Specifically, the paper states:\n",
    "\n",
    "> BART ... pre-trains \n",
    "a model combining Bidirectional and Auto-Regressiv \n",
    "Transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a946aefd-93fa-4ca5-a12e-e00846997d69",
   "metadata": {},
   "source": [
    "Recall that an [autoencoder](ExtraTopics.ipynb#Autoencoder) is a special case of the encoder-decoder architecture in which the input is also provided as the output. The authors not that BART is specifically a Transformer. In this case the encoder creates an intermediary representation of the data that the decoder then learns how to translate back into the original input space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c29ad4-4964-43d5-82b8-8b6133821bf6",
   "metadata": {},
   "source": [
    "Generally spreaking, the term denoising refers to the process of ignoring or removing irrelevant information (the noise) from a dataset so that it accurately reflects the underlying process.\n",
    "\n",
    "In the context of an autoencoder, the term denoise refers to the model ignoring irrelevant tokens in the input sequence while still predicting the right output sequence.\n",
    "\n",
    "The paper goes even further however to abstract the concept of noise so that it applies to missing or \"corrupt\" data (i.e. invalid tokens, random order, etc.). Thus BART is able to decode corrupt input sequences and produce the desired output sequence.\n",
    "\n",
    "It notes that: \n",
    "> Unlike existing denoising autoencoders, which are tailored to specific noising schemes, BART allows us to\n",
    "apply any type of document corruption. In the extremeecase, where all information about the source is lost, BART is equivalent to a language model.\n",
    "\n",
    "Applying this ability to a solution that generates answers to questions, the implication would be that training on corrupted questions will allow the model to yield the correct response regardless of how the question is asked allowing it to yield better accuracy scores.\n",
    "\n",
    "> This approach generalizes the original word masking and next sentence prediction objectives in BERT by forcing the model to reason more about overall sentence length and\n",
    "make longer range transformations to the input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1286a9b8-343c-47cd-8f70-1f2bc90252ce",
   "metadata": {},
   "source": [
    "# Use Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18007046-3075-4c38-b2ad-220e5a6eb2d0",
   "metadata": {},
   "source": [
    "The authors claim that the representations produced by BART can be used in \n",
    "several ways for downstream application including:\n",
    "\n",
    "- Sequence Classification Tasks\n",
    "- Token Classification Tasks\n",
    "- Sequence Generation Tasks\n",
    "- Machine Translation\n",
    "\n",
    "**Note**: The authors use similar terminology as those who published [BERT](./BERT.ipynb) by referring to BART as a model that produces representations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838c88f6-69f3-413e-8c6c-91c295cf4f21",
   "metadata": {},
   "source": [
    "## Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ef15e9-55e4-4dbb-86d0-cded0f07efa7",
   "metadata": {},
   "source": [
    "The paper notes that a major advancement of BART is that it changes the way we think about pretraining and fine tuning.\n",
    "\n",
    "Previously, with BERT, the encoder and decoder were separated by the embeddings (representations). The encoder translated an input sequence into an embedding and the decoder translated the embedding into an output sequence. This required the encoder inputs to be \"aligned\" with the decoder outputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb18192b-c1ac-43d5-93c2-8b6a16fa08ad",
   "metadata": {},
   "source": [
    "**Note**: Recall that [alignment](Neural%20Sequence%20Transducers.ipynb#Alignment) deals with the relationship between input and output tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9430da-d00d-40bc-ad0b-c3932b607906",
   "metadata": {},
   "source": [
    "The authors state:\n",
    "\n",
    "> (with BART)... Inputs to the encoder need not be aligned with decoder outputs, allowing arbitary noise transformations. Here, a document has been corrupted by replacing spans of text with mask symbols. The corrupted document (left) is encoded with a bidirectional model, and then the likelihood of the original document (right) is calculated with an autoregressive decoder. For fine-tuning, an uncorrupted document is input to both the encoder and decoder, and we use representations from the final hidden state of the decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f355e5e3-14a9-4efb-8200-9b339de48e20",
   "metadata": {},
   "source": [
    "The authors go on to discuss the various noise transformations used by BART's encoder (red) to abritrarily introduce noise that the decoder (blue) is trained to remove.\n",
    "\n",
    "<center><img src=\"./images/BART_encoder_transformations.png\" style=\"width: 50%\" ></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f107a707-851b-4dd1-b1f4-a53d095f9cb8",
   "metadata": {},
   "source": [
    "The authors also compare BART to BERT and GPT:\n",
    "\n",
    "> BART uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes\n",
    "\n",
    "<center><img src='images/BART_comparison_with_BERT_and_GPT.png'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8097fe-1a90-4368-81c4-496f4cd72dbb",
   "metadata": {},
   "source": [
    "With BART, the embedding passed between encoder and decoder are \"denoised\". In the case of fine tuning, this would mean that the input sequence is transformed into a sequence that is most like the optimal input for the decoder to produce the desired output."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
