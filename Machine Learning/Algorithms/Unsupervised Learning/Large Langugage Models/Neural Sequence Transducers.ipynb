{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5903c2e6-9d88-4c46-aeed-bd74d8d13893",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d0533a-18e0-407b-b413-aec3052e5e14",
   "metadata": {},
   "source": [
    "In the original [publication](https://arxiv.org/pdf/1211.3711.pdf), Graves defined his sequence transducer in the context of an RNN. In fact, there is a section in the text with the heading \"Recurrent Neural Network Transducer\". As a result, some texts will refer to the Neural Sequence Transducer as an RNNT or RNN-T model. \n",
    "\n",
    "Over time however, the model has been extended and generalized so that it need not use an RNN. Additionally it is considdered a sequence to sequence model. More information on it's relation to other models can be found [here](https://lorenlugosch.github.io/posts/2020/11/transducer/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee3b1ea-56d4-4925-a6a2-366c08992ca3",
   "metadata": {},
   "source": [
    "# Use Cases\n",
    "\n",
    ">Although RNN-T, CTC and AED offer very good accuracy in recognizing speech, RNN-T typically outperform the others and while being naturally suitable for online streaming mode, it enables the development and deployment of real-time speech recognition applications.\n",
    ">\n",
    "> https://whatsnext.nuance.com/innovation-research/automatic-speech-recognition-on-prediction-network-architecture/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ef9fbd-02f3-48da-be61-b59349c1cab7",
   "metadata": {},
   "source": [
    "# Alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbbd05d-4b20-4545-b8f1-568fdffd8c59",
   "metadata": {},
   "source": [
    "## The Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4914b5-d974-4d48-8df7-f36b7642774e",
   "metadata": {},
   "source": [
    "While reading up on the Neural Sequence Tranducer Model I kept coming accross the concept of alignment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55de684d-646d-42e5-aa1c-099a05d3d986",
   "metadata": {},
   "source": [
    "I found that it was one of the core motivations for the [publication](https://arxiv.org/pdf/1211.3711.pdf) of the Transducer.\n",
    "\n",
    "> RNNs traditionally require a pre-defined alignment between the input and output sequences to perform transduction. This is a severe limitation since finding the alignment is the most difficult aspect of\n",
    "many sequence transduction problems.\n",
    "\n",
    "The author claims that prior to the advancements made in this paper:\n",
    "\n",
    "> RNNs are usually restricted to problems where the alignment between the input and output sequence is known in advance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3091e2-ebb1-46da-9785-7ea0c297edc2",
   "metadata": {},
   "source": [
    "At first it was hard to understand what the term was referring to. The math was quite dense and abstract. So I googled to find more intuitive explanations of what alignment is in terms of machine translation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ee30f0-319e-433a-918f-367834e21193",
   "metadata": {},
   "source": [
    "I found the following articles which helped me understand what alignment means and what is being aligned. In short, I found that alignment is the process or mapping the tokens in the input sequence to tokens in the output sequence:\n",
    "\n",
    "Consider the translation problems where there is not a one-to-one mapping between the input sequence and the output sequence. For example, translating French to English, the number of words in the french phrase may not match the english phrase. We can see that when translating the phrase \"It's raining\" to french. The equivalent phrase would be \"Il pleut\". We see our input has the same number of words as the output. But what about \"It is raining\"? In this circomstance we have three inputs to two outputs; there is not a one to one mapping between the words.\n",
    "\n",
    "[stackoverflow answer](https://stats.stackexchange.com/questions/272012/what-does-alignment-between-input-and-output-mean-for-recurrent-neural-network)\n",
    "\n",
    "Additionally, the order of words may not be the same. For example, consider this translation from Portugese to english:\n",
    "\n",
    "```\n",
    "Uma maçã grande e vermelha\n",
    "(1)   (2)  (3)  (4)   (5)\n",
    " |      \\ /   _______/\n",
    " |       X   /\n",
    " |      / \\ /\n",
    " |     /   X\n",
    " |    /   / \\\n",
    "(1) (3) (5)  (2)\n",
    " A  big red apple\n",
    "```\n",
    "\n",
    "[Stackoverlow answer](https://ai.stackexchange.com/questions/26184/what-is-the-purpose-of-alignment-in-the-self-attention-mechanism-of-transforme) -> Transformers will automatically solve alignment while translating.\n",
    "\n",
    "This problem exists in many architectures, not just RNN-Ts ([source](https://arxiv.org/abs/2112.07806)).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8465e088-46e8-4afd-9c45-117c40c26b66",
   "metadata": {},
   "source": [
    "## Implimentation Details\n",
    "\n",
    "One of the core components of that way that the RNN-T handles alignment is the null character or set $\\varnothing$. This character basically means \"nothing\", \"no character\". The $\\varnothing$ character is injected into the output sequence to denote gaps between the input and output sequence. One such permutation might be as follows:\n",
    "\n",
    "$$ i1, i2, i3, i4 => o1, o2, \\varnothing, o3$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdc2be3-75c2-4a51-bf75-d77aeef6a84f",
   "metadata": {},
   "source": [
    "Assuming that there is a permutation which is the correct representation of the output sequence, the model is then trained so that the probability of obtaining the output sequence (which doesn't conatin the $\\varnothing$ character) is maximized given that coresponding permutation. In this case, the model is trained on the alignments and the alignments play an integral role in producing the correct outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91b7f53-d847-47e3-bd9a-3a0328625456",
   "metadata": {},
   "source": [
    "The process of doing maximizing the likelihood is a bit over my head but is discussed in the original paper as well as in this helpful [article](https://lorenlugosch.github.io/posts/2020/11/transducer/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1741b8c9-227c-4344-b56a-2d993291c3ff",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c79e19-32c0-49df-a40a-8e5708c5e59a",
   "metadata": {},
   "source": [
    "The model consists of two RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23ae774-5d49-41e3-a36e-5f749531578c",
   "metadata": {},
   "source": [
    "<center><img src='./images/neural_sequence_transducer_model_architecture.png' style='width:30%'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9eceb9-56c7-4936-a0fe-a34e902141e4",
   "metadata": {},
   "source": [
    "## Transcription Network\n",
    "The first RNN is referred to as the Transcription Network which the author defined as:\n",
    "\n",
    "> a bidirectional RNN (Schuster & Paliwal, 1997) that scans the input sequence $x$ forwards and backwards with two separate hidden layers, both of which feed forward to a single output layer. Bidirectional RNNs are preferred because each output vector depends on the whole input sequence (rather than on the previous inputs only, as\n",
    "is the case with normal RNNs); however we have not tested to what extent this impacts performance.\n",
    "> \n",
    "> ...\n",
    "> \n",
    "> The transcription network is similar to a Connectionist Temporal Classification RNN, which also uses a null output to define a distribution over input-output alignments.\n",
    "\n",
    "\n",
    "This network accepts the input sequence and outputs a sequence of transcription vectors (also referred to as the transcription sequence in the paper)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3801b8e8-9b49-413b-93c7-38f77d42ccf1",
   "metadata": {},
   "source": [
    "## Predition Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca433b2-342a-4cc3-b953-c232c2286e32",
   "metadata": {},
   "source": [
    "The second network is referred to as the Prediction Network. This scans the output sequence (not the teanscription vectors) and outputs a prediction vector sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc393fa-f45e-427e-b1a3-1b267c72a04c",
   "metadata": {},
   "source": [
    "## Output Distribution (Joiner)\n",
    "\n",
    "According to the publication, the output disribuion quantifies the liklihoos of obtaining a particular alignment $a$ given the input sequence $x$. It is defined as $Pr(a \\in \\bar{y*} | x)$.\n",
    "\n",
    "In more recent [articles](https://whatsnext.nuance.com/innovation-research/automatic-speech-recognition-on-prediction-network-architecture/) the calculation of the probability distribution is treated as a discrete module referred to as the joiner. This module is seen as being interchangable, alowing multiple calculations for the distribution. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
