{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588328ed-ab84-4043-9751-ea217ffa2b35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b4b7650-cd0e-4387-8407-642d219dd382",
   "metadata": {},
   "source": [
    "Overview\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e159af5-4772-4042-9fbb-041cd59f4350",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e26d8d4-97b6-4bc8-9dfc-b04360f649ef",
   "metadata": {},
   "source": [
    "# Clarification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca11758f-31f9-4574-afd9-2edc7e08f418",
   "metadata": {},
   "source": [
    "The Neural Transducer is different from, but strongly related to, the Sequence Transducer (also published by Graves). The latter being cited as:\n",
    "\n",
    "- \\[8] Alex Graves. Sequence Transduction with Recurrent Neural Networks. In International Conference on Machine Learning: Representation Learning Workshop, 2012.\n",
    "- \\[9] Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. Speech Recognition with Deep Recurrent Neural Networks. In IEEE International Conference on Acoustics, Speech and Signal Processing, 2013."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c025af-25ed-4bc3-a7c7-93eae3e0adaf",
   "metadata": {},
   "source": [
    "13."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831f154e-565b-410d-9401-22c039d973fc",
   "metadata": {},
   "source": [
    "# Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857576f5-a22c-47ee-aeaa-55aa9eec14dc",
   "metadata": {},
   "source": [
    "## CTC Model Shortcomings\n",
    "\n",
    "I read online, in a number of places that the shortcomings of the CTC model were one of the main reasons for the publication of the transducer. For [example](https://www.assemblyai.com/blog/an-overview-of-transducer-models-for-asr/):\n",
    "\n",
    "> RNNTs were inspired by the limitations of CTC being highly dependent on an external Language Model to perform well.\n",
    "\n",
    "While the publication of the transducer did not directly mention the CTC as a motivation. I did mention the shortcomings of the CTC's independence assumption (which we will cover) in a small section discussing the related \"sequence transducer\" model which the neural transducer claims to generalize. That being said, I was curious, so here is the argument:\n",
    "\n",
    "Connectionist Temporal Classification (CTC) Models are another model [published](https://www.cs.toronto.edu/~graves/icml_2006.pdf?ref=assemblyai.com) by Graves six years earlier in 2006.\n",
    "\n",
    "A CTC is considered to be simple because it consists of one module, the Encoder, which is used to model ...\n",
    "\n",
    "<center><img src='./images/ctc_architecture.png' stype='width:50%'></center>\n",
    "\n",
    "A key assumption is that model predictions are assumed to be independent (i.e. non-correlated). As discussed in this [article](https://www.assemblyai.com/blog/an-overview-of-transducer-models-for-asr/):\n",
    "\n",
    "> a CTC model ... is, theoretically, overall less accurate because the CTC loss function does not incorporate context.\n",
    "> \n",
    "> Imagine a CTC model outputs the transcript \"I have for apples\". As a human reading this, you can immediately spot the error. \"I have for apples\" should be \"I have four apples\". \n",
    "For a CTC model \"for\" is just as sensical as \"four\" as they are phonetically similar. Since a CTC model’s outputs are conditionally independent of each other, the output of the word \"for\" does not take into consideration the surrounding context of words \"I have ... apples\n",
    "> \n",
    "> Because of these shortcomings, CTC models require an external Language Model, trained separately on millions to billions of sentences, to correct any linguistic errors the CTC model may output.\".\n",
    "\n",
    "To summarize the shortcomings:\n",
    "\n",
    "- Reliance on external language model\n",
    "- Fast to train but slow to converge\n",
    "- Poor performance on proper nouns\n",
    "- Worse performance compared to transducer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a2023f-2996-4533-9fab-6a8f7444641c",
   "metadata": {},
   "source": [
    "## Online Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94409fad-1017-4107-8fe0-4806baf13fa6",
   "metadata": {},
   "source": [
    "The motivation according to the original publication was as follows:\n",
    "\n",
    "> Sequence-to-sequence models have achieved impressive results on various tasks. However, they are unsuitable for tasks that require incremental predictions to be made as more data arrives or tasks that have long input sequences and output sequences.\n",
    "> \n",
    "> ...\n",
    ">\n",
    "> Speech recognition is an example of such an online task ... Similarly, instant translation systems\n",
    ">\n",
    "> (In such scenarios) users prefer seeing an ongoing transcription of speech over receiving it at the “end” of a utterance... systems would be much more effective if ... translated online, rather than after entire utterances.\n",
    ">\n",
    "> This is because (these systems) generate an output sequence conditioned on an entire input sequence. In this paper, we present a Neural Transducer that can make incremental predictions as more input arrives, without redoing the entire computation.\n",
    "\n",
    "**Note**: The paper is specifically sites several papers when defining sequence to sequence models as we will discuss later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce93ee6-31e5-4501-ae96-7c594ec00907",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "737ca67e-bb12-46c4-a0ef-1d6ce215f167",
   "metadata": {},
   "source": [
    "# Origin\n",
    "\n",
    "The Neural Transducer was [published](https://arxiv.org/abs/1511.04868) in November 2015.\n",
    "\n",
    "\n",
    "\n",
    "The paper goes on to site a bunch of papers (17, 4, 1, 6, 3, 20, 18, 15, 19) in reference to the term \"sequence to sequence model:\n",
    "\n",
    "- \\[1] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural Machine Translation by Jointly Learning \n",
    "to Align and Translate. In International Conference on Learning Representations, 2015\n",
    "- \\[3] William Chan, Navdeep Jaitly, Quoc V Le, and Oriol Vinyals. Listen, attend and spell. arXiv preprint\n",
    "arXiv:1508.01211, 2015.\n",
    "- \\[4] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger\n",
    "Schwen, and Yoshua Bengio. Learning Phrase Representations using RNN Encoder-Decoder for Statistical\n",
    "Machine Translation. In Conference on Empirical Methods in Natural Language Processing, 201\n",
    "- \\[6] Jan Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk, Kyunghyun Cho, and Yoshua Bengio. AttentionBased Models for Speech Recognition. In Neural Information Processing Systems, 2015.4\n",
    "- \\[15] Alessandro Sordoni, Michel Galley, Michael Auli, Chris Brockett, Yangfeng Ji, Margaret Mitchell, JianYun Nie, Jianfeng Gao, and Bill Dolan. A neural network approach to context-sensitive generation of\n",
    "conversational responses. arXiv preprint arXiv:1506.06714, 2015.\n",
    "- \\[17] Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to Sequence Learning with Neural Networks. In\n",
    "Neural Information Processing Systems, 2014.- \n",
    "- \\[18] Oriol Vinyals, Lukasz Kaiser, Terry Koo, Slav Petrov, Ilya Sutskever, and Geoffrey Hinton. Grammar as a\n",
    "foreign language. In NIPS, 201- 5.\n",
    "- \\[19] Oriol Vinyals and Quoc V. Le. A neural conversational model. In ICML Deep Learning Workshop, 2015.\n",
    "..\n",
    "\n",
    "But it is consistent in refereing to \\[17] and end even borrows a diagram from that paper to explain the differences in model architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525c2356-0c34-44b7-b8a9-314cff8bf12f",
   "metadata": {},
   "source": [
    "> RNNTs however, did not get any real serious attention until the paper “Streaming End-to-end Speech Recognition for Mobile Devices” in 2018, which demonstrated the ability to use RNNTs on mobile devices for accurate speech recognition.\n",
    "> https://www.assemblyai.com/blog/an-overview-of-transducer-models-for-asr/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06103ec6-5ed2-486d-aac3-71f01af8887a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f30a597-aed3-43d2-aa1b-cfe61e1ff048",
   "metadata": {},
   "source": [
    "# Characteristics\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f148fb1d-0cab-4d0b-8e1f-8f3365b594ab",
   "metadata": {},
   "source": [
    "> The Transducer (sometimes called the “RNN Transducer” or “RNN-T”, though it need not use RNNs) is a sequence-to-sequence model.\n",
    ">\n",
    "> https://lorenlugosch.github.io/posts/2020/11/transducer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f05e3d-3285-43ec-9a01-08a734aa6dcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bd6931-17ac-4ef8-a625-77e0c5951461",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "318247b2-5a6c-4d31-93f6-f8879ede110e",
   "metadata": {},
   "source": [
    "According to the [publication](https://arxiv.org/abs/1511.04868) The first major difference is the conditional model is not based on the entire sequence but the currently available (possibly partial) sequence.\n",
    "\n",
    "> Unlike sequence-to-sequence models, the Neural Transducer computes the next-step distribution conditioned on the partially observed input sequence and the partially generated sequence. At each time step, the transducer can decide to emit zero to many output symbols.\n",
    "\n",
    "As such, when new information arrives in a new time step the model can evaluate this inormation and determine whether it has enough information to translate or if it needs to keep waiting until it does.\n",
    "\n",
    "Making an inference, I believe the next major difference between the classes of models lie in the intermediary vector bring passed between the encoder and decoder. In \\[17] a fixed length vector is being passed between encoder and decoder; but looking at the the paper I believe the transducer  \n",
    "\n",
    "Additionally the paper provides the folowing diagram to compare/contrast the transducer from the req2seq model:\n",
    "\n",
    "> <center><img src='./images/seq2seq_vs_transducer.png'></center>\n",
    ">\n",
    "> Figure 1: High-level comparison of our method with sequence-to-sequence models. (a) Sequence-tosequence model [17]. (b) The Neural Transducer (this paper) which emits output symbols as data come in (per block) and transfers the hidden state across blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81164960-e271-419d-941e-9c9ce2722b7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db68c90d-e789-4a5c-878d-80cf23fbc4c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2ebee8e-3765-42e0-8b63-4476d7df7049",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff424f4e-41d0-4f5a-bfc5-2325182ffb55",
   "metadata": {},
   "source": [
    "> The Encoder models the acoustic features of speech, and the Predictor acts as a Language Model to learn language information from the training data. Finally, the Joint network takes in the predictions from the Encoder and Predictor to produce a label.\n",
    "> \n",
    "> The Predictor and the Joiner network are conditionally dependent, so the next prediction is reliant on the previous prediction. These combinations of modules trained jointly make an external Language Model unnecessary to gain high accurac\n",
    ">\n",
    "> https://www.assemblyai.com/blog/an-overview-of-transducer-models-for-asr/y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c80ff2-fbc2-41cb-9276-ff151e29e84f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb0aeec-e94d-42fa-a727-dc34b51dfee4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116bd75a-54e4-453b-ac32-ac753faa5b91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
