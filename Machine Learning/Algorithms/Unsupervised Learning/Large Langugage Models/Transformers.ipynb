{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25b479a4-b643-4a47-9ac8-f577c0c050b0",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c11fd5-c7da-4a7a-ba34-4b7366097c8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ce381fa-5c02-4c93-8fd8-9f94af6e6823",
   "metadata": {},
   "source": [
    "# Origin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf229a7-1403-4ffe-af3c-c317d9d7ad3a",
   "metadata": {},
   "source": [
    "The Transformer model was [published](https://arxiv.org/abs/1706.03762) in 2017 as a solution to address several problems in the machine translation space. Machine translation, as the name suggests, is the process of using a machine (an ml model) to translate from one language into another. For more information about its lineage and the course of events leading to its development see the complementarty article on [encoder-decoders](Encoder-Decoders.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed855a0-2e18-4813-ac3c-e6b5d5ae25ff",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd110e3-f67b-456e-bfee-6173309a9806",
   "metadata": {},
   "source": [
    "In the 2017 [paper](https://arxiv.org/abs/1706.03762) titled \"Attention Is All You Need\" The following architectural diagram is published:\n",
    "\n",
    "<center><img src=\"./images/transformer_model_architecture.png\" style=\"width:50%\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5832c7bb-8560-4099-8182-8364d48219f2",
   "metadata": {},
   "source": [
    "In my humble opinion, this diagram leaves a lot to be desired. I will try to expand and explain the various components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f186204-74e4-4a27-bd63-2919e4414f15",
   "metadata": {},
   "source": [
    "## Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7770a729-5c67-48f8-88cd-20ed928f9640",
   "metadata": {},
   "source": [
    "The paper explains that the \"inputs\" shows in the architectural diagream refers to a sequence of symbols. As this paper is written to address issues of machine translation we can assume that the symbols are linguistic symols aka. alphabetical characters or letters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb65ae5-2e44-4a70-a58d-b51c98e96641",
   "metadata": {},
   "source": [
    "## Input Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cb7ded-8008-456e-93a9-0489625ffeb1",
   "metadata": {},
   "source": [
    "The paper does not specify how the embeddings are calculated. Instead, it states that it uses learned embeddings similar to those used by other sequence transduction models. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cac49c-1d4d-4041-9644-a4812001eded",
   "metadata": {},
   "source": [
    "### Neural Sequence Transducer Model\n",
    "\n",
    "**Note**: Reading through the paper, in all other instances, the term transduction model is prefixed with either \"sequence\" or \"neural sequence\". This Distinction is important as I have found that a Neural Transducer is different than a Neural Sequence Transducer per it's author.\n",
    "\n",
    "Reading through the paper I found a passage which I believe helps define what a Neural Sequence Transduction Model is:\n",
    "\n",
    "> Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\n",
    "\n",
    "Looking at the sources I see:\n",
    "\n",
    "> \\[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly \n",
    "learning to align and translate. CoRR, abs/1409.0473, 2014\n",
    "> \n",
    "> \\[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, \n",
    "and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistica \r\n",
    "machine translation. CoRR, abs/1406.1078, 201\n",
    "> \n",
    "> \\[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\r\n",
    "networks. In Advances in Neural Information Processing Systems, pages 3104â€“3112, 2014\n",
    "\n",
    "And cross referencing these (specifically \\[[2](https://arxiv.org/abs/1409.0473)]) with Graves' paper on Neural Transducers I see the same sources being cited:\n",
    "\n",
    "> Graves, A. (2012). Sequence transduction with recurrent neural networks. In Proceedings of the\r\n",
    "29th International Conference on Machine Learning (ICML 2012).\n",
    ".4.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16cce20-40cb-4fab-a8c7-b63aba185c69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3776ff-89e7-401b-9431-cc6c5d16e1c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49fbe433-3c7e-4b83-b842-55934014684b",
   "metadata": {},
   "source": [
    "Each token in the input sequence is converted into a vector of dimension $d_{model}$. This means that if the input sequence is $N$ tokens long, the resulting embedding matrix would be  $N \\times d_{model}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e8e361-055c-4ea7-ba10-99ade39b05e2",
   "metadata": {},
   "source": [
    "I had not heard this term \"sequence transduction model\" before. I [googled](https://machinelearningmastery.com/transduction-in-machine-learning/) to find out what this term means and I belive it's meaning is derived from that of transductive reasoning or transductive logic.\n",
    "\n",
    "<center><img src='./images/logic_diagram.png' style='width:50%'></center>\n",
    "\n",
    "With transductive logic, we are able to look at examples (i.e. train on examples) and learn how to predict the outcomes without explicitly defining an a prioiri model of all the rules involved in the predictions. Instead, the rules are abstracted away an implicitly \"baked into\" the transductive process as the training process takes place.\n",
    "\n",
    "In this way, some neural networks are transducers; i.e. models that are able to infer a model which allows us to directly predict an output given an input without needed to evaluate an intermediary to affect a rule set.r."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b0b3f8-fdc9-400c-bfc0-00b0cd4a9fac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1dd478ae-24f7-4857-a283-63d71e9477e2",
   "metadata": {},
   "source": [
    "I was able to find a citation for the original [publication](https://arxiv.org/abs/1511.04868) of the transducer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02980274-1e98-48e8-bc3a-53c6843de687",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce84c932-e68b-4b6d-a5db-fd8726dafcc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d530f0d-97f4-4faf-8f29-e4e0d11680f6",
   "metadata": {},
   "source": [
    "I was curious as to what specific algorithm is used to learn the embeddings. Accodring to a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c17b55-13bf-48b4-b8a9-ad8ad3ab5f21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03b6f86c-b99d-438b-959d-d95dc403717a",
   "metadata": {},
   "source": [
    "## Positional Encoding\n",
    "### Purpose\n",
    "The purpose of the positional encoding is to record the location of the tokens in the input sequence relative to each other as well as their absolute position. The positional information of symbols contains or implies important syntactic information that is relevant when training the model and making predictions.\n",
    "\n",
    "As the paper notes, there are many choices for methodology of assigning positional encodings. \n",
    "\n",
    "After googling I found this [article](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/) explaining that there are four criteria for selecting a good positional encoding algorithm:\n",
    "1. Each position should be represented by a unique encoding\n",
    "2. Distance between positions should be consistent\n",
    "3. The algorithm should be deterministic\n",
    "4. The range (output of the function) should be bounded\n",
    "\n",
    "The importance of the last criteria was not initially intuitive. I didnt understant why, for example, simply increment the tokens in the input sequence starting with 0. Reading on, The paper suggests this will likely prevent the transformer model from generalizing. If we try to make predictions for sequences which are longer or of different length than the training set, the model may not perform well as it has been trained to only consider a specific ranfe of values.\n",
    "\n",
    "Conveniently, the method chosen for the transformer model meets these criteria. Additionally, the authors offer the following reasons for why it chose the methods it did:\n",
    "- They needed a way to combine the positional information with the information contained in the word embeddings. Their solution was to sum calculate the pair-wise sum. Thus they needed an algorithm which would result in the exact same dimensionality (i.e. a matrix with the same dimensions).\n",
    "- They wanted the algorithm to be simple so that the attention mechanism in the model could easily learn it. The proposed method is expressed as a simple recursion of a linear function which they hypothesisze would be easy for the model to learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351cec16-4c03-42c4-9391-17623188159a",
   "metadata": {},
   "source": [
    "### Algorithm\n",
    "\n",
    "The transformer accepts input sequences of length $N$. From these inputs it generates the coresponding Input Embeddings of dimension $N \\times d_{model}$, where each token has a coresponding embedding represented by a $d_{model}$ dimension vector. The transformer then calculates the Positional Encodings $PE$ coresponding to each Embedding. Each Position Encoding, for each token, is also represented by a $d_{model}$ dimension vector.\n",
    "\n",
    "<center><img src='./images/position_encoding_simple.png' style=\"width:30%\"></center>\n",
    "The Positional Encoding $PE$ for the token at position $pos$ and dimension $i$ can be calculated as:\n",
    "\n",
    "$$ PE_{(pos,2i)} = sin(pos/10000^{2i/d_{model}}) $$\n",
    "\n",
    "$$ PE_{(pos,2i+1)} = cos(pos/10000^{2i/d_{model}}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d314a82-bc8e-4879-8a89-0d58a53bb386",
   "metadata": {},
   "source": [
    "We can simplify this equation by introducing $w_i=\\frac{1}{10000^{2i/d_{model}}}$ such that:\n",
    "\n",
    "$$ PE_{(pos,2i)} = sin(pos*w_i) $$\n",
    "\n",
    "$$ PE_{(pos,2i+1)} = cos(pos*w_i) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def860b6-1048-49ae-82d9-ddad19bea7e8",
   "metadata": {},
   "source": [
    "This will result in a positional encoding vector for a given token at position $pos$ such that::\n",
    "\n",
    "$$ \\vec{PE}_{pos} = \\begin{bmatrix}\n",
    "sin(pos*w_i) \\\\\n",
    "cos(pos*w_i) \\\\\n",
    "\\vdots \\\\\n",
    "sin(pos*w_{d/2}) \\\\\n",
    "cos(pos*w_{d/2}) \\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Note: we can use $d/2$ rather than $d_{model}$ because we are calculating two values for ach element $i$ in $d_{model}$ so it would be redundant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60923a60-b555-4c86-bb4d-7ce925722f65",
   "metadata": {},
   "source": [
    "#### Intuition\n",
    "\n",
    "At this point I found myself wondering why we are using this complicated structure. It may satisfy the criteria mentioned earlier, but how can this possibly tell us anythign meaningful about the position of a token in a sequence?\n",
    "\n",
    "I found a very helpful explanation in this [article](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9eeaa0-e6fe-4bec-b4bc-39f81cc8c6d4",
   "metadata": {},
   "source": [
    "We can see an example of the values of the sinusoidal functions below. Each token in a sequence with a max length of 50 has a coresponding value for each dimension of the PE space. In the graph below, each row represents the individual PE vector associated with a particular token position. We can see that this algorithm produces a \"criss-cross\" patten which initially is quite similar to that of a grid or matrix.\n",
    "\n",
    "<center><img src='./images/position_encoding_128_bit_example.png' style='width: 75%'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316125fb-6b80-41ce-b561-e633348aa4aa",
   "metadata": {},
   "source": [
    "Looking carefullt at this grid we see that things begin to warp, as you go deeper in the x-axis, you notice that the horizontal y-axis clustering of values being to stretch and drift. It appears as if each depth has it's own rate of change relative to the positions. And this is the key observation which helps explain the method behind the madness.\n",
    "\n",
    "What we are doing here creates a pattern that is very similar to the one created by numbers represented in binary code. Recall that a binary code, or binary number, consists of a series of bits. Ultimately, the number represented by the bit string is calculated by adding together powers of two; the power or exponent being equal to the bit's position in the string. Bit strings are read left to right so, with the least significant bit (ie. the bit coresponding to $2^0$ being the right most bit. For example, whtih 4-bit string, Looking at the number 3 for example, we would have: $0 \\times 2^3 + 0 \\times 2^2 + 1 \\times 2^1 + 1 \\times 2^0$. Extrapolating this further we would arrive at the following table:\n",
    "\n",
    "<center><img src='./images/binary_table.png' style='width:30%'></center>\n",
    "\n",
    "Notice how the least significant bit is changing the fastest while the more significant bits change more slowly. This pattern is very similar to the pattern that is shown by the sinusoidal functions. All we need to do is reverse the order of the bits and the similarity between the patterns emerges:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6a1c4c-57e8-466b-ae5b-d0bb63a1a307",
   "metadata": {},
   "source": [
    "<center><img src='./images/position_encoding_128_bit_example_compared.png' style='width: 100%'></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b23159-f8b3-45c1-9fef-e879c25b8e06",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300b3985-2581-4088-aa07-50a9286f2d4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597dc5d9-ec9d-4072-a6c1-37ec46700718",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d09a92e-9a66-47b3-b59c-1488a960c6e2",
   "metadata": {},
   "source": [
    "The original paper did not have much information about the calculations involved in deriving the positional encoding. I googled and found this [article](https://towardsdatascience.com/transformers-explained-visually-part-2-how-it-works-step-by-step-b49fa4a64f34) which did offer some guidance.\n",
    "\n",
    "It states that the Position Encoding is computed independently of the input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adb4ee2-4a86-464d-aa7a-65eda8f4983b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811dafca-74f8-4dfa-ba4e-154f8dc3a0a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d927862-45ae-4fbf-b638-989cc599a09b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2d95052-0b87-41e7-bdcb-a522256902f6",
   "metadata": {},
   "source": [
    "## Positional Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fac5488-4f2d-4c4d-ae72-bb37cdef1789",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295a80e5-8b60-4a76-9823-030f9782d46f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47a73cd6-732c-4676-a819-86400d5ac017",
   "metadata": {},
   "source": [
    "# Implimentations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d14497a-acc0-4ab0-830b-c4de0748dd30",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0603444b-5ca0-4fa8-a551-98823fad575b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da06e449-cc7a-4031-9501-6bad14965ad0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
