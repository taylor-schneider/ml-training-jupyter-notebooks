{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ce381fa-5c02-4c93-8fd8-9f94af6e6823",
   "metadata": {},
   "source": [
    "# Origin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ea4592-de0f-42c8-9b0b-2b100b9d089a",
   "metadata": {},
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c178ec3f-014c-474c-a557-18aaae4e2392",
   "metadata": {},
   "source": [
    "The transformer is an extension of the encoder-decoder architecture.\n",
    "\n",
    "For more information about its lineage and the course of events leading to the development of the Transformer Model see the complementarty article on [encoder-decoders](Encoder-Decoders.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4e737d-9c49-415a-b87b-401604ceaf1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2675fa-2c75-4937-97c0-7937d7d8d202",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e0456ca-e26d-4cde-88ce-5a411618b62c",
   "metadata": {},
   "source": [
    "Another good article here: https://medium.com/analytics-vidhya/understanding-attention-in-transformers-models-57bada0cce3e\n",
    "\n",
    "> The problem is that by the time the final context vector is computed, contextual information about the earlier states can be lost because the earlier computations do not have access to the later words. In addition, the fixed-sized context vector may not be large enough to retain all of the information.\n",
    "> \n",
    ">So how does the transformer solve these problems?\n",
    "> \n",
    "> The transformer’s encoders and decoders consume the entire sequence and process all words (embedding) in parallel. Because of this parallelization, training time is reduced. Second, this parallel processing means that the context can be computed from all the words together, resulting in a more complete context.\n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccdaa11-a4c5-4a63-a206-c14f5076c7dd",
   "metadata": {},
   "source": [
    "## Publication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf229a7-1403-4ffe-af3c-c317d9d7ad3a",
   "metadata": {},
   "source": [
    "The Transformer model was [published](https://arxiv.org/abs/1706.03762) in 2017 as a solution to address several problems in the machine translation space. Machine translation, as the name suggests, is the process of using a machine (an ml model) to translate or transform text from one language into another. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed855a0-2e18-4813-ac3c-e6b5d5ae25ff",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fbad24-9b75-461a-bc7f-1c981b38729c",
   "metadata": {},
   "source": [
    "In this next section we will review the overall architecture of the original transformer published in the 2017 [paper](https://arxiv.org/abs/1706.03762) titled \"Attention Is All You Need\". \n",
    "\n",
    "There is a bit of a learning curve, and there are a few prerequisite topics to cover before we deep dive into the architecture. I will cover the former first as best I can."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a400dc16-9e95-4370-b825-c8f120362eb7",
   "metadata": {},
   "source": [
    "## Foundational Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2effbacc-4c39-4b53-b587-a2e8ac2b09a5",
   "metadata": {},
   "source": [
    "At it's core, the transformer is an extension of the [Encoder-Decoders](./Encoder-Decoder.ipynb) architecture.\n",
    "\n",
    "<center><img src=\"./images/transformer_model_architecture_simple.png\" style=\"width:50%\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4275f1-848c-475d-8c10-ff4835513417",
   "metadata": {},
   "source": [
    "The transformer is trained so that the the probabilities of the desired outputs are maximized given the inputs.\n",
    "\n",
    "Example of inputs and outputs:\n",
    "https://discuss.pytorch.org/t/meaning-of-outputs-shifted-right/63124\n",
    "\n",
    "https://datascience.stackexchange.com/questions/88981/what-are-the-inputs-to-the-first-decoder-layer-in-a-transformer-model-during-the\n",
    "\n",
    "Shift to the right: https://towardsdatascience.com/attention-is-all-you-need-e498378552f9#:~:text=The%20output%20text%20shifted%20to,be%20predicted%20at%20position%20i."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4ae58a-e172-4196-a17e-19d751ad004c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "25b62ad1-10aa-4520-b0d4-625e005ee9dd",
   "metadata": {},
   "source": [
    "## Archictecture Deep Dive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd110e3-f67b-456e-bfee-6173309a9806",
   "metadata": {},
   "source": [
    "In the 2017 [paper](https://arxiv.org/abs/1706.03762) titled \"Attention Is All You Need\" The following architectural diagram is published:\n",
    "\n",
    "<center><img src=\"./images/transformer_model_architecture.png\" style=\"width:50%\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5832c7bb-8560-4099-8182-8364d48219f2",
   "metadata": {},
   "source": [
    "In my humble opinion, this diagram leaves a lot to be desired. I will try to expand and explain the various components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f186204-74e4-4a27-bd63-2919e4414f15",
   "metadata": {},
   "source": [
    "## Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7770a729-5c67-48f8-88cd-20ed928f9640",
   "metadata": {},
   "source": [
    "The paper explains that the \"inputs\" shows in the architectural diagream refers to a sequence of symbols. As this paper is written to address issues of machine translation we can assume that the symbols are linguistic symols aka. alphabetical characters or letters. These inputs are now, typically referred to as \"tokens\" from an \"input sequence\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb65ae5-2e44-4a70-a58d-b51c98e96641",
   "metadata": {},
   "source": [
    "## Input Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cb7ded-8008-456e-93a9-0489625ffeb1",
   "metadata": {},
   "source": [
    "The paper does not specify how the embeddings are calculated. Instead, it states that it uses learned embeddings similar to those used by other sequence transduction models.\n",
    "\n",
    "Given the lack of specificity, I am inferring that regardless of the nuances of specific implimentations, the underlying use and therefore structure of the embeddings are consistent and therefore interchangeable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a168da93-f130-4c4d-b40f-ec3869e5a84a",
   "metadata": {},
   "source": [
    "#### Neural Sequence Transducer Model\n",
    "\n",
    "**Note**: Reading through the paper, in all other instances, the term transduction model is prefixed with either \"sequence\" or \"neural sequence\". This Distinction is important as I have found that a Neural Transducer is different than a Neural Sequence Transducer per it's author.\n",
    "\n",
    "Reading through the paper I found a passage which I believe helps define what a Neural Sequence Transduction Model is:\n",
    "\n",
    "> Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\n",
    "\n",
    "Looking at the sources I see:\n",
    "\n",
    "> \\[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly \n",
    "learning to align and translate. CoRR, abs/1409.0473, 2014\n",
    "> \n",
    "> \\[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, \n",
    "and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistica \n",
    "machine translation. CoRR, abs/1406.1078, 201\n",
    "> \n",
    "> \\[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\n",
    "networks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014\n",
    "\n",
    "And cross referencing these (specifically \\[[2](https://arxiv.org/abs/1409.0473)]) with Graves' paper on Neural Transducers I see the same sources being cited:\n",
    "\n",
    "> Graves, A. (2012). Sequence transduction with recurrent neural networks. In Proceedings of the\n",
    "29th International Conference on Machine Learning (ICML 2012).\n",
    ".4.\n",
    "\n",
    "\n",
    "I believe this is the original publication of the Neural Sequence Tranducer Model and I expore it in this [notebook](Neural%20Sequence%20Transducers.piynb).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac790fd-2c36-475b-b6e1-e1c08c6b5826",
   "metadata": {},
   "source": [
    "### Implimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b316a95-032a-473b-b3d8-17534d01a7a4",
   "metadata": {},
   "source": [
    "Despite a particular method not being specified, I started googling to understand how the word embeddings can be generated and/or retrieved.\n",
    "\n",
    "I [found](https://discuss.huggingface.co/t/generate-raw-word-embeddings-using-transformer-models-like-bert-for-downstream-process/2958/2) that language models typically use a subword tokenizer first which chops words into smaller pieces. That means that you do not necessarily get one embedding for every word in a sentence, but probably more than one, namely one for all its subword components.\n",
    "\n",
    "One then calculates a co-occurrence matrix between all the tokens in the corpus. That matrix then has its dimensionality reduced leading us with a set of principle axeses each acting as a dimension of the embedding vector. This is described [here](https://medium.com/geekculture/what-are-word-embeddings-6f6f677b13ce)\n",
    "\n",
    "After this, the relevant subword embeddings are averaged to get a more precise and single vector representation of our precise word. [Some](https://discuss.huggingface.co/t/get-word-embeddings-from-transformer-model/6929/2) refer to this as \"mean pooling\".\n",
    "\n",
    "We can see an example of the embeddings being used by the BERT model [here](https://discuss.huggingface.co/t/generate-raw-word-embeddings-using-transformer-models-like-bert-for-downstream-process/2958/2).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574e6777-27ec-4183-bb05-a6fc4c5a832a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d526a6-ae49-4c62-a82c-689ab973e7b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3782baee-336e-4add-a3d9-20e6a8bd410d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a1bd00-28ac-423e-8e0b-aa97df59741f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3776ff-89e7-401b-9431-cc6c5d16e1c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49fbe433-3c7e-4b83-b842-55934014684b",
   "metadata": {},
   "source": [
    "Each token in the input sequence is converted into a vector of dimension $d_{model}$. This means that if the input sequence is $N$ tokens long, the resulting embedding matrix would be  $N \\times d_{model}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e8e361-055c-4ea7-ba10-99ade39b05e2",
   "metadata": {},
   "source": [
    "I had not heard this term \"sequence transduction model\" before. I [googled](https://machinelearningmastery.com/transduction-in-machine-learning/) to find out what this term means and I belive it's meaning is derived from that of transductive reasoning or transductive logic.\n",
    "\n",
    "<center><img src='./images/logic_diagram.png' style='width:50%'></center>\n",
    "\n",
    "With transductive logic, we are able to look at examples (i.e. train on examples) and learn how to predict the outcomes without explicitly defining an a prioiri model of all the rules involved in the predictions. Instead, the rules are abstracted away an implicitly \"baked into\" the transductive process as the training process takes place.\n",
    "\n",
    "In this way, some neural networks are transducers; i.e. models that are able to infer a model which allows us to directly predict an output given an input without needed to evaluate an intermediary to affect a rule set.r."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b0b3f8-fdc9-400c-bfc0-00b0cd4a9fac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1dd478ae-24f7-4857-a283-63d71e9477e2",
   "metadata": {},
   "source": [
    "I was able to find a citation for the original [publication](https://arxiv.org/abs/1511.04868) of the transducer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02980274-1e98-48e8-bc3a-53c6843de687",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce84c932-e68b-4b6d-a5db-fd8726dafcc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d530f0d-97f4-4faf-8f29-e4e0d11680f6",
   "metadata": {},
   "source": [
    "I was curious as to what specific algorithm is used to learn the embeddings. Accodring to a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c17b55-13bf-48b4-b8a9-ad8ad3ab5f21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03b6f86c-b99d-438b-959d-d95dc403717a",
   "metadata": {},
   "source": [
    "## Positional Encoding\n",
    "### Purpose\n",
    "The purpose of the positional encoding is to record the location of the tokens in the input sequence relative to each other as well as their absolute position. The positional information of symbols contains or implies important syntactic information that is relevant when training the model and making predictions.\n",
    "\n",
    "As the paper notes, there are many choices for methodology of assigning positional encodings. \n",
    "\n",
    "After googling I found this [article](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/) explaining that there are four criteria for selecting a good positional encoding algorithm:\n",
    "1. Each position should be represented by a unique encoding\n",
    "2. Distance between positions should be consistent\n",
    "3. The algorithm should be deterministic\n",
    "4. The range (output of the function) should be bounded\n",
    "\n",
    "The importance of the last criteria was not initially intuitive. I didnt understant why, for example, simply increment the tokens in the input sequence starting with 0. Reading on, The paper suggests this will likely prevent the transformer model from generalizing. If we try to make predictions for sequences which are longer or of different length than the training set, the model may not perform well as it has been trained to only consider a specific ranfe of values.\n",
    "\n",
    "Conveniently, the method chosen for the transformer model meets these criteria. Additionally, the authors offer the following reasons for why it chose the methods it did:\n",
    "- They needed a way to combine the positional information with the information contained in the word embeddings. Their solution was to sum calculate the pair-wise sum. Thus they needed an algorithm which would result in the exact same dimensionality (i.e. a matrix with the same dimensions).\n",
    "- They wanted the algorithm to be simple so that the attention mechanism in the model could easily learn it. The proposed method is expressed as a simple recursion of a linear function which they hypothesisze would be easy for the model to learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351cec16-4c03-42c4-9391-17623188159a",
   "metadata": {},
   "source": [
    "### Algorithm\n",
    "\n",
    "The transformer accepts input sequences of length $N$. From these inputs it generates the coresponding Input Embeddings of dimension $N \\times d_{model}$, where each token has a coresponding embedding represented by a $d_{model}$ dimension vector. The transformer then calculates the Positional Encodings $PE$ coresponding to each Embedding. Each Position Encoding, for each token, is also represented by a $d_{model}$ dimension vector.\n",
    "\n",
    "<center><img src='./images/position_encoding_simple.png' style=\"width:30%\"></center>\n",
    "The Positional Encoding $PE$ for the token at position $pos$ and dimension $i$ can be calculated as:\n",
    "\n",
    "$$ PE_{(pos,2i)} = sin(pos/10000^{2i/d_{model}}) $$\n",
    "\n",
    "$$ PE_{(pos,2i+1)} = cos(pos/10000^{2i/d_{model}}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d314a82-bc8e-4879-8a89-0d58a53bb386",
   "metadata": {},
   "source": [
    "We can simplify this equation by introducing $w_i=\\frac{1}{10000^{2i/d_{model}}}$ such that:\n",
    "\n",
    "$$ PE_{(pos,2i)} = sin(pos*w_i) $$\n",
    "\n",
    "$$ PE_{(pos,2i+1)} = cos(pos*w_i) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def860b6-1048-49ae-82d9-ddad19bea7e8",
   "metadata": {},
   "source": [
    "This will result in a positional encoding vector for a given token at position $pos$ such that::\n",
    "\n",
    "$$ \\vec{PE}_{pos} = \\begin{bmatrix}\n",
    "sin(pos*w_i) \\\\\n",
    "cos(pos*w_i) \\\\\n",
    "\\vdots \\\\\n",
    "sin(pos*w_{d/2}) \\\\\n",
    "cos(pos*w_{d/2}) \\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Note: we can use $d/2$ rather than $d_{model}$ because we are calculating two values for ach element $i$ in $d_{model}$ so it would be redundant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60923a60-b555-4c86-bb4d-7ce925722f65",
   "metadata": {},
   "source": [
    "#### Intuition\n",
    "\n",
    "At this point I found myself wondering why we are using this complicated structure. It may satisfy the criteria mentioned earlier, but how can this possibly tell us anythign meaningful about the position of a token in a sequence?\n",
    "\n",
    "I found a very helpful explanation in this [article](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9eeaa0-e6fe-4bec-b4bc-39f81cc8c6d4",
   "metadata": {},
   "source": [
    "We can see an example of the values of the sinusoidal functions below. Each token in a sequence with a max length of 50 has a coresponding value for each dimension of the PE space. In the graph below, each row represents the individual PE vector associated with a particular token position. We can see that this algorithm produces a \"criss-cross\" patten which initially is quite similar to that of a grid or matrix.\n",
    "\n",
    "<center><img src='./images/position_encoding_128_bit_example.png' style='width: 75%'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316125fb-6b80-41ce-b561-e633348aa4aa",
   "metadata": {},
   "source": [
    "Looking carefullt at this grid we see that things begin to warp, as you go deeper in the x-axis, you notice that the horizontal y-axis clustering of values being to stretch and drift. It appears as if each depth has it's own rate of change relative to the positions. And this is the key observation which helps explain the method behind the madness.\n",
    "\n",
    "What we are doing here creates a pattern that is very similar to the one created by numbers represented in binary code. Recall that a binary code, or binary number, consists of a series of bits. Ultimately, the number represented by the bit string is calculated by adding together powers of two; the power or exponent being equal to the bit's position in the string. Bit strings are read left to right so, with the least significant bit (ie. the bit coresponding to $2^0$ being the right most bit. For example, whtih 4-bit string, Looking at the number 3 for example, we would have: $0 \\times 2^3 + 0 \\times 2^2 + 1 \\times 2^1 + 1 \\times 2^0$. Extrapolating this further we would arrive at the following table:\n",
    "\n",
    "<center><img src='./images/binary_table.png' style='width:30%'></center>\n",
    "\n",
    "Notice how the least significant bit is changing the fastest while the more significant bits change more slowly. This pattern is very similar to the pattern that is shown by the sinusoidal functions. All we need to do is reverse the order of the bits and the similarity between the patterns emerges:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6a1c4c-57e8-466b-ae5b-d0bb63a1a307",
   "metadata": {},
   "source": [
    "<center><img src='./images/position_encoding_128_bit_example_compared.png' style='width: 100%'></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b23159-f8b3-45c1-9fef-e879c25b8e06",
   "metadata": {},
   "source": [
    "Now at this point I was wondering... how can this technique possibly work? We are basically decomposing the positional information (which is normally unidimensional) across multiple dimensions. But how can adding various bits of positional information to the various dimensions of the embedding vector actually preserve the positional information for the calculation. The hand waivy answer is that matrix algebra is weird and it just does. Long story short there is a mathematical property that allows this to work. This is defined in greater detail [here](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300b3985-2581-4088-aa07-50a9286f2d4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597dc5d9-ec9d-4072-a6c1-37ec46700718",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adb4ee2-4a86-464d-aa7a-65eda8f4983b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811dafca-74f8-4dfa-ba4e-154f8dc3a0a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d927862-45ae-4fbf-b638-989cc599a09b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2d95052-0b87-41e7-bdcb-a522256902f6",
   "metadata": {},
   "source": [
    "## Positional Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4cc422-07ce-40bb-ae68-24ca49b70a4a",
   "metadata": {},
   "source": [
    "This [article](https://towardsdatascience.com/transformers-explained-visually-part-2-how-it-works-step-by-step-b49fa4a64f34) was helpful in explaining how this is created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f41259-846e-48c3-a4e5-bccaf1f59315",
   "metadata": {},
   "source": [
    "\n",
    "The Positional Encoding is then added to the Word Embedding to produce the Positional Embedding.\n",
    "\n",
    "<center><img src='./images/position_embedding.png' style='width:75%'></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb2dba3-6804-4a21-ac32-e31c20b3640f",
   "metadata": {},
   "source": [
    "This gives a sense of the 3D matrix dimensions in the Transformer.\n",
    "\n",
    "<center><img src='./images/position_embedding_3d.png' style='width:30%'></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80166132-d960-462a-b435-ddaa1c642e06",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8049cea-cf60-4626-8902-f1ae3210424d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f7d550-9ffe-41b2-819d-499a0abee34c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7fc881-c3e1-49c0-81a6-9fdb64c70d48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47a73cd6-732c-4676-a819-86400d5ac017",
   "metadata": {},
   "source": [
    "# Implimentations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d14497a-acc0-4ab0-830b-c4de0748dd30",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0603444b-5ca0-4fa8-a551-98823fad575b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da06e449-cc7a-4031-9501-6bad14965ad0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234a6730-e034-4b7c-ad4a-d38e79afb816",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0a4123-8dc9-44e9-ae94-efeca81c2d65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c9fc0331-92c3-411a-a3b4-2448f7274965",
   "metadata": {},
   "source": [
    "# Text summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0d2683-8141-4290-9f9a-6d15236cddcb",
   "metadata": {},
   "source": [
    "\n",
    "> How text summarization performed using Transformers?\n",
    ">\n",
    "> Text summarization using Transformers can be performed in two ways: extractive summarization and abstractive summarization.\n",
    ">\n",
    "> Extractive summarization: In this approach, the most important sentences or phrases from the original text are selected and combined to form a summary. This can be done using algorithms such as TextRank, which uses graph-based algorithms to rank sentences based on their relevance and importance. Transformers can be used to process the text, extract features, and perform sentence ranking.\n",
    ">\n",
    "> Abstractive summarization: In this approach, a new summary is generated by understanding the context of the original text and generating new phrases and sentences that summarize its content. This can be done using techniques such as encoder-decoder models, where the encoder processes the input text to extract its features and the decoder generates the summary. Transformers can be used as the encoder or decoder in this architecture.\n",
    ">\n",
    "> [source](https://medium.com/@lokaregns/text-summarization-with-hugging-face-transformers-a-beginners-guide-9e6c319bb5ed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e05bc70-9e2b-4bd1-a10a-a5be3283a28d",
   "metadata": {},
   "source": [
    "## Abstractive Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495827e8-8ab5-4ad4-8586-01f44274644f",
   "metadata": {},
   "source": [
    "I googled to find more information about how this actually works.\n",
    "\n",
    "I found the following [approach](https://www.neurond.com/blog/automatic-text-summarization-system-using-transformers) to abstractive summarization:\n",
    "\n",
    "> It uses the BART transformer and PEGASUS. The former helps pre-train a model combining Bidirectional and Auto-Regressive Transformers while the latter, PEGASUS, is a State-of-the-Art model for abstractive text summarization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58705e56-c1ed-4417-a670-9e4409d02914",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed09ee6-86c0-4046-b791-fb68c356fd98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5019e93-c23c-416f-9b23-6b7673232277",
   "metadata": {},
   "source": [
    "# History\n",
    "\n",
    "Besided the timeline in word embeddings, there is a few extra milestones here: https://huggingface.co/learn/nlp-course/chapter1/4?fw=pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb0f438-9ec5-4749-bb99-855aaeb91595",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c294195-9503-4f14-9c8d-63a7b9c4b45d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
