{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67f61370-fd7b-4ab5-8caa-1d83110fbb91",
   "metadata": {},
   "source": [
    "# Transfer Learning & Pre-Trained Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665f4408-53fd-4c67-b2c9-07a9ddf88a1d",
   "metadata": {},
   "source": [
    "The term transfer learning refers to a process of conducting knowledge transfers, or transfering knowledge, between machine learning models. As the \"knowledge\" exists in the trained model weights, the transfer of knowledge involves moving weights from a trained model to an untrained model. The untrained model can then be trained using the transfered weights as initialization values rather than random values. The hope is that these transferred weights are \"closer\" to the optimal weights for this downstream model and thus will make the training process quicker and more accurate.\n",
    "\n",
    "In the situation above, the original model is typically referred to as the \"base model\", the \"pre-trained\" model, or the \"foundational model\". And the process of creating these model weights for downstream consumption is referred to as \"pre-training\" the model. This is a bit confusing because when we say \"the pre-trained model\" we are usually referring to the weights and not the model itself. Some, call a trained model a model instance, so in this context the terminology would be appropriate. The process of training a model using pre-trained weights is referred to as the \"fine tuning\" a model. The resulting model (weights) is referred to as a \"fine-tuned\" model.\n",
    "\n",
    "Note: the data sets and the objectives of the pre-trained and fine-tuned model do not need to be the same. For example we can fine-tune an object detection bodel based on an image classifier. The general intuition is that the more similarities between the pre-trained model and the fine-tuned will corespond to higher accuracy scores. The wider the gap, the more likely you will be training from scratch and the benefits will be marginal if not minimal.\r\n",
    "\r\n",
    "Examples of successful large-scale pre-trained language models ar[e Bidirectional Encoder Representations from Transformers (BER](./BERT.ipynb)T) and the Generative Pre-trained Transformer (GPT-n) series.\r\n",
    "\r\n",
    "https://deci.ai/deep-learning-glossary/pre-trained-model/#:~:text=Share,building%20a%20model%20from%20scratch. https://stats.stackexchange.com/questions/193082/what-is-pre-training-a-neural-network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebc6b96-9ffb-490b-8c6f-43760ff1ffaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a482763-76e1-475a-8c3e-0a573a235c2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
