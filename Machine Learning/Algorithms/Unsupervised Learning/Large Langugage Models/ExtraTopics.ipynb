{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b799220-8d69-4d81-8568-74b11a4de53e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8293725-6826-4ba8-97ae-6588ae62a8c2",
   "metadata": {},
   "source": [
    "# Additional Terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e790b4-b25f-4219-a366-329b11347301",
   "metadata": {},
   "source": [
    "\n",
    "- word embeddings\n",
    "- BLUE score\n",
    "- GLUE benchmark\n",
    "- MultiNLI accuracy\n",
    "- SQuAD \n",
    "- ablation analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7d095d-5c59-4f92-8548-a17ec4e29834",
   "metadata": {},
   "source": [
    "- how is context db used\n",
    "- how is model learning to respond? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da54f18-dcc3-415e-bb21-f1f3f973c4c1",
   "metadata": {},
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67921ca1-023e-4bba-89c6-8262d706d9a0",
   "metadata": {},
   "source": [
    "An autoencoder is a special case of the encoder-decoder architecture in which the input is also provided as the output. In this case the encoder creates an intermediary representation of the data that the decoder then learns how to translate back into the original input space. \n",
    "\n",
    "The classical use case of the autoencoder is dimensionality reduction, compression, and noise reduction. The key expectation of a performant autoencoder is that is it able to represent the input sequence in a more compact format.\n",
    "\n",
    "More recently, the intermediary representation of data has been thought of as a way to identify or extract features. Assuming that the compressed data holds a \"more pure\" representation of useful information, from this we can reverse engineer which features are relevant and irrelevant. I.e. the original features that were reduced can be removed from the feature set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e1662d-abf8-4919-a317-718156fa111f",
   "metadata": {},
   "source": [
    "# Sequence Generation & Sequence To Sequence\n",
    "\n",
    "The term sequence generation often occurs in the context of LLMs. This umbrella term applies to tasks related to the generation of text (ie. sequences of tokens) such as translation, summariazation, and prompt-response. \n",
    "\n",
    "<center>\n",
    "    <img src='./images/sequence_to_sequence_model_comparison.png' style='width:75%'>\n",
    "    <a href='https://www.analyticsvidhya.com/blog/2020/08/a-simple-introduction-to-sequence-to-sequence-models/'>Source</a>\n",
    "</center>\n",
    "\n",
    "\n",
    "\n",
    "With these tasks, the modeler is trying to map sequences in one domain to sequences in another domain. As such the term sequence to sequence is also used interchangably."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ce77e3-ee60-4456-9e8e-a5b0f1472051",
   "metadata": {},
   "source": [
    "**Note**: Sequence to Sequence is often abreviated as seq2seq as noted in the [encoder-decoder notebook](Encoder-Decoders.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1676f9-1bd1-4c6a-85aa-a66ae5437a61",
   "metadata": {},
   "source": [
    "# Alignment\n",
    "\n",
    "I did some googling and found a few articles ([stackoverflow answer](https://stats.stackexchange.com/questions/272012/what-does-alignment-between-input-and-output-mean-for-recurrent-neural-network), [Stackoverlow answer](https://ai.stackexchange.com/questions/26184/what-is-the-purpose-of-alignment-in-the-self-attention-mechanism-of-transforme)) which helped me understand what alignment means and what is being aligned. \n",
    "\n",
    "In short, I found that the alignment is an abstraction for a problem which arises in the context of sequence to seuence mapping. Alignment is or describes the mapping of tokens in the input sequence to tokens in the output sequence.\n",
    "\n",
    "To add some intuition behind the choice of the word alignment, lets consider the translation problem space. We need to map one language to another. For example, consider the task of translating French to English. When translating the phrase \"It's raining\" to french. The equivalent phrase would be \"Il pleut\". We see our input has the same number of words as the output and they are in the same order. Thus the alignment of the tokens between sequences, or the mapping of input tokens to output tokens is trivial.\n",
    "\n",
    "```\n",
    "Il pleut\n",
    "(1)  (2)\n",
    " |    |\n",
    " |    |\n",
    "(1)  (2)\n",
    "It's raining\n",
    "```\n",
    "\n",
    "But what about \"It is raining\"? In this circomstance we have three inputs to two outputs; there is not a one to one mapping between the words.\n",
    "\n",
    "```\n",
    "Il pleut\n",
    "(1)  (2)\n",
    " |     \\\n",
    " |      \\\n",
    "(1) (2) (3)\n",
    "It  is  raining\n",
    "```\n",
    "\n",
    "Additionally, the order of words may not be the same between the two sequences. For example, consider this translation from Portugese to english:\n",
    "\n",
    "```\n",
    "Uma maçã grande e vermelha\n",
    "(1)   (2)  (3)  (4)   (5)\n",
    " |      \\ /   _______/\n",
    " |       X   /\n",
    " |      / \\ /\n",
    " |     /   X\n",
    " |    /   / \\\n",
    "(1) (3) (5)  (2)\n",
    " A  big red apple\n",
    "```\n",
    "\n",
    "We can see that not only is there a difference in length between the input and output sequence, but the order of the coresponding tokens is not the same.\n",
    "\n",
    "As we continue to explore the nuances of machine translation we start to understand that there may be additional complexities to the sequence mappings than just these. Additionally as we think more generically and start thinking about other seq2seq tasks such as text summarization or prompt/response we open the door for even more complexity with respect to the alignment task.\n",
    "\n",
    "So to summarize, alignment is both the process and the end result i.e. the representaion or the mapping itself. Depending on the problem space and model architecture, there will be different implimentations for achieving or producing alignment. In the various notebooks we will explore this in more detail.\n",
    "\n",
    "A more detailed look and comparison of alignment techniques and architectures can be found [here](https://arxiv.org/abs/2112.07806).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3353a5a-d399-4d37-b9c5-c07e8225189e",
   "metadata": {},
   "source": [
    "# Annotation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9481aa9e-fc69-45c3-83d0-b278bba01040",
   "metadata": {},
   "source": [
    "In September 2014, Bahdanau et. al. [published ](https://arxiv.org/abs/1409.0473) *Neural Machine Translation by Jointly Learning to Align and Translate*. In this paper they propose an enhancement(s) to the prior encoder-decoder based implimentations of attention.\n",
    "\n",
    "As we will see, in their paper, they define the annotations as a component of the attention mechanism for the alignment process. Ultimately the prediction of the next target word $y_i$, at time $i$, is based on a conditional probabilities attached to the set target words given some input. The conditional probability is defined to be dependant on the previous word $y_{i-1}$, the hidden states of the decoder $s_i$, and the context vector $c_i$.\n",
    "\n",
    "The context vector is where we see the connection with the concept of anotation. The context vector is defined as the weighted sum of each annotation $h_i$. Each annotation refers to the concatenation of the two coresponding neurons in the hidden forward and backward layers of the encoder.\n",
    "\n",
    "Note: The encoder is defined as a bidirectional RNN which they claim is similar to an LSTM. This RNN has two hidden layers of equal size, each of which coresponds to one of the directions of the bidirectional RNN. One layer contains the forward hidden states and one contains the backward hidden states."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4117f136-12c1-4952-a342-f55ffe37685d",
   "metadata": {},
   "source": [
    "As such, the neurons $(h_1,...,h_{T_x})$ in the hidden hidden layers can be paired based on their upstream input. These paired neurons are concatenated, and this concatenation referred to as the annotation $h_j$ which coresponds to input $x_j$.\n",
    "\n",
    "The mathematical notation was a bit confusing but I belive the annotation would be defined as a tuple or tensor given the equation\n",
    "\n",
    "$$ h_j= \\big[ \\overrightarrow{h_j^T} ;  \\overleftarrow{h_j^T} \\big]^T$$\n",
    "\n",
    "It's a bit confusing, but in the diagram below we can see the encoder's hidden states (annotations) being connected to the hidden states of the decoder through the context vector and ultimately the predicted target word.\n",
    "\n",
    "<center><img src='./images/annotation_diagram.png' style=\"width:25%\"></center>\n",
    "\n",
    ">  the annotation $h_j$ contains the summaries of both the preceding words and the following words. Due to the tendency of RNNs to better represent recent inputs, the annotation $h_j$ will be focused on the words around $x_j$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6652900a-be39-45e2-b4cd-59cb1f41eba9",
   "metadata": {},
   "source": [
    "In the diagram above, the context vector is represented as $\\bigoplus$. Mathematically it is expressed as the weighted sum of the annoatations. As the annotations have two dimensions (one fore each direction), I assume so too does the context vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f8e8ec-b41d-4505-a6da-ad67a71d194b",
   "metadata": {},
   "source": [
    "The weights of each annotation are calculated by a feed forward neural network which is jointly trained with the other components of the model. This model is referred to as the alignment model\n",
    "\n",
    ">  which scores how well the inputs around position $j$ and the output at position $i$ match. The score is based on the RNN hidden state si−1 (just before emitting $y_i$, Eq. (4)) and the $j-th$ annotation $h_j$ of the input sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3371c26a-7ed1-4c97-9c9a-e4853cd8ee92",
   "metadata": {},
   "source": [
    "**Note**: This term alignment shows up elsewhere in the context of NLP. Traditionally the term refers to how tokens in the input sequence are matched or \"aligned\" with tokens in the output seuence. Abstractly I think that intuition applies here; the alignment model calculates the weights which directly impact the probabilities attached to the output tokens. Thus the alignment between the input and output sequence is controlled by the alignment model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23303ed2-9ad5-4b68-977e-399d6605f7d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082afaa7-2b2b-435b-90ac-4c75ee756ea7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
