{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b799220-8d69-4d81-8568-74b11a4de53e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8293725-6826-4ba8-97ae-6588ae62a8c2",
   "metadata": {},
   "source": [
    "# Additional Terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e790b4-b25f-4219-a366-329b11347301",
   "metadata": {},
   "source": [
    "\n",
    "- word embeddings\n",
    "- BLUE score\n",
    "- GLUE benchmark\n",
    "- MultiNLI accuracy\n",
    "- SQuAD \n",
    "- ablation analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7d095d-5c59-4f92-8548-a17ec4e29834",
   "metadata": {},
   "source": [
    "- how is context db used\n",
    "- how is model learning to respond? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da54f18-dcc3-415e-bb21-f1f3f973c4c1",
   "metadata": {},
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67921ca1-023e-4bba-89c6-8262d706d9a0",
   "metadata": {},
   "source": [
    "An autoencoder is a special case of the encoder-decoder architecture in which the input is also provided as the output. In this case the encoder creates an intermediary representation of the data that the decoder then learns how to translate back into the original input space. \n",
    "\n",
    "The classical use case of the autoencoder is dimensionality reduction, compression, and noise reduction. The key expectation of a performant autoencoder is that is it able to represent the input sequence in a more compact format.\n",
    "\n",
    "More recently, the intermediary representation of data has been thought of as a way to identify or extract features. Assuming that the compressed data holds a \"more pure\" representation of useful information, from this we can reverse engineer which features are relevant and irrelevant. I.e. the original features that were reduced can be removed from the feature set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e1662d-abf8-4919-a317-718156fa111f",
   "metadata": {},
   "source": [
    "# Sequence Generation & Sequence To Sequence\n",
    "\n",
    "The term sequence generation often occurs in the context of LLMs. This umbrella term applies to tasks related to the generation of text (ie. sequences of tokens) such as translation, summariazation, and prompt-response. \n",
    "\n",
    "<center>\n",
    "    <img src='./images/sequence_to_sequence_model_comparison.png' style='width:75%'>\n",
    "    <a href='https://www.analyticsvidhya.com/blog/2020/08/a-simple-introduction-to-sequence-to-sequence-models/'>Source</a>\n",
    "</center>\n",
    "\n",
    "\n",
    "\n",
    "With these tasks, the modeler is trying to map sequences in one domain to sequences in another domain. As such the term sequence to sequence is also used interchangably."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ce77e3-ee60-4456-9e8e-a5b0f1472051",
   "metadata": {},
   "source": [
    "**Note**: Sequence to Sequence is often abreviated as seq2seq as noted in the [encoder-decoder notebook](Encoder-Decoders.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1676f9-1bd1-4c6a-85aa-a66ae5437a61",
   "metadata": {},
   "source": [
    "# Alignment\n",
    "\n",
    "[NST Notebook](Neural%20Sequence%20Transducers.ipynb#Alignment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3353a5a-d399-4d37-b9c5-c07e8225189e",
   "metadata": {},
   "source": [
    "# Annotation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9481aa9e-fc69-45c3-83d0-b278bba01040",
   "metadata": {},
   "source": [
    "In September 2014, Bahdanau et. al. [published ](https://arxiv.org/abs/1409.0473) *Neural Machine Translation by Jointly Learning to Align and Translate*. In this paper they propose an enhancement(s) to the prior encoder-decoder based implimentations of attention.\n",
    "\n",
    "As we will see, in their paper, they define the annotations as a component of the attention mechanism for the alignment process. Ultimately the prediction of the next target word $y_i$, at time $i$, is based on a conditional probabilities attached to the set target words given some input. The conditional probability is defined to be dependant on the previous word $y_{i-1}$, the hidden states of the decoder $s_i$, and the context vector $c_i$.\n",
    "\n",
    "The context vector is where we see the connection with the concept of anotation. The context vector is defined as the weighted sum of each annotation $h_i$. Each annotation refers to the concatenation of the two coresponding neurons in the hidden forward and backward layers of the encoder.\n",
    "\n",
    "Note: The encoder is defined as a bidirectional RNN which they claim is similar to an LSTM. This RNN has two hidden layers of equal size, each of which coresponds to one of the directions of the bidirectional RNN. One layer contains the forward hidden states and one contains the backward hidden states."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4117f136-12c1-4952-a342-f55ffe37685d",
   "metadata": {},
   "source": [
    "As such, the neurons $(h_1,...,h_{T_x})$ in the hidden hidden layers can be paired based on their upstream input. These paired neurons are concatenated, and this concatenation referred to as the annotation $h_j$ which coresponds to input $x_j$.\n",
    "\n",
    "The mathematical notation was a bit confusing but I belive the annotation would be defined as a tuple or tensor given the equation\n",
    "\n",
    "$$ h_j= \\big[ \\overrightarrow{h_j^T} ;  \\overleftarrow{h_j^T} \\big]^T$$\n",
    "\n",
    "It's a bit confusing, but in the diagram below we can see the encoder's hidden states (annotations) being connected to the hidden states of the decoder through the context vector and ultimately the predicted target word.\n",
    "\n",
    "<center><img src='./images/annotation_diagram.png' style=\"width:25%\"></center>\n",
    "\n",
    ">  the annotation $h_j$ contains the summaries of both the preceding words and the following words. Due to the tendency of RNNs to better represent recent inputs, the annotation $h_j$ will be focused on the words around $x_j$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6652900a-be39-45e2-b4cd-59cb1f41eba9",
   "metadata": {},
   "source": [
    "In the diagram above, the context vector is represented as $\\bigoplus$. Mathematically it is expressed as the weighted sum of the annoatations. As the annotations have two dimensions (one fore each direction), I assume so too does the context vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f8e8ec-b41d-4505-a6da-ad67a71d194b",
   "metadata": {},
   "source": [
    "The weights of each annotation are calculated by a feed forward neural network which is jointly trained with the other components of the model. This model is referred to as the alignment model\n",
    "\n",
    ">  which scores how well the inputs around position $j$ and the output at position $i$ match. The score is based on the RNN hidden state siâˆ’1 (just before emitting $y_i$, Eq. (4)) and the $j-th$ annotation $h_j$ of the input sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3371c26a-7ed1-4c97-9c9a-e4853cd8ee92",
   "metadata": {},
   "source": [
    "**Note**: This term alignment shows up elsewhere in the context of NLP. Traditionally the term refers to how tokens in the input sequence are matched or \"aligned\" with tokens in the output seuence. Abstractly I think that intuition applies here; the alignment model calculates the weights which directly impact the probabilities attached to the output tokens. Thus the alignment between the input and output sequence is controlled by the alignment model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23303ed2-9ad5-4b68-977e-399d6605f7d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082afaa7-2b2b-435b-90ac-4c75ee756ea7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
