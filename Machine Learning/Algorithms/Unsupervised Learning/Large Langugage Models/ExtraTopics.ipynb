{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b799220-8d69-4d81-8568-74b11a4de53e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8293725-6826-4ba8-97ae-6588ae62a8c2",
   "metadata": {},
   "source": [
    "# Additional Terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e790b4-b25f-4219-a366-329b11347301",
   "metadata": {},
   "source": [
    "\n",
    "- word embeddings\n",
    "- BLUE score\n",
    "- GLUE benchmark\n",
    "- MultiNLI accuracy\n",
    "- SQuAD \n",
    "- ablation analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7d095d-5c59-4f92-8548-a17ec4e29834",
   "metadata": {},
   "source": [
    "- how is context db used\n",
    "- how is model learning to respond? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da54f18-dcc3-415e-bb21-f1f3f973c4c1",
   "metadata": {},
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67921ca1-023e-4bba-89c6-8262d706d9a0",
   "metadata": {},
   "source": [
    "An autoencoder is a special case of the encoder-decoder architecture in which the input is also provided as the output. In this case the encoder creates an intermediary representation of the data that the decoder then learns how to translate back into the original input space. \n",
    "\n",
    "The classical use case of the autoencoder is dimensionality reduction, compression, and noise reduction. The key expectation of a performant autoencoder is that is it able to represent the input sequence in a more compact format.\n",
    "\n",
    "More recently, the intermediary representation of data has been thought of as a way to identify or extract features. Assuming that the compressed data holds a \"more pure\" representation of useful information, from this we can reverse engineer which features are relevant and irrelevant. I.e. the original features that were reduced can be removed from the feature set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e1662d-abf8-4919-a317-718156fa111f",
   "metadata": {},
   "source": [
    "# Sequence Generation\n",
    "\n",
    "The term sequence generation often occurs in the context of LLMs. This umbrella term applies to tasks related to the generation of text (ie. sequences of tokens) such as translation and summariazation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c182c91-810d-493b-a785-2bed97c9a39f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
