{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73ac911f-faab-4c6d-aac8-034c348fe5fa",
   "metadata": {},
   "source": [
    "# Overview\n",
    "In this notebook we explore the basics of spark and get ready to put hands on keyboard."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d065ab5-81fe-4fbc-af87-450db7e05624",
   "metadata": {},
   "source": [
    "# Terminology\n",
    "\n",
    "- SparkContext - Main entry point for Spark functionality. A SparkContext represents the connection to a Spark cluster, and can be used to create RDDs, accumulators and broadcast variables on that cluster.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f414e9ab-94b6-471a-b7e4-6312f7284d1b",
   "metadata": {},
   "source": [
    "Spark provides a helpful [gloassary](https://spark.apache.org/docs/3.2.0/cluster-overview.html#glossary)\n",
    "\n",
    "Spark also provides a [list of configurations for the SparkContext](https://spark.apache.org/docs/latest/configuration.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1b96a1-f2eb-4e2f-a4d0-4acd817c4bc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0c50b64-f229-4521-88b0-9e5b3cc49696",
   "metadata": {},
   "source": [
    "# Object Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b70d555-4498-4e6b-9ec6-4e728675fc19",
   "metadata": {},
   "source": [
    "- Application - While never explicitly defined, a spark application is an executable (code file or snippet) which utlizes a spark cluster. Spark applications are submitted to a spark cluster using one of the two execution modes described below. As mentioned in the [README](README.md) Spark provides APIs in Java, Scala, R, and Python which allow users to define their applciaitons.\n",
    "\n",
    "- Deploy Mode (Execution Mode) - Distinguishes where the driver process runs. In \"cluster\" mode, the framework launches the driver inside of the cluster. In \"client\" mode, the submitter launches the driver outside of the cluster.\n",
    "\n",
    "- [Cache](https://spark.apache.org/docs/latest/quick-start.html#caching) - Apache spark provides a cluster-wide in-memory cache. This is very useful when data is accessed repeatedly, such as when querying a small “hot” dataset or when running an iterative algorithm like PageRank.\n",
    "\n",
    "\n",
    "- SparkSession\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54effd40-6176-43d2-8303-a445adad773b",
   "metadata": {},
   "source": [
    "# Process\n",
    "\n",
    "- Submitting an application\n",
    "\n",
    "https://blog.knoldus.com/cluster-vs-client-execution-modes-for-a-spark-application/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94ae9e6-040b-4285-8480-f97e302fc3ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81f688c6-83c3-4e22-adf8-9d4363f4f8c4",
   "metadata": {},
   "source": [
    "# Software Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2548d847-9b3b-419b-9c15-e60414f42f73",
   "metadata": {},
   "source": [
    "<center><img src=\"images/software-architecture.png\", width=\"400px\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef15191-37ac-4f65-bf08-e3fe3abfb9d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87d150a4-221a-4384-b50b-79cfb1b298ba",
   "metadata": {},
   "source": [
    "## Java and The JVM\n",
    "Spark runs on the JVM. To understand the spark architecture we need to understand this.\n",
    "\n",
    "Spark is written in java and scala. Java runs natively in the JVM while scala can be comiled into Java bytecode and run inside the JVM.\n",
    "\n",
    "The Java Virtual Machine (JVM) is an engine (sometimes called a virtual machine ... confusingly named) which provides the java runtime environment (JRE). In order to execute java code, it must be compiled into a jar file and executed inside the JVM. The Java Development Kit (JDK) allows the java code to access the JRE once inside the JVM.\n",
    "\n",
    "## Py4j\n",
    "Py4j is a python library which allows python to interact with spark. It is the backbone of the pyspark API.\n",
    "\n",
    "On the [homepage](https://www.py4j.org/index.html) Py4j is described as:\n",
    "\n",
    "> A Bridge between Python and Java\n",
    ">\n",
    "> Py4J enables Python programs running in a Python interpreter to dynamically access Java objects in a Java Virtual Machine. Methods are called as if the Java objects resided in the Python interpreter and Java collections can be accessed through standard Python collection methods. Py4J also enables Java programs to call back Python objects.\n",
    "\n",
    "Reading a bit deeper into the [documentation](https://www.py4j.org/getting_started.html) we see that the interop between python and java is provided by a **GatewayServer** instance. This gateway server allows Python programs to communicate with the JVM through a local network socket and send it instructions. This Gateway is referred to as the Java Gateway.\n",
    "\n",
    "This is very important. Recall that the Driver runs in the JVM. If we look at the pyspark source code on [github](https://github.com/apache/spark/blob/e91ef1929201d4e493bb451fef0fb1b45800adae/python/pyspark/java_gateway.py#L214) we can see that a Driver is created in the JVM and a python wrapper provided by py4j and the Java Gateway allows us to manipulate the driver.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a487fdd-be8b-479c-8b8b-c2ad1b2fd30c",
   "metadata": {},
   "source": [
    "## Spark Shell\n",
    "The Spark Shell is an analog of the traditional operating system shell (like BASH, CMD, or PowerShell). The Spark shell provides an interactive command line interface (CLI) through which a user can interact with the Spark API and thus a Spark Cluster (once properly configured). The Spark Shell's CLI allows users to type and execute ad-hoc lines of Scala, Python, or R code. Like any Shell, the Spark Shell follows the REPL (read-evaluate-print loop) pattern.\n",
    "\n",
    "An important point of note is that the Spark Shell acts as the Driver while a user incrementally defines their Application using the CLI (note: I am using the term Application very precisely. See Application defined alsewhere in this document.). We will see later how the Spark Shell is leveraged by the Spark Context.\n",
    "\n",
    "More on the Spark Shell can be found in the official [documentation](https://spark.apache.org/docs/latest/quick-start.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7decdae-1b05-4450-bf81-a3f40f064096",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "223721aa-d97f-4e68-a3ae-bb85d5326e81",
   "metadata": {},
   "source": [
    "## SparkContext\n",
    "\n",
    "\n",
    "\n",
    "Looking at the code. SparkContext object's constructor does the following:\n",
    "- Acceps as SparkConf object (and some other low level configurations)\n",
    "- Uses the spark-submit utility to launch the pyspark-shell program within the JVM\n",
    "- Determines what port the pysaprk-shell is listening on\n",
    "- Creates a Java Gateway using py4j configured to attach to the JVM running pyspark-shell\n",
    "It then creates a Driver\n",
    "\n",
    "\n",
    "Note: I have seen a lot of misinformation\n",
    "\n",
    "The SparkContext can only run on the driver. In fact, the code has checks to make sure we are not creating a spark context from the worker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b8d84a-fb10-4669-bd75-ee8af5634cc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2246bcf-5082-4ed0-8c89-11a7708ff1b8",
   "metadata": {},
   "source": [
    "## The Driver\n",
    "\n",
    "The Driver is a java process that runs in its own JVM. The Driver utilizes several components including the DAGScheduler, TaskScheduler, BackendScheduler and BlockManager to interpret and translate the user defined code in the Application into actual Spark Jobs which can be executed on the cluster. For example, python function calls become Transformations and Actions which are types of Spark Tasks. Once translated, the driver comes up with an execution plan and schedules the work with the Cluster Manager. Additionally, the Application may create data or cache data within the cluster. The Driver is also respobsible for keeping track of these resources. \n",
    "\n",
    "The Driver also hosts the Spark Web UI which allows admins to monitor the utlization of the cluster by the Application.\n",
    "\n",
    "The driver creates the SparkContext, connecting the user program to a given Spark Master\n",
    "\n",
    "### Deploy Mode\n",
    "Spark offers two Deploy Modes (Execution Modes) which configure how and where the Driver runs. \n",
    "\n",
    "In **client mode**, the driver is launched in the same process as the client that submits the application. If using a jupyter notebook, the notebook spins up a subprocess (which is why we previously install java on our machines).\n",
    "\n",
    "In **cluster mode** the driver runs somewhere on the cluster. Exactly where depends on the type of Cluster Manager. The spark-submit utility is typically used to sumbit the applciation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc482c2-2aed-4c44-93ce-a198162b3679",
   "metadata": {},
   "source": [
    "# System Architecture\n",
    "Recall that Spark is a distributed system and thus Spark applciations are distributed appliations. In order to understand how to use Spark we need to understand what it is and how it works. First we will focus on the system architecture before discussing the software that runs on top of the system.\n",
    "\n",
    "A Spark cluster is designed according to the master-slave pattern. A master node is responsible for organizing, provisioning resources on, and distributing work to, the worker nodes who perform the actual computations and return results. Spark has renamed the master/slaves as cluster manager and workers respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f7c72e-75ac-4fc6-b27e-f785c8743256",
   "metadata": {},
   "source": [
    "<center><img src=\"images/cluster-overview.png\", width=\"400px\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffb72dd-1d75-4ee5-9a89-3da445eea971",
   "metadata": {},
   "outputs": [],
   "source": [
    "The SparkContext object is the gateway\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e558cb9-1e16-459b-b5ba-898daf334190",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b5c5039-03d0-452b-9d57-8f67097bb96e",
   "metadata": {},
   "source": [
    "Spark supports the following cluster managers:\n",
    "\n",
    "- [Standalone](https://spark.apache.org/docs/latest/spark-standalone.html) – a simple cluster manager included with Spark that makes it easy to set up a cluster.\n",
    " - [Apache Mesos](https://spark.apache.org/docs/latest/running-on-mesos.html) – a general cluster manager that can also run Hadoop MapReduce and service applications. (Deprecated)\n",
    "- [Hadoop YARN](https://spark.apache.org/docs/latest/running-on-yarn.html) – the resource manager in Hadoop 2.\n",
    "- [Kubernetes](https://spark.apache.org/docs/latest/running-on-kubernetes.html) – an open-source system for automating deployment, scaling, and management of containerized applications.\n",
    "\n",
    "https://spark.apache.org/docs/latest/cluster-overview.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f7ee93-dea3-4c75-8a9a-3333c5c805e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
