{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73ac911f-faab-4c6d-aac8-034c348fe5fa",
   "metadata": {},
   "source": [
    "# Overview\n",
    "In this notebook we explore the basics of spark and get ready to put hands on keyboard."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d065ab5-81fe-4fbc-af87-450db7e05624",
   "metadata": {},
   "source": [
    "# Terminology\n",
    "\n",
    "- SparkContext - Main entry point for Spark functionality. A SparkContext represents the connection to a Spark cluster, and can be used to create RDDs, accumulators and broadcast variables on that cluster.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f414e9ab-94b6-471a-b7e4-6312f7284d1b",
   "metadata": {},
   "source": [
    "Spark provides a helpful [gloassary](https://spark.apache.org/docs/3.2.0/cluster-overview.html#glossary)\n",
    "\n",
    "Spark also provides a [list of configurations for the SparkContext](https://spark.apache.org/docs/latest/configuration.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1b96a1-f2eb-4e2f-a4d0-4acd817c4bc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0c50b64-f229-4521-88b0-9e5b3cc49696",
   "metadata": {},
   "source": [
    "# Object Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b70d555-4498-4e6b-9ec6-4e728675fc19",
   "metadata": {},
   "source": [
    "- Application - While never explicitly defined, a spark application is an executable (code file or snippet) which utlizes a spark cluster. Spark applications are submitted to a spark cluster using one of the two execution modes described below. As mentioned in the [README](README.md) Spark provides APIs in Java, Scala, R, and Python which allow users to define their applciaitons.\n",
    "\n",
    "- Deploy Mode (Execution Mode) - Distinguishes where the driver process runs. In \"cluster\" mode, the framework launches the driver inside of the cluster. In \"client\" mode, the submitter launches the driver outside of the cluster.\n",
    "\n",
    "- [Cache](https://spark.apache.org/docs/latest/quick-start.html#caching) - Apache spark provides a cluster-wide in-memory cache. This is very useful when data is accessed repeatedly, such as when querying a small “hot” dataset or when running an iterative algorithm like PageRank.\n",
    "\n",
    "\n",
    "- SparkSession\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54effd40-6176-43d2-8303-a445adad773b",
   "metadata": {},
   "source": [
    "# Process\n",
    "\n",
    "- Submitting an application\n",
    "\n",
    "https://blog.knoldus.com/cluster-vs-client-execution-modes-for-a-spark-application/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94ae9e6-040b-4285-8480-f97e302fc3ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81f688c6-83c3-4e22-adf8-9d4363f4f8c4",
   "metadata": {},
   "source": [
    "# Software Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2548d847-9b3b-419b-9c15-e60414f42f73",
   "metadata": {},
   "source": [
    "<center><img src=\"images/software-architecture.png\", width=\"400px\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef15191-37ac-4f65-bf08-e3fe3abfb9d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87d150a4-221a-4384-b50b-79cfb1b298ba",
   "metadata": {},
   "source": [
    "## Java and The JVM\n",
    "Spark runs on the JVM. To understand the spark architecture we need to understand this.\n",
    "\n",
    "Spark is written in java and scala. Java runs natively in the JVM while scala can be comiled into Java bytecode and run inside the JVM.\n",
    "\n",
    "The Java Virtual Machine (JVM) is an engine (sometimes called a virtual machine ... confusingly named) which provides the java runtime environment (JRE). In order to execute java code, it must be compiled into a jar file and executed inside the JVM. The Java Development Kit (JDK) allows the java code to access the JRE once inside the JVM.\n",
    "\n",
    "## Py4j\n",
    "Py4j is a python library which allows python to interact with spark. It is the backbone of the pyspark API.\n",
    "\n",
    "On the [homepage](https://www.py4j.org/index.html) Py4j is described as:\n",
    "\n",
    "> A Bridge between Python and Java\n",
    ">\n",
    "> Py4J enables Python programs running in a Python interpreter to dynamically access Java objects in a Java Virtual Machine. Methods are called as if the Java objects resided in the Python interpreter and Java collections can be accessed through standard Python collection methods. Py4J also enables Java programs to call back Python objects.\n",
    "\n",
    "Reading a bit deeper into the [documentation](https://www.py4j.org/getting_started.html) we see that the interop between python and java is provided by a **GatewayServer** instance. This gateway server allows Python programs to communicate with the JVM through a local network socket and send it instructions. It also has a callback functionality so that objects in the JVM can update objects in python in an event based manner. This Gateway is referred to as the Java Gateway in various points in the source code.\n",
    "\n",
    "This is very important. Recall that the Driver runs in the JVM. If we look at the pyspark source code on [github](https://github.com/apache/spark/blob/e91ef1929201d4e493bb451fef0fb1b45800adae/python/pyspark/java_gateway.py#L214) we can see that a Driver is created in the JVM and a python wrapper provided by py4j and the Java Gateway allows us to manipulate the driver.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a487fdd-be8b-479c-8b8b-c2ad1b2fd30c",
   "metadata": {},
   "source": [
    "## Spark Shell\n",
    "The Spark Shell is an analog of the traditional operating system shell (like BASH, CMD, or PowerShell). The Spark shell provides an interactive command line interface (CLI) through which a user can interact with the Spark API and thus a Spark Cluster (once properly configured). The Spark Shell's CLI allows users to type and execute ad-hoc lines of Scala, Python, or R code. Like any Shell, the Spark Shell follows the REPL (read-evaluate-print loop) pattern.\n",
    "\n",
    "An important point of note is that the Spark Shell acts as the Driver while a user incrementally defines their Application using the CLI (note: I am using the term Application very precisely. See Application defined alsewhere in this document.). We will see later that the Spark Shell is leveraged in a few ways. \n",
    "\n",
    "- spark submit\n",
    "- sparkcontext\n",
    "\n",
    "More on the Spark Shell can be found in the official [documentation](https://spark.apache.org/docs/latest/quick-start.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a930fe-3cae-467b-b01d-82cca720d318",
   "metadata": {},
   "source": [
    "## Entry Points\n",
    "As mentioned previously, users develop applications which leverage Apache Spark. As such, many articles and documentations talk about the entry point of a Spark application. This was a bit confusing; traditionally applications are said to have entry points or *main()* functions. These entrypoints are the gateway where the execution of an application begins and the user gets access to the runtime environment and/or API that the applciation was built to interact with. With Spark, the entrypoint gives the program access to the Spark environment.\n",
    "\n",
    "In Spark 1.x, three entry points were introduced: SparkContext, SQLContext and HiveContext. Since Spark 2.x, a new entry point called SparkSession has been introduced that essentially combined all functionalities available in the three aforementioned contexts. Note that all contexts are still available even in newest Spark releases, mostly for backward compatibility purposes.\n",
    "\n",
    "More information on these entrypoints can be found [here](https://towardsdatascience.com/sparksession-vs-sparkcontext-vs-sqlcontext-vs-hivecontext-741d50c9486a)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223721aa-d97f-4e68-a3ae-bb85d5326e81",
   "metadata": {},
   "source": [
    "### SparkContext\n",
    "\n",
    "The SparkContext is an integral part of Spark. Unfortunately There is no explicit definition anywhere in the official documentation and there are a lot of confusing, contradicting, muttled definitions floating around in third party articles.\n",
    "\n",
    "Note: The spark context is language specific, and we will be looking at the python SparkContext. That being said, I think it's safe to say most of what we say about the python API will apply to the other language bindings.\n",
    "\n",
    "I decided to have a look at the [source code](https://github.com/apache/spark/blob/e91ef1929201d4e493bb451fef0fb1b45800adae/python/pyspark/context.py#L66) to really understand what this object is. In the class defintion I see some comments but they are not particularely helpful:\n",
    "\n",
    "> Main entry point for Spark functionality. A SparkContext represents the connection to a Spark cluster, and can be used to create RDDs and broadcast variables on that cluster.\n",
    "\n",
    "This doesn't really help tie it in with the rest of the components So I decided to look at the SparkContext object's constructor. According to the code, it does the following:\n",
    "\n",
    "1. First the SparkContext ensures it is not running in a task on the worker node (it will raise an exception if it is). If it is not in a worker, the code assumes it is running in a driver. This phrasing was a bit confusing until I got deeper and understood how the py4j/JavaGateway worked (we will come back to this). \n",
    "\n",
    "2. Next the SparkContext initializes. This starts by considering a SparkConf object (and some other low level configurations for our driver and spark cluster) to constructs a Shell command. Based on the SparkConf the Shell command runs the spark-submit utility. If no target is specified (like when running from a jupyter notebook) the spark-submit utility is instructed to run the spark shell (in the case of python this is the pyspark shell). The Spark Shell acts as the Driver. Once the Spark Shell program is running, the SparkContext will determine the port it is listening on and leverage the magic of py4j. It  creates the JavaGateway and configures it so that the SparkContext can communicate with the driver (and other objects) running in the JVM. Specifically the SparkContext running in the python interpreter gets a reference to the JavaSparkContext object running in the JVM (through the marking of py4j). This Java object is what actually connects an API to the Spark cluster.\n",
    "\n",
    "Note: I have seen a lot of confusing statements made about the driver and the spark context. For example I have seen statements that the Driver creates the SparkContext. This is a half truth. In the case of python, scala, or R, the SparkContext exists in both the JVM and the respective language. The driver does create the JavaSparkContext but not the accompanying SparkContext for the language binding. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dd160d-f7ce-4820-8731-9861854fd6ce",
   "metadata": {},
   "source": [
    "I have found that running multiple SparkContexts in a single JVM is not reccomended."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2246bcf-5082-4ed0-8c89-11a7708ff1b8",
   "metadata": {},
   "source": [
    "## The Driver\n",
    "\n",
    "The Driver is a java process that runs in its own JVM. The Driver utilizes several components including the DAGScheduler, TaskScheduler, BackendScheduler and BlockManager to interpret and translate the user defined code in the Application into actual Spark Jobs which can be executed on the cluster. For example, python function calls become Transformations and Actions which are types of Spark Tasks. Once translated, the driver comes up with an execution plan and schedules the work with the Cluster Manager. Additionally, the Application may create data or cache data within the cluster. The Driver is also respobsible for keeping track of these resources. \n",
    "\n",
    "The Driver also hosts the Spark Web UI which allows admins to monitor the utlization of the cluster by the Application.\n",
    "\n",
    "The driver creates the SparkContext, connecting the user program to a given Spark Master\n",
    "\n",
    "### Deploy Mode\n",
    "Spark offers two methods to configure how and where the Driver runs. This is referred to as Deploy Mode or Execution Mode. \n",
    "\n",
    "Note: Most of our examples will deal with the \"Client Mode\" method of using spark. This is the method by which we can leverage spark through a jupyter notebook. Even if we tell the notebok to run in cluster mode, we will see that this setting is ignored.\n",
    "\n",
    "\n",
    "#### Client Mode\n",
    "In **client mode**, the driver is launched in the same process as the client that submits the application. An example of this is when using spark from a jupyter notebook. Here, the notebook spins up a SparkContext which launches the pyspark program as the driver in a subprocess. This is why we previously install java on our machines; the driver requires java.\n",
    "\n",
    "\n",
    "#### Cluster Mode\n",
    "\n",
    "In **cluster mode** however, the driver runs somewhere on the cluster (on a worker node). Exactly where depends on the type of Cluster Manager. This is useful as it allows the client to \"fire and forget\". The client can submit the application, walk away and come back to a set of completed results. This method of execution also useful when one needs to minimize the network latency between the driver and the workers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc482c2-2aed-4c44-93ce-a198162b3679",
   "metadata": {},
   "source": [
    "# System Architecture\n",
    "Recall that Spark is a distributed system and thus Spark applciations are distributed appliations. In order to understand how to use Spark we need to understand what it is and how it works. First we will focus on the system architecture before discussing the software that runs on top of the system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f7c72e-75ac-4fc6-b27e-f785c8743256",
   "metadata": {},
   "source": [
    "<center><img src=\"images/cluster-overview.png\", width=\"400px\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e8a0b9-78d1-436f-8b2d-757daf9a1e23",
   "metadata": {},
   "source": [
    "A Spark cluster is designed according to the traditional master-slave pattern. Typically, a master node is responsible for organizing, provisioning resources on, and distributing work to, the worker nodes who perform the actual computations and return results. Spark has renamed the master/slaves as cluster manager and workers respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec5c100-797b-4fd1-9e4d-71984f6e5eac",
   "metadata": {},
   "source": [
    "## The Cluster Manager (The master node)\n",
    "The Cluster Manager communicates with the SparkContext to understand what work needs to be accomplished. It takes the instructions and coordinates the execution with the worker nodes. In some cases, the Cluster Manager also provisions the instances of the worker node based on configurations passed to it from the SparkConf object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5c5039-03d0-452b-9d57-8f67097bb96e",
   "metadata": {},
   "source": [
    "Spark supports the following cluster managers:\n",
    "\n",
    "- [Standalone](https://spark.apache.org/docs/latest/spark-standalone.html) – a simple cluster manager included with Spark that makes it easy to set up a cluster.\n",
    " - [Apache Mesos](https://spark.apache.org/docs/latest/running-on-mesos.html) – a general cluster manager that can also run Hadoop MapReduce and service applications. (Deprecated)\n",
    "- [Hadoop YARN](https://spark.apache.org/docs/latest/running-on-yarn.html) – the resource manager in Hadoop 2.\n",
    "- [Kubernetes](https://spark.apache.org/docs/latest/running-on-kubernetes.html) – an open-source system for automating deployment, scaling, and management of containerized applications.\n",
    "\n",
    "https://spark.apache.org/docs/latest/cluster-overview.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68534b7-8c3a-4d8d-89f5-e8fb0adb97dd",
   "metadata": {},
   "source": [
    "## The Worker Node\n",
    "Performs the set of operations assigned to it by the Cluster Manager and returns the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffb72dd-1d75-4ee5-9a89-3da445eea971",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e558cb9-1e16-459b-b5ba-898daf334190",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f7ee93-dea3-4c75-8a9a-3333c5c805e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
