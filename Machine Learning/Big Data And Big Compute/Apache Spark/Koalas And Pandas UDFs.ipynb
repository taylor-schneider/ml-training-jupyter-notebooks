{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "627043cf-1b19-438c-bd25-456af4756da4",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "In Apache Spark 2.3. a new feature called Pandas UDFs was introduced. More can be found [here](https://databricks.com/blog/2017/10/30/introducing-vectorized-udfs-for-pyspark.html). \n",
    "\n",
    "A UDF stands for User Defined Function, meaning, as the name suggests, that a user writes the definition of a python function to be used with pandas. \n",
    "\n",
    "So why the distinction and what's the big real? Normally Pandas tries to be fast and defines it's built in operations in terms of vectors. This is why sometimes you'll see the term vectorized UDF popping up in mathematical libraries like pandas. The distrinction is that typically UDFs operate one row at a time (iteratively) unlike their vectorized cousins which operate in parrallel. As such things can be very slow. To get around this, users would define their UDFs in a language that was faster than the one they worked in, and would then call those libraries from their high level framework. For example spark users might choose java or scala. While there is a speed uptick, this adds complexity for users who are not multilingual. \n",
    "\n",
    "This is what makes apache sparks Pandas UDFs so exciting. Spark has integrated with the [Apache Arrow](https://arrow.apache.org/) project to provide the best of both worlds. Apache \n",
    "Arrow provides a standardized cross language columnar memory format and several usedul high performance libraries. By leveraging Arrow, Spark is abl to provide the ability to define low-overhead high-performance UDFs entierly in python.\n",
    "\n",
    "Conveniently many Koalas APIs utilize pandas UDFs under the hood. With the rollout of Apache Spark 3.0 even more UDFs have been introduced and leveraged as part of the Koalas API. As a result the Koalas 1.0.0 sees a 20 - 25% boost in performance when running against the Apache spark 3.0 framework.\n",
    "\n",
    "<center><img src=\"images/koalas_udf_speed.png\" width=\"400px\"></center>\n",
    "\n",
    "A very good blog article on this Koalas 1.0.0 release can be found [here](https://databricks.com/blog/2020/06/24/introducing-koalas-1-0.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759b4db5-c96d-4b09-bcf4-571c04bb8068",
   "metadata": {},
   "source": [
    "# 1. Get Setup\n",
    "## 1.1. Create spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "427352e8-b28f-4667-90f8-4afe085b0412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a helper module\n",
    "import importlib.util\n",
    "spec = importlib.util.spec_from_file_location(\"spark_helper\", \"../../../Utilities/spark_helper.py\")\n",
    "spark_helper = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(spark_helper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d46e3cc2-ab14-4234-b4ed-03d5f0832db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting SPARK_HOME\n",
      "/opt/spark\n",
      "\n",
      "Running findspark.init() function\n",
      "['/opt/spark/python', '/opt/spark/python/lib/py4j-0.10.9-src.zip', '/usr/lib64/python36.zip', '/usr/lib64/python3.6', '/usr/lib64/python3.6/lib-dynload', '', '/usr/local/lib64/python3.6/site-packages', '/usr/local/lib/python3.6/site-packages', '/usr/lib64/python3.6/site-packages', '/usr/lib/python3.6/site-packages', '/usr/local/lib/python3.6/site-packages/IPython/extensions', '/root/.ipython']\n",
      "\n",
      "Setting PYSPARK_PYTHON\n",
      "/usr/bin/python3\n",
      "\n",
      "Determining IP Of Server\n",
      "The ip was detected as: 15.4.12.12\n",
      "\n",
      "Configuring URL for kubernetes master\n",
      "k8s://https://15.4.7.11:6443\n",
      "\n",
      "Creating Spark Session\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "spark_app_name = \"spark-jupyter-win\"\n",
    "docker_image = \"tschneider/pyspark:v6-beta\"\n",
    "k8_master_ip = \"15.4.7.11\"\n",
    "spark_session = spark_helper.create_spark_session(spark_app_name, docker_image, k8_master_ip)\n",
    "sc = spark_session.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0afadf82-f610-49c2-9f18-d097e3f1d651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                        READY     STATUS    RESTARTS   AGE\n",
      "spark-jupyter-win-9a5ad77d95b8a2c5-exec-1   1/1       Running   0          1m\n",
      "spark-jupyter-win-9a5ad77d95b8a2c5-exec-2   1/1       Running   0          1m\n",
      "spark-jupyter-win-9a5ad77d95b8a2c5-exec-3   1/1       Running   0          1m\n"
     ]
    }
   ],
   "source": [
    "! kubectl -n spark get pod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd0f6c8-0e6e-4cd5-8583-70cfb8e0d804",
   "metadata": {},
   "source": [
    "# 2. Types of Pandas UDFs\n",
    "AS of Spark 2.3. There are two types of Pandas UDFs: Scalar and GroupedMap. \n",
    "\n",
    "We will explore each individually.\n",
    "\n",
    "## 2.1. Scalar Pandas UDFs (Vectorized UDFs)\n",
    "Scalar Pandas UDFs allow us to vectorize scalar functions we define in python.\n",
    "\n",
    "Recall from mathematics that a vector has a magnitude and direction while scalars have only a value. A linear function, or a linear operation/transformation, is when one applies a scalar to some object. For example we might multiply a vector $[4, 6]$ by the scalar $5$ resulting in another vector $[20, 25]$ with the same dimenstions.\n",
    "\n",
    "Within the pandas/spark framework, vectors are represented as a series while scalars are represented by built in types such as int, float, double, etc. Thus scalar functions are those which accept a scalar as a parameter and return a scalar of the same dimension.\n",
    "\n",
    "Below we will see examples of Pandas UDFs which apply scalar operations on our data (vectors). The cool thing, is that the spark framework will take care of vectorizing the functions for us! We do not have to worry abotu it and our code stays very clean and simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "888f42bb-fe76-44e5-a2c6-897fce80df18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a82b84e3-7d91-440b-bba3-0338b7150829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>b</td>\n",
       "      <td>444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>c</td>\n",
       "      <td>555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>b</td>\n",
       "      <td>222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>a</td>\n",
       "      <td>333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A  B    C\n",
       "0  1  a  333\n",
       "1  2  b  444\n",
       "2  3  c  555\n",
       "3  4  b  222\n",
       "4  5  a  333"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from databricks import koalas\n",
    "\n",
    "koalas_dataframe = koalas.DataFrame({\n",
    "    \"A\" : [1,2,3,4,5],\n",
    "    \"B\" : [\"a\", \"b\", \"c\", \"b\", \"a\"],\n",
    "    \"C\" : [333, 444, 555, 222, 333]\n",
    "})\n",
    "\n",
    "koalas_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "594d8869-cdd3-44fc-bea5-bcfc7f267585",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType, lit\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ab96e8b-e227-4079-a507-a71895d712aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(A,LongType,false),StructField(B,StringType,false),StructField(C,LongType,false)))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "koalas_dataframe.to_spark().schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7da12d19-877c-4438-884a-45b945caa10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def square(x):\n",
    "    return x ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22ebebf0-edd1-4fd7-9bc2-ca8a3e352f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/python/pyspark/sql/pandas/functions.py:392: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  \"in the future releases. See SPARK-28264 for more details.\", UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    110889\n",
       "1    197136\n",
       "2    308025\n",
       "3     49284\n",
       "4    110889\n",
       "Name: C, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "koalas_dataframe[\"C\"].apply(square)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a2a2bcba-93a2-4028-bd84-e5140f5f4d22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>E</th>\n",
       "      <th>F</th>\n",
       "      <th>G</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>333</td>\n",
       "      <td>foobar</td>\n",
       "      <td>5</td>\n",
       "      <td>barfoo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>b</td>\n",
       "      <td>444</td>\n",
       "      <td>foobar</td>\n",
       "      <td>5</td>\n",
       "      <td>barfoo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>c</td>\n",
       "      <td>555</td>\n",
       "      <td>foobar</td>\n",
       "      <td>5</td>\n",
       "      <td>barfoo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>b</td>\n",
       "      <td>222</td>\n",
       "      <td>foobar</td>\n",
       "      <td>5</td>\n",
       "      <td>barfoo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>a</td>\n",
       "      <td>333</td>\n",
       "      <td>foobar</td>\n",
       "      <td>5</td>\n",
       "      <td>barfoo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A  B    C       E  F       G\n",
       "0  1  a  333  foobar  5  barfoo\n",
       "1  2  b  444  foobar  5  barfoo\n",
       "2  3  c  555  foobar  5  barfoo\n",
       "3  4  b  222  foobar  5  barfoo\n",
       "4  5  a  333  foobar  5  barfoo"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def foobar(df):\n",
    "    df[\"E\"] = \"foobar\"\n",
    "    df[\"F\"] = 5\n",
    "    df[\"G\"] = \"barfoo\"\n",
    "    return df\n",
    "\n",
    "koalas_dataframe.groupby(\"A\").apply(foobar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3618f98f-861c-46be-894a-3e9d52dd4e2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301213a2-7c7e-4515-ba53-5a3ad845e03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_scalar_func(col: pandas.Series) -> numpy.int32:\n",
    "    return col * 5\n",
    "\n",
    "return_schema = StructType([StructField(\"C\", IntegerType())])\n",
    "my_udf = pandas_udf(my_scalar_func, \"int\", PandasUDFType.SCALAR)\n",
    "\n",
    "\n",
    "my_udf(lit(koalas_dataframe[\"C\"].to_pandas()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2873a1b-d107-4aca-8b00-cb8df2cb06ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3cad32-40a2-4314-b58d-03d22a3d4220",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c6c65d-0c76-4967-84ea-4255ad8b8546",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e9660a-bfbf-466a-a6bb-34ea8d24919a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_session.sparkContext.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
