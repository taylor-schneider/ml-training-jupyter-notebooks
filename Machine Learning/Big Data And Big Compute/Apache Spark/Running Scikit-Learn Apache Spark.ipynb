{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "In this notebook we are going to look at running scikit-learn models against an Apache Spark cluster. Unlike the models packaged with Apache Spark, scikit-learn models are not built to be distributed and thus cannot parallelize calculations involved to train the model.\n",
    "\n",
    "It assumes you are already familiar with running spark and have read the following notebooks:\n",
    "- [Install Apache Spark Prerequisites](Install%20Apache%20Spark%20Prerequisites.ipynb)\n",
    "- [Spark Pi - The Hello World Example For Apache spark](Spark%20Pi%20-%20The%20Hello%20World%20Example%20For%20Apache%20spark.ipynb)\n",
    "- [Intro To Koalas](Intro%20To%20Koalas.ipynb)\n",
    "- <a href=\"../Cluster%20Analysis/K-Means.ipynb\">Cluster Analysis/K-Means</a>\n",
    "\n",
    "## Adjenda\n",
    "1. Create SparkContext\n",
    "2. Create Web Server To Host Data\n",
    "3. Load The Data\n",
    "8. Prepare Worker Nodes\n",
    "9. Submit Python Code To Spark Cluster\n",
    "10. Cleanup Spark and Kubernetes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Create SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a helper module\n",
    "import importlib.util\n",
    "spec = importlib.util.spec_from_file_location(\"spark_helper\", \"../../../Utilities/spark_helper.py\")\n",
    "spark_helper = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(spark_helper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting SPARK_HOME\n",
      "/usr/lib/spark-3.1.1-bin-hadoop2.7\n",
      "\n",
      "Running findspark.init() function\n",
      "['/usr/lib/spark-3.1.1-bin-hadoop2.7/python', '/usr/lib/spark-3.1.1-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip', '/ml-training-jupyter-notebooks/Machine Learning/Big Data And Big Compute/Apache Spark', '/usr/local/lib/python39.zip', '/usr/local/lib/python3.9', '/usr/local/lib/python3.9/lib-dynload', '', '/usr/local/lib/python3.9/site-packages']\n",
      "\n",
      "Setting PYSPARK_PYTHON\n",
      "/usr/local/bin/python3\n",
      "\n",
      "Configuring URL for kubernetes master\n",
      "k8s://https://15.4.7.11:6443\n",
      "\n",
      "Determining IP Of Server\n",
      "The ip was detected as: 15.4.12.12\n",
      "\n",
      "Creating SparkConf Object\n",
      "('spark.master', 'k8s://https://15.4.7.11:6443')\n",
      "('spark.app.name', 'spark-jupyter-sklearn')\n",
      "('spark.submit.deploy.mode', 'cluster')\n",
      "('spark.kubernetes.container.image', 'tschneider/apache-spark-k8:v7')\n",
      "('spark.kubernetes.namespace', 'spark')\n",
      "('spark.kubernetes.pyspark.pythonVersion', '3')\n",
      "('spark.kubernetes.authenticate.driver.serviceAccountName', 'spark-sa')\n",
      "('spark.kubernetes.authenticate.serviceAccountName', 'spark-sa')\n",
      "('spark.executor.instances', '3')\n",
      "('spark.executor.cores', '2')\n",
      "('spark.executor.memory', '4096m')\n",
      "('spark.executor.memoryOverhead', '1024m')\n",
      "('spark.driver.memory', '1024m')\n",
      "('spark.driver.host', '15.4.12.12')\n",
      "('spark.files.overwrite', 'true')\n",
      "('spark.files.useFetchCache', 'false')\n",
      "\n",
      "Creating SparkSession Object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/15 21:32:02 WARN Utils: Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 15.4.12.12 instead (on interface eth0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "spark_app_name = \"spark-jupyter-sklearn\"\n",
    "docker_image = \"tschneider/apache-spark-k8:v7\"\n",
    "k8_master_ip = \"15.4.7.11\"\n",
    "spark_session = spark_helper.create_spark_session(spark_app_name, docker_image, k8_master_ip)\n",
    "sc = spark_session.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at kubernetes to see that out worker nodes were created. \n",
    "\n",
    "The first time we create the spark context with a given docker image, the image will need to be downloaded (which takes some time). As a result, we may see the pods with a status of \"ContainerCreating\". In this case, we will need to wait until the containers are in a \"Running\" state.\n",
    "\n",
    "```\n",
    "kubectl -n spark get pod\n",
    "NAME                                        READY   STATUS              RESTARTS   AGE\n",
    "spark-jupyter-win-3ed7f27984f7563a-exec-1   0/1     ContainerCreating   0          12m\n",
    "spark-jupyter-win-3ed7f27984f7563a-exec-2   0/1     ContainerCreating   0          12m\n",
    "spark-jupyter-win-3ed7f27984f7563a-exec-3   0/1     ContainerCreating   0          12m\n",
    "```\n",
    "\n",
    "We can check the status of the docker pull by logging into the container and running the docker pull command to attach to the running process:\n",
    "```\n",
    "kubectl -n spark exec -ti docker pull tschneider/pyspark:v3 docker pull tschneider/pyspark:v4\n",
    "v3: Pulling from tschneider/pyspark\n",
    "2d473b07cdd5: Already exists\n",
    "71d236fb1195: Already exists\n",
    "2e22160d8cab: Already exists\n",
    "e99d962ac218: Pull complete\n",
    "Digest: sha256:eb74701b4ae909c40046ff68b1044b09b11895e175c955dfd8afe9fe680309cf\n",
    "Status: Downloaded newer image for tschneider/pyspark:v3\n",
    "docker.io/tschneider/pyspark:v3\n",
    "[root@os004k8-worker002 ~]# docker pull tschneider/pyspark:v4\n",
    "v4: Pulling from tschneider/pyspark\n",
    "2d473b07cdd5: Already exists\n",
    "71d236fb1195: Already exists\n",
    "2e22160d8cab: Already exists\n",
    "c556a717fe5d: Downloading [=======================>                           ]  578.7MB/1.246GB\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                            READY     STATUS    RESTARTS   AGE\n",
      "spark-jupyter-sklearn-03a4877e5fa79c97-exec-1   1/1       Running   0          21s\n",
      "spark-jupyter-sklearn-03a4877e5fa79c97-exec-2   1/1       Running   0          21s\n",
      "spark-jupyter-sklearn-03a4877e5fa79c97-exec-3   1/1       Running   0          21s\n"
     ]
    }
   ],
   "source": [
    "! kubectl -n spark get pod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Setup Datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symlink exists at /nasdaq_2019.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "csv_file_name = \"nasdaq_2019.csv\"\n",
    "csv_relative_file_path = \"../../../Example Data Sets/{0}\".format(csv_file_name)\n",
    "csv_absolute_file_path = os.path.abspath(csv_relative_file_path)\n",
    "csv_link_path = \"/{0}\".format(csv_file_name)\n",
    "\n",
    "if os.path.exists(csv_link_path):\n",
    "    if os.path.islink(csv_link_path):\n",
    "        print(\"Symlink exists at {0}\".format(csv_link_path))\n",
    "    elif os.path.isfile(file):\n",
    "        print(\"File exists at {0}\".format(csv_link_path))\n",
    "    else:\n",
    "        raise Exception(\"Something is wrong. An object exists where we want to create a symlink.\")\n",
    "else:\n",
    "    os.symlink(csv_absolute_file_path, csv_link_path)\n",
    "    print(\"Symlink created as {0} -> {1}\".format(csv_link_path, csv_absolute_file_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. Create web server to host data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Determine the current working directory. \n",
    "\n",
    "Note: There is a trick to doing this inside a jupyter notebook and so we will use a special library to get that information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ml-training-jupyter-notebooks\n"
     ]
    }
   ],
   "source": [
    "import pyprojroot\n",
    "project_root_dir  = pyprojroot.here()\n",
    "print(project_root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Load the module for the webserver from our utilities directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the module for the web server we wrote\n",
    "import importlib.util\n",
    "spec = importlib.util.spec_from_file_location(\"PythonHttpFileServer\", \"../../../Utilities/PythonHttpFileServer.py\")\n",
    "PythonHttpFileServer = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(PythonHttpFileServer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Configure logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the logger and log level\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Start the webserver in a new thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_sub_dir = \"Example Data Sets\"\n",
    "web_root = os.path.join(project_root_dir, data_sub_dir)\n",
    "\n",
    "if not os.path.exists(web_root):\n",
    "    raise Exception(\"The web root for the server does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Starting server on port 80\n",
      "INFO:root:Web root specified as: /ml-training-jupyter-notebooks/Example Data Sets\n"
     ]
    }
   ],
   "source": [
    "# Start the webserver in a thread so the cell is not stuck in a running state\n",
    "import threading\n",
    "\n",
    "var_exists = 'web_server_thread' in locals() or 'web_server_thread' in globals()\n",
    "if not var_exists:\n",
    "    web_server_port = 80\n",
    "    web_server_args = (web_server_port, web_root)\n",
    "    web_server_thread = threading.Thread(target=PythonHttpFileServer.run_server, args=web_server_args)\n",
    "    web_server_thread.start()\n",
    "else:\n",
    "    print(\"Web Server thread already exists\")\n",
    "    print(\"To kill it you need to restart the kernel.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Load The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data is not as intuitive as one would think. We Instruct the spark cluster to download a file from the web server. But when we do this, the file is not actually downloaded. Remember, spark is lazy. Instead, a link to the url is stored in the spark session object so that the file can be downloaded when we need to perform an operation on it.\n",
    "\n",
    "Later, we will tell spark to return a handle to a dataframe consisting of the data contained in this url. At that point, lazy spark, will download the data file to each worker (at their root), open the file, and distribute the data accross the cluster. We will see proof of this when we call the addFile() function. Our server logs will show that the workers are making web requests to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Add the file using Spark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app 'PythonHttpFileServer' (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:werkzeug: * Running on all addresses.\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "INFO:werkzeug: * Running on http://15.4.12.12:80/ (Press CTRL+C to quit)\n",
      "INFO:root:Get /ml-training-jupyter-notebooks/Example Data Sets/nasdaq_2019.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading file 'http://15.4.12.12:80/nasdaq_2019.csv' to Spark cluster.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:15.4.12.12 - - [15/Jan/2022 21:32:31] \"GET /nasdaq_2019.csv HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "ip_address = spark_helper.determine_ip_address()\n",
    "csv_file_name = \"nasdaq_2019.csv\"\n",
    "csv_file_url = \"http://{0}:{1}/{2}\".format(ip_address, web_server_port, csv_file_name)\n",
    "print(\"Uploading file '{0}' to Spark cluster.\".format(csv_file_url))\n",
    "sc.addFile(csv_file_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Use koalas to open the file on spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the utility function to convert a date string to a datetime object from our utilities module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the utilities module we wrote\n",
    "import importlib.util\n",
    "spec = importlib.util.spec_from_file_location(\"utilities\", \"../../../Utilities/utilities.py\")\n",
    "utilities = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(utilities)\n",
    "\n",
    "# Define a mapping to convert our data field to the correct type\n",
    "converter_mapping = {\n",
    "    \"date\": utilities.convert_date_string_to_date\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Load our OHCLV data Into a koalas dataframe and pull out a single day in the say way we would in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:spark:Patching spark automatically. You can disable it by setting SPARK_KOALAS_AUTOPATCH=false in your environment\n",
      "INFO:root:Get /ml-training-jupyter-notebooks/Example Data Sets/nasdaq_2019.csv\n",
      "INFO:werkzeug:15.4.7.101 - - [15/Jan/2022 21:32:40] \"GET /nasdaq_2019.csv HTTP/1.1\" 200 -\n",
      "INFO:root:Get /ml-training-jupyter-notebooks/Example Data Sets/nasdaq_2019.csv\n",
      "INFO:root:Get /ml-training-jupyter-notebooks/Example Data Sets/nasdaq_2019.csv\n",
      "INFO:werkzeug:15.4.7.103 - - [15/Jan/2022 21:32:44] \"GET /nasdaq_2019.csv HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:15.4.7.102 - - [15/Jan/2022 21:32:44] \"GET /nasdaq_2019.csv HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "# Avoid a warning\n",
    "import os\n",
    "os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\n",
    "\n",
    "from databricks import koalas\n",
    "koalas_dataframe = koalas.read_csv(u\"file:////nasdaq_2019.csv\", converters=converter_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should see the workers download the file in the logs. If we log into the nodes we can see the file is located on the filesystem root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                            READY     STATUS    RESTARTS   AGE\n",
      "spark-jupyter-sklearn-03a4877e5fa79c97-exec-1   1/1       Running   0          49s\n",
      "spark-jupyter-sklearn-03a4877e5fa79c97-exec-2   1/1       Running   0          49s\n",
      "spark-jupyter-sklearn-03a4877e5fa79c97-exec-3   1/1       Running   0          49s\n"
     ]
    }
   ],
   "source": [
    "! kubectl -n spark get pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nasdaq_2019.csv\n"
     ]
    }
   ],
   "source": [
    "! kubectl -n spark exec -ti spark-jupyter-sklearn-03a4877e5fa79c97-exec-1 -- find / -name nasdaq_2019.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data loaded into a koalas dataframe we can access the data in the same way we would from a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>interval</th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AABA</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>70.90</td>\n",
       "      <td>71.5200</td>\n",
       "      <td>70.3250</td>\n",
       "      <td>70.57</td>\n",
       "      <td>10234800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAL</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>33.14</td>\n",
       "      <td>33.6632</td>\n",
       "      <td>32.5301</td>\n",
       "      <td>32.88</td>\n",
       "      <td>8995100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAME</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>2.43</td>\n",
       "      <td>2.4300</td>\n",
       "      <td>2.4000</td>\n",
       "      <td>2.40</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAOI</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>10.70</td>\n",
       "      <td>10.8900</td>\n",
       "      <td>10.0100</td>\n",
       "      <td>10.18</td>\n",
       "      <td>883100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAON</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>50.57</td>\n",
       "      <td>50.9850</td>\n",
       "      <td>48.5600</td>\n",
       "      <td>49.73</td>\n",
       "      <td>180200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker interval        date   open     high      low  close    volume\n",
       "0   AABA        D  2019-07-01  70.90  71.5200  70.3250  70.57  10234800\n",
       "1    AAL        D  2019-07-01  33.14  33.6632  32.5301  32.88   8995100\n",
       "2   AAME        D  2019-07-01   2.43   2.4300   2.4000   2.40       500\n",
       "3   AAOI        D  2019-07-01  10.70  10.8900  10.0100  10.18    883100\n",
       "4   AAON        D  2019-07-01  50.57  50.9850  48.5600  49.73    180200"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "koalas_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>interval</th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>93620</th>\n",
       "      <td>AABA</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>57.94</td>\n",
       "      <td>57.94</td>\n",
       "      <td>57.94</td>\n",
       "      <td>57.94</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93621</th>\n",
       "      <td>AAL</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>32.11</td>\n",
       "      <td>32.11</td>\n",
       "      <td>32.11</td>\n",
       "      <td>32.11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93622</th>\n",
       "      <td>AAME</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>2.41</td>\n",
       "      <td>2.41</td>\n",
       "      <td>2.41</td>\n",
       "      <td>2.41</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93623</th>\n",
       "      <td>AAOI</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>15.43</td>\n",
       "      <td>15.43</td>\n",
       "      <td>15.43</td>\n",
       "      <td>15.43</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93624</th>\n",
       "      <td>AAON</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>35.06</td>\n",
       "      <td>35.06</td>\n",
       "      <td>35.06</td>\n",
       "      <td>35.06</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ticker interval        date   open   high    low  close  volume\n",
       "93620   AABA        D  2019-01-01  57.94  57.94  57.94  57.94       0\n",
       "93621    AAL        D  2019-01-01  32.11  32.11  32.11  32.11       0\n",
       "93622   AAME        D  2019-01-01   2.41   2.41   2.41   2.41       0\n",
       "93623   AAOI        D  2019-01-01  15.43  15.43  15.43  15.43       0\n",
       "93624   AAON        D  2019-01-01  35.06  35.06  35.06  35.06       0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort based on the date column\n",
    "koalas_dataframe = koalas_dataframe.sort_values(\"date\")\n",
    "df_01_01_2019 = koalas_dataframe.loc[koalas_dataframe[\"date\"] == '2019-01-01']\n",
    "df_01_01_2019.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Prepare Worker Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order for us to be able to execute code on our spark cluster, we need to install relevant python libraries on the worker nodes. If you do not, you might see an error as follows when asking spark to do something:\n",
    "```\n",
    "PythonException: \n",
    "  An exception was thrown from the Python worker. Please see the stack trace below.\n",
    "Traceback (most recent call last):\n",
    "  File \"/usr/local/lib/python3.6/site-packages/pyspark/worker.py\", line 588, in main\n",
    "    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n",
    "  File \"/usr/local/lib/python3.6/site-packages/pyspark/worker.py\", line 421, in read_udfs\n",
    "    arg_offsets, f = read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=0)\n",
    "  File \"/usr/local/lib/python3.6/site-packages/pyspark/worker.py\", line 249, in read_single_udf\n",
    "    f, return_type = read_command(pickleSer, infile)\n",
    "  File \"/usr/local/lib/python3.6/site-packages/pyspark/worker.py\", line 69, in read_command\n",
    "    command = serializer._read_with_length(file)\n",
    "  File \"/usr/local/lib/python3.6/site-packages/pyspark/serializers.py\", line 160, in _read_with_length\n",
    "    return self.loads(obj)\n",
    "  File \"/usr/local/lib/python3.6/site-packages/pyspark/serializers.py\", line 430, in loads\n",
    "    return pickle.loads(obj, encoding=encoding)\n",
    "  File \"/usr/local/lib/python3.6/site-packages/pyspark/cloudpickle/cloudpickle.py\", line 562, in subimport\n",
    "    __import__(name)\n",
    "ModuleNotFoundError: No module named 'pandas'\n",
    "```\n",
    "\n",
    "Dont be confused, even though the module is installed locally, it does not mean it is installed on the work.\n",
    "\n",
    "For the purposes of this notebook, we will be using pandas, numpy, koalas, scikit-learn, sklearn. Luclikly they have already been installed on the docker image we are using for our worker (configured when we created the spark session). \n",
    "\n",
    "If you are unsure of what is installed on your workers, we can log into each worker pod in our kubernetes cluster and execute shell commands to interrogate or even install things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                            READY     STATUS    RESTARTS   AGE\n",
      "spark-jupyter-sklearn-03a4877e5fa79c97-exec-1   1/1       Running   0          1m\n",
      "spark-jupyter-sklearn-03a4877e5fa79c97-exec-2   1/1       Running   0          1m\n",
      "spark-jupyter-sklearn-03a4877e5fa79c97-exec-3   1/1       Running   0          1m\n"
     ]
    }
   ],
   "source": [
    "! kubectl -n spark get pod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package         Version\n",
      "--------------- -------\n",
      "click           8.0.3\n",
      "cloudpickle     2.0.0\n",
      "cycler          0.11.0\n",
      "findspark       1.4.2\n",
      "Flask           2.0.2\n",
      "fonttools       4.28.5\n",
      "future          0.18.2\n",
      "hyperopt        0.2.7\n",
      "itsdangerous    2.0.1\n",
      "Jinja2          3.0.3\n",
      "joblib          1.1.0\n",
      "kiwisolver      1.3.2\n",
      "kneed           0.7.0\n",
      "koalas          0.32.0\n",
      "MarkupSafe      2.0.1\n",
      "matplotlib      3.5.1\n",
      "netifaces       0.11.0\n",
      "networkx        2.6.3\n",
      "numpy           1.22.0\n",
      "packaging       21.3\n",
      "pandas          1.3.5\n",
      "Pillow          9.0.0\n",
      "pip             21.3.1\n",
      "progressbar     2.5\n",
      "py4j            0.10.9\n",
      "pyarrow         6.0.1\n",
      "pyparsing       3.0.6\n",
      "pyspark         3.1.1\n",
      "python-dateutil 2.8.2\n",
      "pytz            2021.3\n",
      "scikit-learn    1.0.2\n",
      "scipy           1.7.3\n",
      "setuptools      49.2.1\n",
      "six             1.16.0\n",
      "sklearn         0.0\n",
      "threadpoolctl   3.0.0\n",
      "tqdm            4.62.3\n",
      "Werkzeug        2.0.2\n"
     ]
    }
   ],
   "source": [
    "! kubectl -n spark exec -ti spark-jupyter-sklearn-03a4877e5fa79c97-exec-1 -- pip3 list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.9/site-packages (1.3.5)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.9/site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.9/site-packages (from pandas) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! kubectl -n spark exec -ti spark-jupyter-sklearn-03a4877e5fa79c97-exec-1 -- pip3 install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Submit Python Code To Spark Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section of the notebook we are going to apply the kmeans algorithm from sklearn to each date in our koalas_dataframe object.\n",
    "To do this, we are going to write a function that applies the algorithm to a dataframe; the assumption being the dataframe only contains data related to the same date.\n",
    "Note: Most of this is a review and reworking of the content contained in \n",
    "<a href=\"../Cluster%20Analysis/K-Means.ipynb\">Cluster Analysis/K-Means.ipynb</a>.\n",
    "\n",
    "We create our data frame for testing based on a subset of our real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>interval</th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>93620</th>\n",
       "      <td>AABA</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>57.94</td>\n",
       "      <td>57.94</td>\n",
       "      <td>57.94</td>\n",
       "      <td>57.94</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93621</th>\n",
       "      <td>AAL</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>32.11</td>\n",
       "      <td>32.11</td>\n",
       "      <td>32.11</td>\n",
       "      <td>32.11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93622</th>\n",
       "      <td>AAME</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>2.41</td>\n",
       "      <td>2.41</td>\n",
       "      <td>2.41</td>\n",
       "      <td>2.41</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93623</th>\n",
       "      <td>AAOI</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>15.43</td>\n",
       "      <td>15.43</td>\n",
       "      <td>15.43</td>\n",
       "      <td>15.43</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93624</th>\n",
       "      <td>AAON</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>35.06</td>\n",
       "      <td>35.06</td>\n",
       "      <td>35.06</td>\n",
       "      <td>35.06</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ticker interval        date   open   high    low  close  volume\n",
       "93620   AABA        D  2019-01-01  57.94  57.94  57.94  57.94       0\n",
       "93621    AAL        D  2019-01-01  32.11  32.11  32.11  32.11       0\n",
       "93622   AAME        D  2019-01-01   2.41   2.41   2.41   2.41       0\n",
       "93623   AAOI        D  2019-01-01  15.43  15.43  15.43  15.43       0\n",
       "93624   AAON        D  2019-01-01  35.06  35.06  35.06  35.06       0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_01_01_2019.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then write and test our function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "koalas.set_option('compute.ops_on_diff_frames', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "model = KMeans(n_clusters=5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_kmeans_on_dataframe(df, column_names=[\"open\"]):\n",
    "    \n",
    "    # Create a copy of our dataframe so we can play around\n",
    "    tmp = df.copy()\n",
    "    columns = column_names.copy()\n",
    "\n",
    "    # IF we only supplied one column name, we will need to create a bogus column\n",
    "    # Kmeans requires a 2D array so we will add a static column\n",
    "    bogus_column = None\n",
    "    if len(column_names) < 2:\n",
    "        bogus_column = \"Y\"\n",
    "        tmp[bogus_column] = [1 for x in range(0, tmp.shape[0])]\n",
    "        columns.append(bogus_column)\n",
    "        \n",
    "    # Set the parameters for our model\n",
    "    # It expects a 2D array where the columns are our features\n",
    "    model_parameters = tmp[[*columns]].to_numpy()\n",
    "    \n",
    "    # Train the model\n",
    "    trained_model = model.fit(model_parameters)\n",
    "    \n",
    "    # Extract the information\n",
    "    cluster_indices = trained_model.labels_.astype(int).tolist()\n",
    "    if bogus_column in columns:\n",
    "        cluster_centroids = [trained_model.cluster_centers_[i][0] for i in cluster_indices]\n",
    "    else:\n",
    "        cluster_centroids = [str(trained_model.cluster_centers_[i].tolist()) for i in cluster_indices]\n",
    "        \n",
    "    # Update the dataframe (setting special options to allow koalas to work)\n",
    "#    option_value = koalas.get_option('compute.ops_on_diff_frames')\n",
    "    #koalas.set_option('compute.ops_on_diff_frames', True)\n",
    "\n",
    "    tmp[\"cluster_centroids\"] = koalas.Series(cluster_centroids, index=tmp.index.to_numpy())\n",
    "    tmp[\"cluster_indices\"] = koalas.Series(cluster_indices, index=tmp.index.to_numpy())\n",
    "\n",
    "#    koalas.set_option('compute.ops_on_diff_frames', option_value)\n",
    "    \n",
    "    # Determine which columns we want to return\n",
    "    columns = tmp.columns.to_list()\n",
    "    if bogus_column in columns:\n",
    "        columns.remove(bogus_column)\n",
    "    \n",
    "    return tmp[[*columns]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>interval</th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>cluster_centroids</th>\n",
       "      <th>cluster_indices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>93816</th>\n",
       "      <td>APPF</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>59.22</td>\n",
       "      <td>59.22</td>\n",
       "      <td>59.22</td>\n",
       "      <td>59.22</td>\n",
       "      <td>0</td>\n",
       "      <td>[67.53273664825046, 67.53273664825046]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94988</th>\n",
       "      <td>HMST</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>21.23</td>\n",
       "      <td>21.23</td>\n",
       "      <td>21.23</td>\n",
       "      <td>21.23</td>\n",
       "      <td>0</td>\n",
       "      <td>[13.056601167315154, 13.056601167315154]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95031</th>\n",
       "      <td>HYGS</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0</td>\n",
       "      <td>[13.056601167315154, 13.056601167315154]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95149</th>\n",
       "      <td>IRIX</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>4.70</td>\n",
       "      <td>4.70</td>\n",
       "      <td>4.70</td>\n",
       "      <td>4.70</td>\n",
       "      <td>0</td>\n",
       "      <td>[13.056601167315154, 13.056601167315154]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95284</th>\n",
       "      <td>LBTYB</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>21.00</td>\n",
       "      <td>21.00</td>\n",
       "      <td>21.00</td>\n",
       "      <td>21.00</td>\n",
       "      <td>0</td>\n",
       "      <td>[13.056601167315154, 13.056601167315154]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ticker interval        date   open   high    low  close  volume                         cluster_centroids  cluster_indices\n",
       "93816   APPF        D  2019-01-01  59.22  59.22  59.22  59.22       0    [67.53273664825046, 67.53273664825046]                0\n",
       "94988   HMST        D  2019-01-01  21.23  21.23  21.23  21.23       0  [13.056601167315154, 13.056601167315154]                2\n",
       "95031   HYGS        D  2019-01-01   5.00   5.00   5.00   5.00       0  [13.056601167315154, 13.056601167315154]                2\n",
       "95149   IRIX        D  2019-01-01   4.70   4.70   4.70   4.70       0  [13.056601167315154, 13.056601167315154]                2\n",
       "95284  LBTYB        D  2019-01-01  21.00  21.00  21.00  21.00       0  [13.056601167315154, 13.056601167315154]                2"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perform_kmeans_on_dataframe(df_01_01_2019, column_names=[\"open\", \"close\"]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "apply() got an unexpected keyword argument 'column_names'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [28]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datetime\n\u001b[1;32m      2\u001b[0m now \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m----> 3\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mdf_01_01_2019\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mperform_kmeans_on_dataframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mopen\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m later \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m      5\u001b[0m difference \u001b[38;5;241m=\u001b[39m (later \u001b[38;5;241m-\u001b[39m now)\u001b[38;5;241m.\u001b[39mtotal_seconds()\n",
      "\u001b[0;31mTypeError\u001b[0m: apply() got an unexpected keyword argument 'column_names'"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "result = df_01_01_2019.groupby(\"date\").apply(perform_kmeans_on_dataframe, column_names=[\"open\"])\n",
    "later = datetime.now()\n",
    "difference = (later - now).total_seconds()\n",
    "print(\"total calculation time: {0}s\".format(difference))\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run this function against out entire dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "result = koalas_dataframe.groupby(\"date\").apply(perform_kmeans_on_dataframe, column_names=[\"open\"])\n",
    "later = datetime.now()\n",
    "difference = (later - now).total_seconds()\n",
    "print(\"total calculation time: {0}s\".format(difference))\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that computing every date is almost as fast as computing a single date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Cleanup Spark Cluster On Kubernetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if os.path.exists(csv_link_path) and os.path.islink(csv_link_path):\n",
    "    os.unlink(csv_link_path)\n",
    "    print(\"Deleted symlinked data file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! kubectl -n spark get pod"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
