{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "Previously we have seen examples of running the k-means algorithm provided by both scikit-learn and MLlib. In the previous notebook regarding [Running MLlib Algorothms](Running%20MLlib%20Algorithms%20%28k-means%29.ipynb) we learned that the MLlib algorithms are parallelized. This is a very useful feature as it allows us to shorten the amount of time to train/test a model on a large data set.\n",
    "\n",
    "The problem exists however with the Spark implimentation; it does not support nested parallelism.\n",
    "\n",
    "## Spark Doesnt Support Nested Parallelism\n",
    "\n",
    "What we mean to say is that Apache Spark doesn't support any form of nesting in terms of spark managed parallelism. Distributed operations can be initialized only by the driver (ie. not in a worker process created by the driver). Having looked at the source code there are a few reasons for this. The main reason is that Sparks code checks and prevents a SparkContext from being created on a worker. Without access to the SparkContext we do not have access to distributed data structures, like Spark DataFrame, and execution of parallel processing functions, like training an MLlib algorithm.\n",
    "\n",
    "If you tried to have a Spark Executor (worker) create and train an MLlib algorithm you would see the following error pop up:\n",
    "\n",
    "```\n",
    "AttributeError: Cannot load _jvm from SparkContext. Is SparkContext initialized?\n",
    "```\n",
    "\n",
    "This took me a while to figure out, but it is pointed out in this [discussion](https://coderedirect.com/questions/310003/run-ml-algorithm-inside-map-function-in-spark)\n",
    "\n",
    "This is inconvenient as spark provides a number of useful mechanism for kicking off parallel processes based on our dataset. For example, in the previous notebook using scikit-learn models, we use the `.groupby(date).apply(training_function)` functionality to kick off a training job for each group of data in parallel. But unfortunayely, doe to the limitations noted above, we are not able to use this api with mlib. Again, this is because the training  function is executed on the worker where a spark context is not found and cannot be created.\n",
    "\n",
    "## Options For Home Grown Nested Parllalelism\n",
    "\n",
    "Ok, so we cant use a single SparkContext to provide nested parallelism, what are our options? Based on my understanding, I suspect the following two alternatives are possible (we prove one out in this notebook, the other is for a rainy day):\n",
    "1. Have the driver kick off and manage parralel operations using a single SparkContext\n",
    "2. Have the driver kick off and manage parralel operations using multiple SparkContexts pointing to multiple clusters\n",
    "3. Hack the worker to create a SparkContext / Submit a PR to Spark's official Repo\n",
    "\n",
    "**Note**: If you have other suggestions, please open a github issue so we can chat!\n",
    "\n",
    "## Option 1: Driver Manages Parallel Calls To SparkContext\n",
    "We can do this using multithreading or multiprocessing. The idea being that in parrallel, we make calls to the same SparkContext object and submit work to the same spark cluster.\n",
    "\n",
    "Before crafting a solution, we need a refresher on some advanced programming topics can terminologies.\n",
    "\n",
    "> An operating system is in charge of running processes. The OS allocates memory and CPU resources for the process. Once allocated the process can utilize the resources as it likes. Multiprocessing is when a program asks the OS to spin up multiple \"subprocesses\" each with their own separate memory and cpu allocations. Multithreading is when a single process created multiple threads to execute work in parrallel instead of multiple processes. Multithreading allows for threads to share memory and cpu resources allocated to the process. Multiprocessing does not. That being said, I will solve this process using multithreading as it's convenient to pass information between threads. It could be done with multiprocessing... One might argue it should... but that's for another notebook one day.\n",
    "\n",
    "Ok, so what are we building? We will be creating a function which will launch multiple threads, tell them to do work in parallel, collect the results, and assemble the result rest into one big result. Sometimes this is called \"map reduce\" sometimes its called \"divide and conquer\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## There Be Dragons Ahead\n",
    "**Note**: This is a very complex subject and requires engineering skills as well as knowledge of troubleshooting spark. See gotchas section.\n",
    "\n",
    "One could make the case that we are abusing Spark (ie. using it in a way it was not designed). As a systems engineer that's basically my job so Nuts to that. With most distributed systems we will run into problems that we cannot explain or cannot reproduce. While writing this notebook I documented some issues I ran into and their solutions.\n",
    "\n",
    "You might run out of memory when you have multiple jobs competing against eachother. \n",
    "\n",
    "```\n",
    "Py4JJavaError: An error occurred while calling o689322.createOrReplaceTempView.\n",
    ": java.lang.OutOfMemoryError: Java heap space\n",
    "```\n",
    "\n",
    "As such you may need to adjust your sparkConf to allocate more memory to the driver/worker.\n",
    "```python\n",
    "    sparkConf.set(\"spark.executor.instances\", \"3\")\n",
    "    sparkConf.set(\"spark.executor.cores\", \"2\")\n",
    "    sparkConf.set(\"spark.executor.memory\", \"4096m\")\n",
    "    sparkConf.set(\"spark.executor.memoryOverhead\", \"1024m\")\n",
    "    sparkConf.set(\"spark.driver.memory\", \"1024m\")\n",
    "```\n",
    "\n",
    "Thing blowing up may cause other things to blow up unexpectedly. I coded a bug into my solution where i was killing a job before it finished writing data which corrupted the data file it was writing... Don't do this.\n",
    "\n",
    "```\n",
    "Py4JJavaError: An error occurred while calling o436701.csv.\n",
    ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 22489.0 failed 4 times, most recent failure: Lost task 3.3 in stage 22489.0 (TID 311712) (10.42.0.1 executor 2): java.io.EOFException: Cannot seek after EOF\n",
    "```\n",
    "\n",
    "I encountered some other issues and wrapped my function in retry logic just incase (this may not have been needed as the issue might have been unavoidable and also resolved in a later version of code)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "It assumes you have already read the following notebooks:\n",
    "- [Install Apache Spark Prerequisites](Install%20Apache%20Spark%20Prerequisites.ipynb)\n",
    "- [Spark Pi - The Hello World Example For Apache spark](Spark%20Pi%20-%20The%20Hello%20World%20Example%20For%20Apache%20spark.ipynb)\n",
    "- [Intro To Koalas](Intro%20To%20Koalas.ipynb)\n",
    "- [K-Means](../../Algorithms/Unsupervised%20Learning/Cluster%20Analysis/K-Means.ipynb)\n",
    "- [Load CSV Into Apache Spark On Kubernetes](Load%20CSV%20Into%20Apache%20Spark%20On%20Kubernetes.ipynb)\n",
    "\n",
    "The instructions are basically the same as [Running MLlib Algorothms](Running%20MLlib%20Algorithms%20%28k-means%29.ipynb) with the added framework I mentioned.\n",
    "\n",
    "## Adjenda\n",
    "1. Create SparkContext\n",
    "2. Create Web Server To Host Data\n",
    "3. Load The Data\n",
    "4. Develop Parallelization Framework\n",
    "5. Test Framework Performance\n",
    "8. Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Create SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/ml-training-jupyter-notebooks\n"
     ]
    }
   ],
   "source": [
    "import pyprojroot\n",
    "project_root_dir  = pyprojroot.here()\n",
    "print(project_root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading module: /root/ml-training-jupyter-notebooks/Utilities/spark_helper.py\n"
     ]
    }
   ],
   "source": [
    "# Load a helper module\n",
    "import os\n",
    "import importlib.util\n",
    "module_name = \"spark_helper\"\n",
    "module_dir = os.path.join(project_root_dir, \"Utilities\", \"{0}.py\".format(module_name))\n",
    "if not os.path.exists(module_dir):\n",
    "    print(\"The helper module does not exist\")\n",
    "print(\"Loading module: {0}\".format(module_dir))\n",
    "spec = importlib.util.spec_from_file_location(module_name, module_dir)\n",
    "spark_helper = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(spark_helper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting SPARK_HOME\n",
      "/opt/spark\n",
      "\n",
      "Running findspark.init() function\n",
      "['/opt/spark/python', '/opt/spark/python/lib/py4j-0.10.9-src.zip', '/usr/lib64/python36.zip', '/usr/lib64/python3.6', '/usr/lib64/python3.6/lib-dynload', '', '/usr/local/lib64/python3.6/site-packages', '/usr/local/lib/python3.6/site-packages', '/usr/lib64/python3.6/site-packages', '/usr/lib/python3.6/site-packages', '/usr/local/lib/python3.6/site-packages/IPython/extensions', '/root/.ipython']\n",
      "\n",
      "Setting PYSPARK_PYTHON\n",
      "/usr/bin/python3\n",
      "\n",
      "Configuring URL for kubernetes master\n",
      "k8s://https://15.4.7.11:6443\n",
      "\n",
      "Determining IP Of Server\n",
      "The ip was detected as: 15.4.12.12\n",
      "\n",
      "Creating SparkConf Object\n",
      "('spark.master', 'k8s://https://15.4.7.11:6443')\n",
      "('spark.app.name', 'spark-jupyter-mlib')\n",
      "('spark.submit.deploy.mode', 'cluster')\n",
      "('spark.kubernetes.container.image', 'tschneider/pyspark:v5')\n",
      "('spark.kubernetes.namespace', 'spark')\n",
      "('spark.kubernetes.pyspark.pythonVersion', '3')\n",
      "('spark.kubernetes.authenticate.driver.serviceAccountName', 'spark-sa')\n",
      "('spark.kubernetes.authenticate.serviceAccountName', 'spark-sa')\n",
      "('spark.executor.instances', '3')\n",
      "('spark.executor.cores', '2')\n",
      "('spark.executor.memory', '4096m')\n",
      "('spark.executor.memoryOverhead', '1024m')\n",
      "('spark.driver.memory', '1024m')\n",
      "('spark.driver.host', '15.4.12.12')\n",
      "('spark.files.overwrite', 'true')\n",
      "('spark.files.useFetchCache', 'false')\n",
      "\n",
      "Creating SparkSession Object\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "spark_app_name = \"spark-jupyter-mlib\"\n",
    "docker_image = \"tschneider/pyspark:v5\"\n",
    "k8_master_ip = \"15.4.7.11\"\n",
    "spark_session = spark_helper.create_spark_session(spark_app_name, docker_image, k8_master_ip)\n",
    "sc = spark_session.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                         READY     STATUS    RESTARTS   AGE\n",
      "spark-jupyter-mlib-87245e7dce89ac10-exec-1   1/1       Running   0          20s\n",
      "spark-jupyter-mlib-87245e7dce89ac10-exec-2   1/1       Running   0          20s\n",
      "spark-jupyter-mlib-87245e7dce89ac10-exec-3   1/1       Running   0          19s\n"
     ]
    }
   ],
   "source": [
    "! kubectl -n spark get pods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Setup Datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_name = \"Example Data Sets\"\n",
    "data_dir_path = os.path.join(project_root_dir, data_dir_name)\n",
    "spark_helper.link_data_dir_to_root(data_dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the module for the web server we wrote\n",
    "import importlib.util\n",
    "spec = importlib.util.spec_from_file_location(\"PythonHttpFileServer\", \"../../../Utilities/PythonHttpFileServer.py\")\n",
    "PythonHttpFileServer = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(PythonHttpFileServer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Web root and data file exist!\n",
      "web root: /root/ml-training-jupyter-notebooks/Example Data Sets\n",
      "data file: /root/ml-training-jupyter-notebooks/Example Data Sets/nasdaq_2019.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "data_dir_name = \"Example Data Sets\"\n",
    "web_root = os.path.join(project_root_dir, data_dir_name)\n",
    "\n",
    "if not os.path.exists(web_root):\n",
    "    raise Exception(\"The web root for the server does not exist.\")\n",
    "\n",
    "csv_file_name = \"nasdaq_2019.csv\"\n",
    "csv_file_path = os.path.join(web_root, csv_file_name)\n",
    "\n",
    "if not os.path.exists(csv_file_path):\n",
    "    raise Exception(\"The data file does not exist.\")\n",
    "    \n",
    "print(\"Web root and data file exist!\")\n",
    "print(\"web root: {0}\".format(web_root))\n",
    "print(\"data file: {0}\".format(csv_file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Starting server on port 80\n",
      "INFO:root:Web root specified as: /root/ml-training-jupyter-notebooks/Example Data Sets\n"
     ]
    }
   ],
   "source": [
    "# Import the library\n",
    "import threading\n",
    "\n",
    "# Configure the logger and log level (incase we need/want to debug)\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Create and start the thread if it doesnt exist\n",
    "var_exists = 'web_server_thread' in locals() or 'web_server_thread' in globals()\n",
    "if not var_exists:\n",
    "    web_server_port = 80\n",
    "    web_server_args = (web_server_port, web_root)\n",
    "    web_server_thread = threading.Thread(target=PythonHttpFileServer.run_server, args=web_server_args)\n",
    "    web_server_thread.start()\n",
    "else:\n",
    "    print(\"Web Server thread already exists\")\n",
    "    print(\"To kill it you need to restart the kernel.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Load The Data \n",
    "To help debug/test our framework we will load the data and build a few utilities targeting pices of the data. Once everything works we will run everything in parallel. Again, this step is just for Testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Add File To Spark Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app 'PythonHttpFileServer' (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:werkzeug: * Running on all addresses.\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "INFO:werkzeug: * Running on http://15.4.12.12:80/ (Press CTRL+C to quit)\n",
      "INFO:root:Get /root/ml-training-jupyter-notebooks/Example Data Sets/nasdaq_2019.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading file 'http://15.4.12.12:80/nasdaq_2019.csv' to Spark cluster.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:15.4.12.12 - - [18/Dec/2021 17:14:52] \"GET /nasdaq_2019.csv HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "ip_address = spark_helper.determine_ip_address()\n",
    "csv_file_name = \"nasdaq_2019.csv\"\n",
    "csv_file_url = \"http://{0}:{1}/{2}\".format(ip_address, web_server_port, csv_file_name)\n",
    "print(\"Uploading file '{0}' to Spark cluster.\".format(csv_file_url))\n",
    "sc.addFile(csv_file_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Use Koalas To Load Data File Into DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the utility function to convert a date string to a datetime object from our utilities module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the utilities module we wrote\n",
    "import importlib.util\n",
    "spec = importlib.util.spec_from_file_location(\"utilities\", \"../../../Utilities/utilities.py\")\n",
    "utilities = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(utilities)\n",
    "\n",
    "# Define a mapping to convert our data field to the correct type\n",
    "converter_mapping = {\n",
    "    \"date\": utilities.convert_date_string_to_date\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Load our OHCLV data Into a koalas dataframe and pull out a single day in the say way we would in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Get /root/ml-training-jupyter-notebooks/Example Data Sets/nasdaq_2019.csv\n",
      "INFO:werkzeug:15.4.7.102 - - [18/Dec/2021 17:15:01] \"GET /nasdaq_2019.csv HTTP/1.1\" 200 -\n",
      "INFO:root:Get /root/ml-training-jupyter-notebooks/Example Data Sets/nasdaq_2019.csv\n",
      "INFO:werkzeug:15.4.7.101 - - [18/Dec/2021 17:15:05] \"GET /nasdaq_2019.csv HTTP/1.1\" 200 -\n",
      "INFO:root:Get /root/ml-training-jupyter-notebooks/Example Data Sets/nasdaq_2019.csv\n",
      "INFO:werkzeug:15.4.7.103 - - [18/Dec/2021 17:15:05] \"GET /nasdaq_2019.csv HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "# Avoid a warning\n",
    "import os\n",
    "os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\n",
    "os.environ[\"SPARK_KOALAS_AUTOPATCH\"] = \"0\"\n",
    "\n",
    "from databricks import koalas\n",
    "koalas_dataframe = koalas.read_csv(u\"file:////nasdaq_2019.csv\", converters=converter_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>interval</th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AABA</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>70.90</td>\n",
       "      <td>71.5200</td>\n",
       "      <td>70.3250</td>\n",
       "      <td>70.57</td>\n",
       "      <td>10234800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAL</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>33.14</td>\n",
       "      <td>33.6632</td>\n",
       "      <td>32.5301</td>\n",
       "      <td>32.88</td>\n",
       "      <td>8995100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAME</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>2.43</td>\n",
       "      <td>2.4300</td>\n",
       "      <td>2.4000</td>\n",
       "      <td>2.40</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAOI</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>10.70</td>\n",
       "      <td>10.8900</td>\n",
       "      <td>10.0100</td>\n",
       "      <td>10.18</td>\n",
       "      <td>883100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAON</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>50.57</td>\n",
       "      <td>50.9850</td>\n",
       "      <td>48.5600</td>\n",
       "      <td>49.73</td>\n",
       "      <td>180200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker interval        date   open     high      low  close    volume\n",
       "0   AABA        D  2019-07-01  70.90  71.5200  70.3250  70.57  10234800\n",
       "1    AAL        D  2019-07-01  33.14  33.6632  32.5301  32.88   8995100\n",
       "2   AAME        D  2019-07-01   2.43   2.4300   2.4000   2.40       500\n",
       "3   AAOI        D  2019-07-01  10.70  10.8900  10.0100  10.18    883100\n",
       "4   AAON        D  2019-07-01  50.57  50.9850  48.5600  49.73    180200"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "koalas_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Develop Parralelization Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our objective is to apply the kmeans algorithm from Spark MLlib to each date in our koalas_dataframe object. Because the data contain mutually exclusive data, we can parallelize accross this variable.\n",
    "\n",
    "To do this, we are going to write a utility function that applies the algorithm to a dataframe; the assumption being the dataframe only contains data related to the same date.\n",
    "\n",
    "We will then write a function which wraps this function so that it plugs into a parallelizesd execution framework.\n",
    "\n",
    "Finally we will write the parallelization engine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Create Utility Function\n",
    "We create our data frame for testing based on a subset of our real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>interval</th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>96799</th>\n",
       "      <td>AABA</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>56.78</td>\n",
       "      <td>58.01</td>\n",
       "      <td>56.47</td>\n",
       "      <td>57.49</td>\n",
       "      <td>10532400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96800</th>\n",
       "      <td>AAL</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>31.46</td>\n",
       "      <td>32.65</td>\n",
       "      <td>31.05</td>\n",
       "      <td>32.48</td>\n",
       "      <td>5229400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96801</th>\n",
       "      <td>AAME</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>2.43</td>\n",
       "      <td>2.49</td>\n",
       "      <td>2.43</td>\n",
       "      <td>2.49</td>\n",
       "      <td>1700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96802</th>\n",
       "      <td>AAOI</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>15.00</td>\n",
       "      <td>16.29</td>\n",
       "      <td>14.85</td>\n",
       "      <td>15.88</td>\n",
       "      <td>478300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96803</th>\n",
       "      <td>AAON</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>34.57</td>\n",
       "      <td>35.40</td>\n",
       "      <td>34.37</td>\n",
       "      <td>35.07</td>\n",
       "      <td>124800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ticker interval        date   open   high    low  close    volume\n",
       "96799   AABA        D  2019-01-02  56.78  58.01  56.47  57.49  10532400\n",
       "96800    AAL        D  2019-01-02  31.46  32.65  31.05  32.48   5229400\n",
       "96801   AAME        D  2019-01-02   2.43   2.49   2.43   2.49      1700\n",
       "96802   AAOI        D  2019-01-02  15.00  16.29  14.85  15.88    478300\n",
       "96803   AAON        D  2019-01-02  34.57  35.40  34.37  35.07    124800"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_01_02_2019 = koalas_dataframe.loc[koalas_dataframe[\"date\"] == '2019-01-02'].copy()\n",
    "df_01_02_2019.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then write and test our function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import pyspark\n",
    "from pyspark.sql.functions import pandas_udf, udf\n",
    "from pyspark.sql.types import *\n",
    "from databricks import koalas\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "koalas.set_option(\"compute.ops_on_diff_frames\", True)\n",
    "\n",
    "def perform_kmeans_on_dataframe(df, column_names):\n",
    "    \n",
    "    # Create a copy of our dataframe so we can play around\n",
    "    tmp = df.copy()\n",
    "    columns = column_names.copy()\n",
    "    \n",
    "    # Create our model\n",
    "    model = KMeans().setK(5).setSeed(42)\n",
    "\n",
    "    # Do some magic to get the data in the right format for the spark model\n",
    "    from pyspark.ml.feature import VectorAssembler\n",
    "    assembler = VectorAssembler(inputCols=column_names, outputCol=\"features\")\n",
    "    if type(tmp) == koalas.frame.DataFrame:\n",
    "        model_parameters = assembler.transform(tmp[[*columns]].to_spark())\n",
    "    elif type(tmp) == pandas.DataFrame:\n",
    "        tmp = koalas.DataFrame(tmp)\n",
    "        model_parameters = assembler.transform(tmp[[*columns]].to_spark())\n",
    "\n",
    "    # Train the model\n",
    "    trained_model = model.fit(model_parameters)\n",
    "    \n",
    "    # Extract the cluster information for the training data\n",
    "    predictions = trained_model.transform(model_parameters)\n",
    "    cluster_indices = predictions.select(\"prediction\")\n",
    "    cluster_indices = koalas.DataFrame(cluster_indices).to_numpy().reshape(-1)\n",
    "    cluster_indices = koalas.Series(cluster_indices, index=tmp.index.to_numpy())\n",
    "    tmp[\"cluster_indices\"] = cluster_indices\n",
    "    cluster_centroids = trained_model.clusterCenters()\n",
    "    tmp[\"cluster_centroids\"] = tmp[\"cluster_indices\"].apply(lambda i: str(cluster_centroids[i]))\n",
    "\n",
    "    return tmp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/pyspark/sql/pandas/functions.py:392: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  \"in the future releases. See SPARK-28264 for more details.\", UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>interval</th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>cluster_indices</th>\n",
       "      <th>cluster_centroids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>96829</th>\n",
       "      <td>ACLS</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>17.48</td>\n",
       "      <td>18.14</td>\n",
       "      <td>16.920</td>\n",
       "      <td>17.79</td>\n",
       "      <td>284200</td>\n",
       "      <td>0</td>\n",
       "      <td>[12.97707127 13.2648608 ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96923</th>\n",
       "      <td>ALLK</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>51.26</td>\n",
       "      <td>53.94</td>\n",
       "      <td>50.115</td>\n",
       "      <td>51.99</td>\n",
       "      <td>151900</td>\n",
       "      <td>3</td>\n",
       "      <td>[66.74787778 67.59067778]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97216</th>\n",
       "      <td>BRPAU</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>10.55</td>\n",
       "      <td>10.55</td>\n",
       "      <td>10.550</td>\n",
       "      <td>10.55</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[12.97707127 13.2648608 ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97658</th>\n",
       "      <td>DWFI</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>22.21</td>\n",
       "      <td>22.24</td>\n",
       "      <td>22.200</td>\n",
       "      <td>22.20</td>\n",
       "      <td>41300</td>\n",
       "      <td>0</td>\n",
       "      <td>[12.97707127 13.2648608 ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97699</th>\n",
       "      <td>EFAS</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>15.08</td>\n",
       "      <td>15.08</td>\n",
       "      <td>15.080</td>\n",
       "      <td>15.08</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[12.97707127 13.2648608 ]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ticker interval        date   open   high     low  close  volume  cluster_indices          cluster_centroids\n",
       "96829   ACLS        D  2019-01-02  17.48  18.14  16.920  17.79  284200                0  [12.97707127 13.2648608 ]\n",
       "96923   ALLK        D  2019-01-02  51.26  53.94  50.115  51.99  151900                3  [66.74787778 67.59067778]\n",
       "97216  BRPAU        D  2019-01-02  10.55  10.55  10.550  10.55       0                0  [12.97707127 13.2648608 ]\n",
       "97658   DWFI        D  2019-01-02  22.21  22.24  22.200  22.20   41300                0  [12.97707127 13.2648608 ]\n",
       "97699   EFAS        D  2019-01-02  15.08  15.08  15.080  15.08       0                0  [12.97707127 13.2648608 ]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perform_kmeans_on_dataframe(df_01_02_2019, column_names=[\"open\", \"close\"]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: The warning above is coming from code used in the internals of Koalas. Do not worry about this warning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Create Wrapper Function\n",
    "Now that the utility function has been tested on smaller dataframes, we are safe to run it on a large data set in parrallel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2019-01-01'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a list of the dates in our dataframe\n",
    "dates = koalas_dataframe[\"date\"].unique().sort_values().to_numpy()\n",
    "dates[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the dataframe which is no longer needed and free up spark resources\n",
    "if 'koalas_dataframe' in locals() or 'koalas_dataframe' in globals():\n",
    "    del koalas_dataframe\n",
    "if 'df_01_02_2019' in locals() or 'df_01_02_2019' in globals():\n",
    "    del df_01_02_2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to build some utilities to facilitate the parralization of these opersations. As we mentioned in the Gotchas section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a wrapper function to call our kmeans function on our data and do some other managerial things \n",
    "import time\n",
    "\n",
    "def thread_wrapper__func(params, retries=5):\n",
    "\n",
    "    global completed_ops\n",
    "\n",
    "    # Retrieve params\n",
    "    date = params[0]\n",
    "    progress_bar = params[1]\n",
    "    lock = params[2]\n",
    "    input_file_path = params[3]\n",
    "    result_file_path = params[4]\n",
    "    thread_result = None\n",
    "    try:\n",
    "        # Load data\n",
    "        thread_df = koalas.read_csv(input_file_path, converters=converter_mapping)\n",
    "                \n",
    "        while True:\n",
    "            try:\n",
    "                # Train the model\n",
    "                date_df = thread_df.loc[thread_df[\"date\"] == date]    \n",
    "                thread_result = perform_kmeans_on_dataframe(date_df, column_names=[\"open\", \"close\"])\n",
    "\n",
    "                # Force spark to not be lazy and to do the computation\n",
    "                thread_result.shape  \n",
    "\n",
    "                # Record the results to a local data file\n",
    "                lock.acquire()\n",
    "                if completed_ops == 0:                    \n",
    "                    thread_result.to_pandas().to_csv(result_file_path, mode='a', index=False)\n",
    "                else:\n",
    "                    thread_result.to_pandas().to_csv(result_file_path, mode='a', index=False, header=False)\n",
    "                lock.release()\n",
    "                \n",
    "                # Return results and timing info to the ThreadHelper\n",
    "                return thread_result\n",
    "\n",
    "            except Exception as e:\n",
    "                retries -= 1\n",
    "                if retries > 0:\n",
    "                    time.sleep(1)\n",
    "                else:\n",
    "                    raise e\n",
    "    finally:  \n",
    "        # Update the progress bar\n",
    "        lock.acquire()\n",
    "        if thread_result is not None:\n",
    "            completed_ops += 1\n",
    "            progress_bar.update(completed_ops)\n",
    "        lock.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write a utility function to create a progress bar (again, magagerial stuff for parrallelism)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import progressbar\n",
    "\n",
    "def create_progress_bar(num_ops):\n",
    "\n",
    "    progress_bar_widgets = [\n",
    "        progressbar.Bar('=', '[', ']'), \n",
    "        ' ', \n",
    "        progressbar.FormatLabel('Processed: %(value)d / {0} ops'.format(num_ops)),\n",
    "        ' ', \n",
    "        progressbar.ETA()\n",
    "    ]\n",
    "    return  progressbar.ProgressBar(maxval=num_ops, widgets=progress_bar_widgets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Create Main Parallelization Function\n",
    "Write a function to kick things off in parrallel and returns the results. The trick here is that the results file is going to be stored in our example datasets folder and served to the workers via our web server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a ThreadPool and kick off the parrallel training sessions\n",
    "import os\n",
    "from multiprocessing.pool import ThreadPool\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "import threading\n",
    "\n",
    "def run_multithreaded_kmeans(dates):\n",
    "    try:\n",
    "        print(\"Create vars to help with synchronization\")\n",
    "        mutex = threading.Lock()\n",
    "        num_threads = 10\n",
    "        thread_pool = ThreadPool(num_threads)\n",
    "            \n",
    "        print(\"Create result file to store results from threads\")\n",
    "        project_root_dir  = pyprojroot.here()\n",
    "        result_file_name = \"results.csv\"\n",
    "        result_file_path = os.path.join(project_root_dir, \"Example Data Sets\", result_file_name)\n",
    "        do_work = False\n",
    "        if not os.path.exists(result_file_path):          \n",
    "            with open(result_file_path, 'w') as fp:\n",
    "                pass\n",
    "            do_work = True\n",
    "        else:\n",
    "            print(\"No work to do, results file already exists!\")\n",
    "        \n",
    "        print(\"Setting up local datastore for driver\")\n",
    "        data_dir_name = \"Example Data Sets\"\n",
    "        data_dir_path = os.path.join(project_root_dir, data_dir_name)\n",
    "        spark_helper.link_data_dir_to_root(data_dir_path)\n",
    "\n",
    "        if do_work:\n",
    "            print(\"No result file exists, doing work...\")\n",
    "            start = datetime.now()\n",
    "            print(\"Starting: {0}\".format(start))\n",
    "            \n",
    "            print(\"Create objects to help track multithreading progress\")\n",
    "            num_ops = len(dates)\n",
    "            bar = create_progress_bar(num_ops)\n",
    "            bar.start()\n",
    "            global completed_ops\n",
    "            completed_ops = 0\n",
    "        \n",
    "            print(\"Create an iterator of params for the thread function\")\n",
    "            \n",
    "            input_file_name = \"nasdaq_2019.csv\"\n",
    "            input_file_path = \"file:///{0}\".format(input_file_name)\n",
    "        \n",
    "            iterator = zip(dates, \n",
    "                           itertools.repeat(bar), \n",
    "                           itertools.repeat(mutex),\n",
    "                           itertools.repeat(input_file_path),\n",
    "                           itertools.repeat(result_file_path))\n",
    "            \n",
    "            print(\"Adding data file to cluster\")\n",
    "            sc.addFile(csv_file_url)\n",
    "            \n",
    "            print(\"Run training sessions for each data in parrallel\")\n",
    "            results = thread_pool.map(thread_wrapper__func, iterator)\n",
    "\n",
    "            # Record the end time\n",
    "            calc_end = datetime.now()\n",
    "            calc_diff = (calc_end - start).total_seconds()\n",
    "            print(\"Ending: {0}\".format(calc_end))\n",
    "            print(\"Total calculation time: {0}s\".format(calc_diff))\n",
    "            date_diff = calc_diff / num_ops\n",
    "            print(\"Time per date: {0}s\".format(date_diff))\n",
    "\n",
    "        print(\"Load results file\")\n",
    "        ip_address = spark_helper.determine_ip_address()\n",
    "        web_server_port = 80\n",
    "        result_file_url = \"http://{0}:{1}/{2}\".format(ip_address, web_server_port, result_file_name)\n",
    "        worker_result_file_path = \"file:///{0}\".format(result_file_name)\n",
    "        spark_helper.link_data_dir_to_root(data_dir_path)\n",
    "        try:\n",
    "            mutex.acquire()\n",
    "            sc.addFile(result_file_url)\n",
    "            merged_df = koalas.read_csv(worker_result_file_path, converters=converter_mapping)\n",
    "        finally:\n",
    "            mutex.release()\n",
    "        \n",
    "        return merged_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Cleanup spark\n",
    "        sc.cancelAllJobs()\n",
    "        # Raise error\n",
    "        raise e\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[                                         ] Processed: 0 / 4 ops ETA:  --:--:--\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create vars to help with synchronization\n",
      "Create result file to store results from threads\n",
      "Setting up local datastore for driver\n",
      "No result file exists, doing work...\n",
      "Starting: 2021-12-19 15:21:48.016231\n",
      "Create objects to help track multithreading progress\n",
      "Create an iterator of params for the thread function\n",
      "Adding data file to cluster\n",
      "Run training sessions for each data in parrallel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/pyspark/sql/pandas/functions.py:392: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  \"in the future releases. See SPARK-28264 for more details.\", UserWarning)\n",
      "[==========================================] Processed: 4 / 4 ops ETA:  0:00:00\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending: 2021-12-19 15:23:25.415392\n",
      "Total calculation time: 97.399161s\n",
      "Time per date: 24.34979025s\n",
      "Load results file\n"
     ]
    }
   ],
   "source": [
    "# Run the calculation\n",
    "merged_df = run_multithreaded_kmeans(dates[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Get /root/ml-training-jupyter-notebooks/Example Data Sets/results.csv\n",
      "INFO:werkzeug:15.4.7.103 - - [18/Dec/2021 17:18:33] \"GET /results.csv HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>interval</th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>cluster_indices</th>\n",
       "      <th>cluster_centroids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3185</th>\n",
       "      <td>APPF</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>59.22</td>\n",
       "      <td>59.22</td>\n",
       "      <td>59.22</td>\n",
       "      <td>59.22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[48.53718713 48.53718713]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3186</th>\n",
       "      <td>HMST</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>21.23</td>\n",
       "      <td>21.23</td>\n",
       "      <td>21.23</td>\n",
       "      <td>21.23</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[10.78294524 10.78294524]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3187</th>\n",
       "      <td>HYGS</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[10.78294524 10.78294524]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3188</th>\n",
       "      <td>IRIX</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>4.70</td>\n",
       "      <td>4.70</td>\n",
       "      <td>4.70</td>\n",
       "      <td>4.70</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[10.78294524 10.78294524]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3189</th>\n",
       "      <td>LBTYB</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>21.00</td>\n",
       "      <td>21.00</td>\n",
       "      <td>21.00</td>\n",
       "      <td>21.00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[10.78294524 10.78294524]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ticker interval        date   open   high    low  close  volume  cluster_indices          cluster_centroids\n",
       "3185   APPF        D  2019-01-01  59.22  59.22  59.22  59.22       0                0  [48.53718713 48.53718713]\n",
       "3186   HMST        D  2019-01-01  21.23  21.23  21.23  21.23       0                1  [10.78294524 10.78294524]\n",
       "3187   HYGS        D  2019-01-01   5.00   5.00   5.00   5.00       0                1  [10.78294524 10.78294524]\n",
       "3188   IRIX        D  2019-01-01   4.70   4.70   4.70   4.70       0                1  [10.78294524 10.78294524]\n",
       "3189  LBTYB        D  2019-01-01  21.00  21.00  21.00  21.00       0                1  [10.78294524 10.78294524]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the union df\n",
    "merged_df.loc[merged_df[\"date\"] == dates[0]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If anything went wrong, we should ask the SparkContext to kill any abandoned jobs that may be lingering in ghosted threads from the thread pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.cancelAllJobs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Test The Framework\n",
    "Now we can run a large set of dates using our threadpool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_session.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting SPARK_HOME\n",
      "/opt/spark\n",
      "\n",
      "Running findspark.init() function\n",
      "['/opt/spark/python', '/opt/spark/python/lib/py4j-0.10.9-src.zip', '/opt/spark/python', '/tmp/spark-47e36b6f-2b85-4b44-85ac-fbec18ff038a/userFiles-9b2530ee-f778-4c63-af38-4ef2a4ad10fa', '/opt/spark/python/lib/py4j-0.10.9-src.zip', '/opt/spark/python', '/tmp/spark-47e36b6f-2b85-4b44-85ac-fbec18ff038a/userFiles-9d6ff110-54e9-4802-9e96-8fca9e9eb950', '/opt/spark/python/lib/py4j-0.10.9-src.zip', '/opt/spark/python', '/tmp/spark-47e36b6f-2b85-4b44-85ac-fbec18ff038a/userFiles-a5396c6f-2ce8-43aa-964c-ffb80ad3266a', '/opt/spark/python/lib/py4j-0.10.9-src.zip', '/opt/spark/python', '/tmp/spark-47e36b6f-2b85-4b44-85ac-fbec18ff038a/userFiles-3091eaef-c355-43d9-91cc-369373f24ca9', '/opt/spark/python/lib/py4j-0.10.9-src.zip', '/opt/spark/python', '/tmp/spark-47e36b6f-2b85-4b44-85ac-fbec18ff038a/userFiles-9c27a48b-d3c6-4d24-b76e-48f159c706ba', '/opt/spark/python/lib/py4j-0.10.9-src.zip', '/usr/lib64/python36.zip', '/usr/lib64/python3.6', '/usr/lib64/python3.6/lib-dynload', '', '/usr/local/lib64/python3.6/site-packages', '/usr/local/lib/python3.6/site-packages', '/usr/lib64/python3.6/site-packages', '/usr/lib/python3.6/site-packages', '/usr/local/lib/python3.6/site-packages/IPython/extensions', '/root/.ipython']\n",
      "\n",
      "Setting PYSPARK_PYTHON\n",
      "/usr/bin/python3\n",
      "\n",
      "Configuring URL for kubernetes master\n",
      "k8s://https://15.4.7.11:6443\n",
      "\n",
      "Determining IP Of Server\n",
      "The ip was detected as: 15.4.12.12\n",
      "\n",
      "Creating SparkConf Object\n",
      "('spark.executor.instances', '3')\n",
      "('spark.kubernetes.container.image', 'tschneider/pyspark:v5')\n",
      "('spark.driver.host', '15.4.12.12')\n",
      "('spark.executor.memoryOverhead', '1024m')\n",
      "('spark.driver.memory', '1024m')\n",
      "('spark.executor.cores', '2')\n",
      "('spark.kubernetes.pyspark.pythonVersion', '3')\n",
      "('spark.executor.memory', '4096m')\n",
      "('spark.kubernetes.namespace', 'spark')\n",
      "('spark.kubernetes.authenticate.serviceAccountName', 'spark-sa')\n",
      "('spark.submit.deploy.mode', 'cluster')\n",
      "('spark.master', 'k8s://https://15.4.7.11:6443')\n",
      "('spark.submit.pyFiles', '')\n",
      "('spark.files.overwrite', 'true')\n",
      "('spark.submit.deployMode', 'client')\n",
      "('spark.kubernetes.authenticate.driver.serviceAccountName', 'spark-sa')\n",
      "('spark.app.name', 'spark-jupyter-mlib')\n",
      "('spark.ui.showConsoleProgress', 'true')\n",
      "('spark.files.useFetchCache', 'false')\n",
      "\n",
      "Creating SparkSession Object\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "spark_session = spark_helper.create_spark_session(spark_app_name, docker_image, k8_master_ip)\n",
    "sc = spark_session.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_file_path = os.path.join(data_dir_path, \"results.csv\")\n",
    "os.remove(results_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "koalas.set_option(\"compute.ops_on_diff_frames\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Get /root/ml-training-jupyter-notebooks/Example Data Sets/nasdaq_2019.csv\n",
      "INFO:werkzeug:15.4.12.12 - - [18/Dec/2021 18:08:14] \"GET /nasdaq_2019.csv HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create vars to help with synchronization\n",
      "Create result file to store results from threads\n",
      "Setting up local datastore for driver\n",
      "No result file exists, doing work...\n",
      "Starting: 2021-12-18 18:08:13.928080\n",
      "Create objects to help track multithreading progress\n",
      "Create an iterator of params for the thread function\n",
      "Adding data file to cluster\n",
      "Run training sessions for each data in parrallel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Get /root/ml-training-jupyter-notebooks/Example Data Sets/nasdaq_2019.csv\n",
      "INFO:root:Get /root/ml-training-jupyter-notebooks/Example Data Sets/nasdaq_2019.csv\n",
      "INFO:root:Get /root/ml-training-jupyter-notebooks/Example Data Sets/nasdaq_2019.csv\n",
      "INFO:werkzeug:15.4.7.101 - - [18/Dec/2021 18:08:15] \"GET /nasdaq_2019.csv HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:15.4.7.103 - - [18/Dec/2021 18:08:15] \"GET /nasdaq_2019.csv HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:15.4.7.102 - - [18/Dec/2021 18:08:15] \"GET /nasdaq_2019.csv HTTP/1.1\" 200 -\n",
      "/usr/local/lib/python3.6/site-packages/pyspark/sql/pandas/functions.py:392: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  \"in the future releases. See SPARK-28264 for more details.\", UserWarning)\n",
      "INFO:root:Get /root/ml-training-jupyter-notebooks/Example Data Sets/results.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending: 2021-12-18 18:51:39.730662\n",
      "Total calculation time: 2605.802582s\n",
      "Time per date: 16.49242140506329s\n",
      "Load results file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:15.4.12.12 - - [18/Dec/2021 18:51:40] \"GET /results.csv HTTP/1.1\" 200 -\n",
      "INFO:root:Get /root/ml-training-jupyter-notebooks/Example Data Sets/results.csv\n",
      "INFO:werkzeug:15.4.7.103 - - [18/Dec/2021 18:51:40] \"GET /results.csv HTTP/1.1\" 200 -\n",
      "INFO:root:Get /root/ml-training-jupyter-notebooks/Example Data Sets/results.csv\n",
      "INFO:werkzeug:15.4.7.101 - - [18/Dec/2021 18:51:41] \"GET /results.csv HTTP/1.1\" 200 -\n",
      "INFO:root:Get /root/ml-training-jupyter-notebooks/Example Data Sets/results.csv\n",
      "INFO:werkzeug:15.4.7.102 - - [18/Dec/2021 18:51:44] \"GET /results.csv HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "merged_df = run_multithreaded_kmeans(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>interval</th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>cluster_indices</th>\n",
       "      <th>cluster_centroids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>99161</th>\n",
       "      <td>APEN</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-10</td>\n",
       "      <td>3.79</td>\n",
       "      <td>3.79</td>\n",
       "      <td>3.63</td>\n",
       "      <td>3.630</td>\n",
       "      <td>2700</td>\n",
       "      <td>0</td>\n",
       "      <td>[13.64757638 13.71332506]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99162</th>\n",
       "      <td>ASFI</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-10</td>\n",
       "      <td>4.32</td>\n",
       "      <td>4.32</td>\n",
       "      <td>4.26</td>\n",
       "      <td>4.265</td>\n",
       "      <td>16100</td>\n",
       "      <td>0</td>\n",
       "      <td>[13.64757638 13.71332506]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99163</th>\n",
       "      <td>CPIX</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-10</td>\n",
       "      <td>6.45</td>\n",
       "      <td>6.50</td>\n",
       "      <td>6.11</td>\n",
       "      <td>6.110</td>\n",
       "      <td>3400</td>\n",
       "      <td>0</td>\n",
       "      <td>[13.64757638 13.71332506]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99164</th>\n",
       "      <td>CZWI</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-10</td>\n",
       "      <td>11.08</td>\n",
       "      <td>11.25</td>\n",
       "      <td>11.07</td>\n",
       "      <td>11.170</td>\n",
       "      <td>6200</td>\n",
       "      <td>0</td>\n",
       "      <td>[13.64757638 13.71332506]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99165</th>\n",
       "      <td>FTXH</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-10</td>\n",
       "      <td>20.16</td>\n",
       "      <td>20.16</td>\n",
       "      <td>19.92</td>\n",
       "      <td>20.050</td>\n",
       "      <td>1400</td>\n",
       "      <td>0</td>\n",
       "      <td>[13.64757638 13.71332506]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ticker interval        date   open   high    low   close  volume  cluster_indices          cluster_centroids\n",
       "99161   APEN        D  2019-01-10   3.79   3.79   3.63   3.630    2700                0  [13.64757638 13.71332506]\n",
       "99162   ASFI        D  2019-01-10   4.32   4.32   4.26   4.265   16100                0  [13.64757638 13.71332506]\n",
       "99163   CPIX        D  2019-01-10   6.45   6.50   6.11   6.110    3400                0  [13.64757638 13.71332506]\n",
       "99164   CZWI        D  2019-01-10  11.08  11.25  11.07  11.170    6200                0  [13.64757638 13.71332506]\n",
       "99165   FTXH        D  2019-01-10  20.16  20.16  19.92  20.050    1400                0  [13.64757638 13.71332506]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.loc[merged_df[\"date\"] == dates[7]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If anything went wrong, we should ask the SparkContext to kill any abandoned jobs that may be lingering in ghosted threads from the thread pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.cancelAllJobs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that computing every date is almost as fast as computing a single date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Compare Results\n",
    "\n",
    "We timed ourselves when running against a single day, a few days, and all days to show the operations are occurring in parrallel.\n",
    "\n",
    "At first glance, it looks like the actual per date computation got faster. This is a bit strange. My only guess is that loading the data file was now spread accross multiple dates instead of one in the calculation.\n",
    "\n",
    "97 seconds / 4 dates = 24.25 secondsper date\n",
    "2605 seconds / 158 dates = 16.5 seconds per date\n",
    "\n",
    "The important thing here is that we could scale our cluster out more and more nodes and the comutation should decrease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 6. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing Symlink: /root/ml-training-jupyter-notebooks/Example Data Sets/Test Scores.csv -> /Test Scores.csv\n",
      "Removing Symlink: /root/ml-training-jupyter-notebooks/Example Data Sets/nasdaq_2019.csv -> /nasdaq_2019.csv\n",
      "Removing Symlink: /root/ml-training-jupyter-notebooks/Example Data Sets/.ipynb_checkpoints -> /.ipynb_checkpoints\n",
      "Removing Symlink: /root/ml-training-jupyter-notebooks/Example Data Sets/results.csv -> /results.csv\n"
     ]
    }
   ],
   "source": [
    "spark_helper.unlink_data_dir_from_root(data_dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_session.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "! kubectl -n spark get pod"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
