{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "Previously we have seen examples of running the k-means algorithm provided by scikit-learn. In this notebook we are going to look at the Apache Spark MLib implimentation instead. Unlike the models packaged with scikit-learn, Apache Spark models are built to be distributed and can parallelize calculations. This fact can cause some headaches due to the assumptions/design that the spark framework asserts. We will cover this is the gotcha section.\n",
    "\n",
    "It assumes you have already read the following notebooks:\n",
    "- [Install Apache Spark Prerequisites](Install%20Apache%20Spark%20Prerequisites.ipynb)\n",
    "- [Spark Pi - The Hello World Example For Apache spark](Spark%20Pi%20-%20The%20Hello%20World%20Example%20For%20Apache%20spark.ipynb)\n",
    "- [Intro To Koalas](Intro%20To%20Koalas.ipynb)\n",
    "- <a href=\"../Cluster%20Analysis/K-Means.ipynb\">Cluster Analysis/K-Means</a>\n",
    "\n",
    "The instructions are basically the same as [Running Scikit-Learn Apache Spark](Running%20Scikit-Learn%20Apache%20Spark.ipynb) once you get the kubernetes stuff setup.\n",
    "\n",
    "## Adjenda\n",
    "1. Create SparkContext\n",
    "2. Create Web Server To Host Data\n",
    "3. Load The Data\n",
    "8. Prepare Worker Nodes\n",
    "9. Submit Python Code To Spark Cluster\n",
    "10. Cleanup Spark and Kubernetes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gotchas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Doesnt Support Nested Parallelism\n",
    "\n",
    "Apache Spark doesn't support any form of nesting in terms of spark managed parallelism. Distributed operations can be initialized only by the driver (ie. not in a worker process created by the driver). This includes access to distributed data structures, like Spark DataFrame, and execution of parallel processing functions, like training an MLlib algorithm.\n",
    "\n",
    "If you tried to have a spark worker create and train an MLlib algorithm you would see the following error pop up:\n",
    "\n",
    "```\n",
    "AttributeError: Cannot load _jvm from SparkContext. Is SparkContext initialized?\n",
    "```\n",
    "\n",
    "This took me a while to figure out, but it is pointed out [here](https://coderedirect.com/questions/310003/run-ml-algorithm-inside-map-function-in-spark)\n",
    "\n",
    "This is inconvenient as spark provides a number of useful mechanism for kicking off parallel processes based on our dataset. For example, in the previous notebook we use the groupby(criteria).apply(func) function to kick off a training job for each group of data in parallel. We would not be able to use this api with mlib as the function func is executed on the worker where a spark context is not found.\n",
    "\n",
    "We can however manage the parallelism outside spark from the driver node. We can do this using multithreading or multiprocessing. Before we get into those topics, we need a refresher on operating system design: An operating system is in charge of running processes. The OS allocates memory and CPU resources for the process. Once allocated the process can utilize the resources as it likes. Multiprocessing is when a program asks the OS to spin up multiple \"subprocesses\" each with their own separate memory and cpu allocations. Multithreading is when a single process created multiple threads to execute work in parrallel instead of multiple processes. Multithreading allows for threads to share memory and cpu resources allocated to the process. Multiprocessing does not.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Create SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a helper module\n",
    "import importlib.util\n",
    "spec = importlib.util.spec_from_file_location(\"spark_helper\", \"../../../Utilities/spark_helper.py\")\n",
    "spark_helper = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(spark_helper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting SPARK_HOME\n",
      "/opt/spark\n",
      "\n",
      "Running findspark.init() function\n",
      "['/opt/spark/python', '/opt/spark/python/lib/py4j-0.10.9-src.zip', '/usr/lib64/python36.zip', '/usr/lib64/python3.6', '/usr/lib64/python3.6/lib-dynload', '', '/usr/local/lib64/python3.6/site-packages', '/usr/local/lib/python3.6/site-packages', '/usr/lib64/python3.6/site-packages', '/usr/lib/python3.6/site-packages', '/usr/local/lib/python3.6/site-packages/IPython/extensions', '/root/.ipython']\n",
      "\n",
      "Setting PYSPARK_PYTHON\n",
      "/usr/bin/python3\n",
      "\n",
      "Determining IP Of Server\n",
      "The ip was detected as: 15.4.12.12\n",
      "\n",
      "Configuring URL for kubernetes master\n",
      "k8s://https://15.4.7.11:6443\n",
      "\n",
      "Creating Spark Session\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "spark_app_name = \"spark-jupyter-mlib\"\n",
    "docker_image = \"tschneider/pyspark:v5\"\n",
    "k8_master_ip = \"15.4.7.11\"\n",
    "spark_session = spark_helper.create_spark_session(spark_app_name, docker_image, k8_master_ip)\n",
    "sc = spark_session.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                         READY     STATUS    RESTARTS   AGE\n",
      "spark-jupyter-mlib-64173c7da4cf89cf-exec-1   1/1       Running   0          2m\n",
      "spark-jupyter-mlib-64173c7da4cf89cf-exec-2   1/1       Running   0          2m\n",
      "spark-jupyter-mlib-64173c7da4cf89cf-exec-3   1/1       Running   0          2m\n"
     ]
    }
   ],
   "source": [
    "! kubectl -n spark get pods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Setup Datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symlink exists at /nasdaq_2019.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "csv_file_name = \"nasdaq_2019.csv\"\n",
    "csv_relative_file_path = \"../../../Example Data Sets/{0}\".format(csv_file_name)\n",
    "csv_absolute_file_path = os.path.abspath(csv_relative_file_path)\n",
    "csv_link_path = \"/{0}\".format(csv_file_name)\n",
    "\n",
    "if os.path.exists(csv_link_path):\n",
    "    if os.path.islink(csv_link_path):\n",
    "        print(\"Symlink exists at {0}\".format(csv_link_path))\n",
    "    elif os.path.isfile(file):\n",
    "        print(\"File exists at {0}\".format(csv_link_path))\n",
    "    else:\n",
    "        raise Exception(\"Something is wrong. An object exists where we want to create a symlink.\")\n",
    "else:\n",
    "    os.symlink(csv_absolute_file_path, csv_link_path)\n",
    "    print(\"Symlink created as {0} -> {1}\".format(csv_link_path, csv_absolute_file_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. Create web server to host data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Determine the current working directory. \n",
    "\n",
    "Note: There is a trick to doing this inside a jupyter notebook and so we will use a special library to get that information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/ml-training-jupyter-notebooks\n"
     ]
    }
   ],
   "source": [
    "import pyprojroot\n",
    "project_root_dir  = pyprojroot.here()\n",
    "print(project_root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Load the module for the webserver from our utilities directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the module for the web server we wrote\n",
    "import importlib.util\n",
    "spec = importlib.util.spec_from_file_location(\"PythonHttpFileServer\", \"../../../Utilities/PythonHttpFileServer.py\")\n",
    "PythonHttpFileServer = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(PythonHttpFileServer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Configure logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the logger and log level\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Start the webserver in a new thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_sub_dir = \"Example Data Sets\"\n",
    "web_root = os.path.join(project_root_dir, data_sub_dir)\n",
    "\n",
    "if not os.path.exists(web_root):\n",
    "    raise Exception(\"The web root for the server does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Starting server on port 80\n",
      "INFO:root:Web root specified as: /root/ml-training-jupyter-notebooks/Example Data Sets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app 'PythonHttpFileServer' (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:werkzeug: * Running on all addresses.\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "INFO:werkzeug: * Running on http://15.4.12.12:80/ (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "# Start the webserver in a thread so the cell is not stuck in a running state\n",
    "import threading\n",
    "\n",
    "var_exists = 'web_server_thread' in locals() or 'web_server_thread' in globals()\n",
    "if not var_exists:\n",
    "    web_server_port = 80\n",
    "    web_server_args = (web_server_port, web_root)\n",
    "    web_server_thread = threading.Thread(target=PythonHttpFileServer.run_server, args=web_server_args)\n",
    "    web_server_thread.start()\n",
    "else:\n",
    "    print(\"Web Server thread already exists\")\n",
    "    print(\"To kill it you need to restart the kernel.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Load The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Add the file using Spark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Get /root/ml-training-jupyter-notebooks/Example Data Sets/nasdaq_2019.csv\n",
      "INFO:werkzeug:15.4.12.12 - - [10/Dec/2021 14:54:20] \"GET /nasdaq_2019.csv HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading file 'http://15.4.12.12:80/nasdaq_2019.csv' to Spark cluster.\n"
     ]
    }
   ],
   "source": [
    "ip_address = spark_helper.determine_ip_address()\n",
    "csv_file_name = \"nasdaq_2019.csv\"\n",
    "csv_file_url = \"http://{0}:{1}/{2}\".format(ip_address, web_server_port, csv_file_name)\n",
    "print(\"Uploading file '{0}' to Spark cluster.\".format(csv_file_url))\n",
    "sc.addFile(csv_file_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Use koalas to open the file on spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the utility function to convert a date string to a datetime object from our utilities module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the utilities module we wrote\n",
    "import importlib.util\n",
    "spec = importlib.util.spec_from_file_location(\"utilities\", \"../../../Utilities/utilities.py\")\n",
    "utilities = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(utilities)\n",
    "\n",
    "# Define a mapping to convert our data field to the correct type\n",
    "converter_mapping = {\n",
    "    \"date\": utilities.convert_date_string_to_date\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Load our OHCLV data Into a koalas dataframe and pull out a single day in the say way we would in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoid a warning\n",
    "import os\n",
    "os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\n",
    "os.environ[\"SPARK_KOALAS_AUTOPATCH\"] = \"0\"\n",
    "\n",
    "from databricks import koalas\n",
    "koalas_dataframe = koalas.read_csv(u\"file:////nasdaq_2019.csv\", converters=converter_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>interval</th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AABA</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>70.90</td>\n",
       "      <td>71.5200</td>\n",
       "      <td>70.3250</td>\n",
       "      <td>70.57</td>\n",
       "      <td>10234800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAL</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>33.14</td>\n",
       "      <td>33.6632</td>\n",
       "      <td>32.5301</td>\n",
       "      <td>32.88</td>\n",
       "      <td>8995100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAME</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>2.43</td>\n",
       "      <td>2.4300</td>\n",
       "      <td>2.4000</td>\n",
       "      <td>2.40</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAOI</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>10.70</td>\n",
       "      <td>10.8900</td>\n",
       "      <td>10.0100</td>\n",
       "      <td>10.18</td>\n",
       "      <td>883100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAON</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>50.57</td>\n",
       "      <td>50.9850</td>\n",
       "      <td>48.5600</td>\n",
       "      <td>49.73</td>\n",
       "      <td>180200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker interval        date   open     high      low  close    volume\n",
       "0   AABA        D  2019-07-01  70.90  71.5200  70.3250  70.57  10234800\n",
       "1    AAL        D  2019-07-01  33.14  33.6632  32.5301  32.88   8995100\n",
       "2   AAME        D  2019-07-01   2.43   2.4300   2.4000   2.40       500\n",
       "3   AAOI        D  2019-07-01  10.70  10.8900  10.0100  10.18    883100\n",
       "4   AAON        D  2019-07-01  50.57  50.9850  48.5600  49.73    180200"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "koalas_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Set some options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "koalas.set_option('compute.ops_on_diff_frames', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Submit Python Code To Spark Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section of the notebook we are going to apply the kmeans algorithm from sklearn to each date in our koalas_dataframe object.\n",
    "To do this, we are going to write a function that applies the algorithm to a dataframe; the assumption being the dataframe only contains data related to the same date.\n",
    "Note: Most of this is a review and reworking of the content contained in \n",
    "<a href=\"../Cluster%20Analysis/K-Means.ipynb\">Cluster Analysis/K-Means.ipynb</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Setup Utility Function\n",
    "We create our data frame for testing based on a subset of our real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>interval</th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>96799</th>\n",
       "      <td>AABA</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>56.78</td>\n",
       "      <td>58.01</td>\n",
       "      <td>56.47</td>\n",
       "      <td>57.49</td>\n",
       "      <td>10532400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96800</th>\n",
       "      <td>AAL</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>31.46</td>\n",
       "      <td>32.65</td>\n",
       "      <td>31.05</td>\n",
       "      <td>32.48</td>\n",
       "      <td>5229400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96801</th>\n",
       "      <td>AAME</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>2.43</td>\n",
       "      <td>2.49</td>\n",
       "      <td>2.43</td>\n",
       "      <td>2.49</td>\n",
       "      <td>1700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96802</th>\n",
       "      <td>AAOI</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>15.00</td>\n",
       "      <td>16.29</td>\n",
       "      <td>14.85</td>\n",
       "      <td>15.88</td>\n",
       "      <td>478300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96803</th>\n",
       "      <td>AAON</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>34.57</td>\n",
       "      <td>35.40</td>\n",
       "      <td>34.37</td>\n",
       "      <td>35.07</td>\n",
       "      <td>124800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ticker interval        date   open   high    low  close    volume\n",
       "96799   AABA        D  2019-01-02  56.78  58.01  56.47  57.49  10532400\n",
       "96800    AAL        D  2019-01-02  31.46  32.65  31.05  32.48   5229400\n",
       "96801   AAME        D  2019-01-02   2.43   2.49   2.43   2.49      1700\n",
       "96802   AAOI        D  2019-01-02  15.00  16.29  14.85  15.88    478300\n",
       "96803   AAON        D  2019-01-02  34.57  35.40  34.37  35.07    124800"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_01_02_2019 = koalas_dataframe.loc[koalas_dataframe[\"date\"] == '2019-01-02'].copy()\n",
    "df_01_02_2019.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then write and test our function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import pyspark\n",
    "from pyspark.sql.functions import pandas_udf, udf\n",
    "from pyspark.sql.types import *\n",
    "from databricks import koalas\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "def perform_kmeans_on_dataframe(df, column_names):\n",
    "    \n",
    "    # Create a copy of our dataframe so we can play around\n",
    "    tmp = df.copy()\n",
    "    columns = column_names.copy()\n",
    "    \n",
    "    # Create our model\n",
    "    model = KMeans().setK(5).setSeed(42)\n",
    "\n",
    "    # Do some magic to get the data in the right format for the spark model\n",
    "    from pyspark.ml.feature import VectorAssembler\n",
    "    assembler = VectorAssembler(inputCols=column_names, outputCol=\"features\")\n",
    "    if type(tmp) == koalas.frame.DataFrame:\n",
    "        model_parameters = assembler.transform(tmp[[*columns]].to_spark())\n",
    "    elif type(tmp) == pandas.DataFrame:\n",
    "        tmp = koalas.DataFrame(tmp)\n",
    "        model_parameters = assembler.transform(tmp[[*columns]].to_spark())\n",
    "\n",
    "    # Train the model\n",
    "    trained_model = model.fit(model_parameters)\n",
    "    \n",
    "    # Extract the cluster information for the training data\n",
    "    predictions = trained_model.transform(model_parameters)\n",
    "    cluster_indices = predictions.select(\"prediction\")\n",
    "    cluster_indices = koalas.DataFrame(cluster_indices).to_numpy().reshape(-1)\n",
    "    cluster_indices = koalas.Series(cluster_indices, index=tmp.index.to_numpy())\n",
    "    tmp[\"cluster_indices\"] = cluster_indices\n",
    "    cluster_centroids = trained_model.clusterCenters()\n",
    "    tmp[\"cluster_centroids\"] = tmp[\"cluster_indices\"].apply(lambda i: str(cluster_centroids[i]))\n",
    "\n",
    "    return tmp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/python/pyspark/sql/pandas/functions.py:392: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  \"in the future releases. See SPARK-28264 for more details.\", UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>interval</th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>cluster_indices</th>\n",
       "      <th>cluster_centroids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>96829</th>\n",
       "      <td>ACLS</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>17.48</td>\n",
       "      <td>18.14</td>\n",
       "      <td>16.920</td>\n",
       "      <td>17.79</td>\n",
       "      <td>284200</td>\n",
       "      <td>0</td>\n",
       "      <td>[12.97707127 13.2648608 ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96923</th>\n",
       "      <td>ALLK</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>51.26</td>\n",
       "      <td>53.94</td>\n",
       "      <td>50.115</td>\n",
       "      <td>51.99</td>\n",
       "      <td>151900</td>\n",
       "      <td>3</td>\n",
       "      <td>[66.74787778 67.59067778]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97216</th>\n",
       "      <td>BRPAU</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>10.55</td>\n",
       "      <td>10.55</td>\n",
       "      <td>10.550</td>\n",
       "      <td>10.55</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[12.97707127 13.2648608 ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97658</th>\n",
       "      <td>DWFI</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>22.21</td>\n",
       "      <td>22.24</td>\n",
       "      <td>22.200</td>\n",
       "      <td>22.20</td>\n",
       "      <td>41300</td>\n",
       "      <td>0</td>\n",
       "      <td>[12.97707127 13.2648608 ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97699</th>\n",
       "      <td>EFAS</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>15.08</td>\n",
       "      <td>15.08</td>\n",
       "      <td>15.080</td>\n",
       "      <td>15.08</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[12.97707127 13.2648608 ]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ticker interval        date   open   high     low  close  volume  cluster_indices          cluster_centroids\n",
       "96829   ACLS        D  2019-01-02  17.48  18.14  16.920  17.79  284200                0  [12.97707127 13.2648608 ]\n",
       "96923   ALLK        D  2019-01-02  51.26  53.94  50.115  51.99  151900                3  [66.74787778 67.59067778]\n",
       "97216  BRPAU        D  2019-01-02  10.55  10.55  10.550  10.55       0                0  [12.97707127 13.2648608 ]\n",
       "97658   DWFI        D  2019-01-02  22.21  22.24  22.200  22.20   41300                0  [12.97707127 13.2648608 ]\n",
       "97699   EFAS        D  2019-01-02  15.08  15.08  15.080  15.08       0                0  [12.97707127 13.2648608 ]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perform_kmeans_on_dataframe(df_01_02_2019, column_names=[\"open\", \"close\"]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/python/pyspark/sql/pandas/functions.py:392: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  \"in the future releases. See SPARK-28264 for more details.\", UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>interval</th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>cluster_indices</th>\n",
       "      <th>cluster_centroids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>96829</th>\n",
       "      <td>ACLS</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>17.48</td>\n",
       "      <td>18.14</td>\n",
       "      <td>16.920</td>\n",
       "      <td>17.79</td>\n",
       "      <td>284200</td>\n",
       "      <td>1</td>\n",
       "      <td>[13.76070419 14.05760128]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96923</th>\n",
       "      <td>ALLK</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>51.26</td>\n",
       "      <td>53.94</td>\n",
       "      <td>50.115</td>\n",
       "      <td>51.99</td>\n",
       "      <td>151900</td>\n",
       "      <td>0</td>\n",
       "      <td>[73.51000825 74.42198144]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97216</th>\n",
       "      <td>BRPAU</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>10.55</td>\n",
       "      <td>10.55</td>\n",
       "      <td>10.550</td>\n",
       "      <td>10.55</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[13.76070419 14.05760128]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97658</th>\n",
       "      <td>DWFI</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>22.21</td>\n",
       "      <td>22.24</td>\n",
       "      <td>22.200</td>\n",
       "      <td>22.20</td>\n",
       "      <td>41300</td>\n",
       "      <td>1</td>\n",
       "      <td>[13.76070419 14.05760128]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97699</th>\n",
       "      <td>EFAS</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>15.08</td>\n",
       "      <td>15.08</td>\n",
       "      <td>15.080</td>\n",
       "      <td>15.08</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[13.76070419 14.05760128]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ticker interval        date   open   high     low  close  volume  cluster_indices          cluster_centroids\n",
       "96829   ACLS        D  2019-01-02  17.48  18.14  16.920  17.79  284200                1  [13.76070419 14.05760128]\n",
       "96923   ALLK        D  2019-01-02  51.26  53.94  50.115  51.99  151900                0  [73.51000825 74.42198144]\n",
       "97216  BRPAU        D  2019-01-02  10.55  10.55  10.550  10.55       0                1  [13.76070419 14.05760128]\n",
       "97658   DWFI        D  2019-01-02  22.21  22.24  22.200  22.20   41300                1  [13.76070419 14.05760128]\n",
       "97699   EFAS        D  2019-01-02  15.08  15.08  15.080  15.08       0                1  [13.76070419 14.05760128]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perform_kmeans_on_dataframe(df_01_02_2019.to_pandas(), column_names=[\"open\", \"close\"]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Run Utility Function In Parrallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the utility function has been tested on smaller dataframes, we can run it on a large data set. We will time ourselves when running against a single day vs all days to show the operations are occurring in parrallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2019-01-01'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a list of the dates in our dataframe\n",
    "dates = koalas_dataframe[\"date\"].unique().sort_values().to_numpy()\n",
    "date1 = dates[0]\n",
    "date1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "del koalas_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_01_02_2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to train our kmeans model which will be executed in each thread\n",
    "import time\n",
    "\n",
    "def thread_func(params, retries=5):\n",
    "   \n",
    "    # Retrieve params\n",
    "    date = params[0]\n",
    "    progress_bar = params[1]\n",
    "    lock = params[2]\n",
    "    result_file_path = params[3]\n",
    "    thread_result = None\n",
    "    try:\n",
    "        # Load data\n",
    "        thread_df = koalas.read_csv(u\"file:////nasdaq_2019.csv\", converters=converter_mapping)\n",
    "                \n",
    "        while True:\n",
    "            try:\n",
    "                # Train the model\n",
    "                date_df = thread_df.loc[thread_df[\"date\"] == date]    \n",
    "                thread_result = perform_kmeans_on_dataframe(date_df, column_names=[\"open\", \"close\"])\n",
    "\n",
    "                # Force spark to not be lazy and to do the computation\n",
    "                thread_result.shape  \n",
    "\n",
    "                # Return results and timing info to the ThreadHelper\n",
    "                return thread_result\n",
    "\n",
    "            except Exception as e:\n",
    "                retries -= 1\n",
    "                if retries > 0:\n",
    "                    time.sleep(1)\n",
    "                else:\n",
    "                    raise e\n",
    "    finally:  \n",
    "        # Update the progress bar\n",
    "        lock.acquire()\n",
    "        if thread_result is not None:\n",
    "            thread_result.to_pandas().to_csv(result_file_path, mode='a')\n",
    "        global completed_ops\n",
    "        completed_ops += 1\n",
    "        progress_bar.update(completed_ops)\n",
    "        lock.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import progressbar\n",
    "\n",
    "def create_progress_bar(num_ops):\n",
    "\n",
    "    progress_bar_widgets = [\n",
    "        progressbar.Bar('=', '[', ']'), \n",
    "        ' ', \n",
    "        progressbar.FormatLabel('Processed: %(value)d / {0} ops'.format(num_ops)),\n",
    "        ' ', \n",
    "        progressbar.ETA()\n",
    "    ]\n",
    "    return  progressbar.ProgressBar(maxval=num_ops, widgets=progress_bar_widgets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[                                         ] Processed: 0 / 2 ops ETA:  --:--:--\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting: 2021-12-10 15:00:12.819677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/python/pyspark/sql/pandas/functions.py:392: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  \"in the future releases. See SPARK-28264 for more details.\", UserWarning)\n",
      "[=====================                     ] Processed: 1 / 2 ops ETA:  0:00:49\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending: 2021-12-10 15:01:02.082435\n",
      "Total calculation time: 49.262758s\n",
      "Load results\n",
      "Ending: 2021-12-10 15:01:03.907149\n",
      "Total Merge time: 1.824714s\n",
      "Total wall time: 51.087472s\n",
      "Time per date: 51.087472s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>interval</th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>93620</th>\n",
       "      <td>AABA</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>57.94</td>\n",
       "      <td>57.94</td>\n",
       "      <td>57.94</td>\n",
       "      <td>57.94</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93621</th>\n",
       "      <td>AAL</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>32.11</td>\n",
       "      <td>32.11</td>\n",
       "      <td>32.11</td>\n",
       "      <td>32.11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93622</th>\n",
       "      <td>AAME</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>2.41</td>\n",
       "      <td>2.41</td>\n",
       "      <td>2.41</td>\n",
       "      <td>2.41</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93623</th>\n",
       "      <td>AAOI</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>15.43</td>\n",
       "      <td>15.43</td>\n",
       "      <td>15.43</td>\n",
       "      <td>15.43</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93624</th>\n",
       "      <td>AAON</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>35.06</td>\n",
       "      <td>35.06</td>\n",
       "      <td>35.06</td>\n",
       "      <td>35.06</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ticker interval        date   open   high    low  close  volume\n",
       "93620   AABA        D  2019-01-01  57.94  57.94  57.94  57.94       0\n",
       "93621    AAL        D  2019-01-01  32.11  32.11  32.11  32.11       0\n",
       "93622   AAME        D  2019-01-01   2.41   2.41   2.41   2.41       0\n",
       "93623   AAOI        D  2019-01-01  15.43  15.43  15.43  15.43       0\n",
       "93624   AAON        D  2019-01-01  35.06  35.06  35.06  35.06       0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a ThreadPool and kick off the parrallel training sessions\n",
    "from multiprocessing.pool import ThreadPool\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "import threading\n",
    "\n",
    "# Record the start time\n",
    "start = datetime.now()\n",
    "print(\"Starting: {0}\".format(start))\n",
    "\n",
    "# Create objects to help track multithreading progress\n",
    "num_ops = len(dates[0:1])\n",
    "bar = create_progress_bar(num_ops*2)\n",
    "bar.start()\n",
    "global completed_ops\n",
    "completed_ops = 0\n",
    "\n",
    "# Create vars to help with multi-threading\n",
    "mutex = threading.Lock()\n",
    "num_threads = 10\n",
    "thread_pool = ThreadPool(num_threads)\n",
    "\n",
    "# Create/cleanup a datafile to store results from threads\n",
    "import os\n",
    "result_file_path = \"results.csv\"\n",
    "if os.path.exists(result_file_path):\n",
    "    print(\"Cleaning up results\")\n",
    "    os.remove(result_file_path)\n",
    "\n",
    "# Create an iterator of params for the thread function\n",
    "iterator = zip(dates[0:1], \n",
    "               itertools.repeat(bar), \n",
    "               itertools.repeat(mutex), \n",
    "               itertools.repeat(result_file_path))\n",
    "\n",
    "# Run training sessions for each data in parrallel\n",
    "results = thread_pool.map(thread_func, iterator)\n",
    "\n",
    "# Record the end time\n",
    "calc_end = datetime.now()\n",
    "calc_diff = (calc_end - start).total_seconds()\n",
    "print(\"Ending: {0}\".format(calc_end))\n",
    "print(\"Total calculation time: {0}s\".format(calc_diff))\n",
    "\n",
    "# Merge the results\n",
    "print(\"Load results\")\n",
    "merged_df = koalas.read_csv(result_file_path, converters=converter_mapping)\n",
    "\n",
    "# Record the end time\n",
    "merge_end = datetime.now()\n",
    "merge_diff = (merge_end - calc_end).total_seconds()\n",
    "print(\"Ending: {0}\".format(merge_end))\n",
    "print(\"Total Merge time: {0}s\".format(merge_diff)) \n",
    "\n",
    "# Print wall time\n",
    "wall = (merge_end - start).total_seconds()\n",
    "print(\"Total wall time: {0}s\".format(wall)) \n",
    "\n",
    "# Calculate the time per date\n",
    "date_diff = wall / num_ops\n",
    "print(\"Time per date: {0}s\".format(date_diff))\n",
    "\n",
    "# Show the union df\n",
    "merged_df.loc[merged_df[\"date\"] == dates[0]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If anything went wrong, we should ask the SparkContext to kill any abandoned jobs that may be lingering in ghosted threads from the thread pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.cancelAllJobs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run a large set of dates using our threadpool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a ThreadPool and kick off the parrallel training sessions\n",
    "from multiprocessing.pool import ThreadPool\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "import threading\n",
    "\n",
    "# Record the start time\n",
    "start = datetime.now()\n",
    "print(\"Starting: {0}\".format(start))\n",
    "\n",
    "# Create objects to help track multithreading progress\n",
    "num_ops = len(dates)\n",
    "bar = create_progress_bar(num_ops*2)\n",
    "bar.start()\n",
    "global completed_ops\n",
    "completed_ops = 0\n",
    "\n",
    "# Create vars to help with multi-threading\n",
    "mutex = threading.Lock()\n",
    "num_threads = 10\n",
    "thread_pool = ThreadPool(num_threads)\n",
    "\n",
    "# Create/cleanup a datafile to store results from threads\n",
    "import os\n",
    "result_file_path = \"results.csv\"\n",
    "if os.path.exists(result_file_path):\n",
    "    print(\"Cleaning up results\")\n",
    "    os.remove(result_file_path)\n",
    "\n",
    "# Create an iterator of params for the thread function\n",
    "iterator = zip(dates, \n",
    "               itertools.repeat(bar), \n",
    "               itertools.repeat(mutex), \n",
    "               itertools.repeat(result_file_path))\n",
    "\n",
    "# Run training sessions for each data in parrallel\n",
    "results = thread_pool.map(thread_func, iterator)\n",
    "\n",
    "# Record the end time\n",
    "calc_end = datetime.now()\n",
    "calc_diff = (calc_end - start).total_seconds()\n",
    "print(\"Ending: {0}\".format(calc_end))\n",
    "print(\"Total calculation time: {0}s\".format(calc_diff))\n",
    "\n",
    "# Merge the results\n",
    "print(\"Load results\")\n",
    "merged_df = koalas.read_csv(u\"file:////nasdaq_2019.csv\", converters=converter_mapping)\n",
    "\n",
    "# Record the end time\n",
    "merge_end = datetime.now()\n",
    "merge_diff = (merge_end - calc_end).total_seconds()\n",
    "print(\"Ending: {0}\".format(merge_end))\n",
    "print(\"Total Merge time: {0}s\".format(merge_diff)) \n",
    "\n",
    "# Print wall time\n",
    "wall = (merge_end - start).total_seconds()\n",
    "print(\"Total wall time: {0}s\".format(wall)) \n",
    "\n",
    "# Calculate the time per date\n",
    "date_diff = wall / num_ops\n",
    "print(\"Time per date: {0}s\".format(date_diff))\n",
    "\n",
    "# Show the union df\n",
    "merged_df.loc[merged_df[\"date\"] == dates[0]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, every time we see the csv file get reloaded, we know that a worker crashed. I was watching the linux OS hosing the kubernetes/spark workers. If the CPU/Memory got pegged at 100% utilization the worker would crash. \n",
    "\n",
    "In some cases the driver may run into an issue as follows:\n",
    "\n",
    "```\n",
    "Py4JJavaError: An error occurred while calling o689322.createOrReplaceTempView.\n",
    ": java.lang.OutOfMemoryError: Java heap space\n",
    "```\n",
    "\n",
    "For example when merging all the tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If anything went wrong, we should ask the SparkContext to kill any abandoned jobs that may be lingering in ghosted threads from the thread pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.cancelAllJobs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that computing every date is almost as fast as computing a single date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 6. Cleanup Spark Cluster On Kubernetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if os.path.exists(csv_link_path) and os.path.islink(csv_link_path):\n",
    "    os.unlink(csv_link_path)\n",
    "    print(\"Deleted symlinked data file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! kubectl -n spark get pod"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
