{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "In this notebook we are going to create a SparkContext object. As we will see, this object is our communication channel with the Apache Spark cluster. It allows us to load data and execute code on the Spark cluster.\n",
    "\n",
    "As discussed in the [README](README.md) we will see that creating the SparkContext that is configured to use kubernetes will automagically spin up a set of spark workers which run on the kubernetes cluster. The driver will run locally in \"client mode\".\n",
    "\n",
    "Spark provides a Dashboard for monitoring the work being executed on the cluster. When we create the sparkContext from our jupyter notebook, a service will be spun up to listen on port 4040 (on the machine hosting the nupyter notebook. In my case, the dashboard was available at the following URL http://15.1.1.23:4040. \n",
    "\n",
    "When the cluster is first created, we can expand the event timeline to see when the driver and executors are added to the cluster.\n",
    "\n",
    "<center><img src=\"images/spark_dashboard_event_timeline.png\" width=\"600px\"/></center>\n",
    "\n",
    "Once jobs have been submitted we can see the jobs that are/have run. It resembles the following:\n",
    "\n",
    "<center><img src=\"images/Apache%20Spark%20Dashboard.png\" width=\"600px\"/></center>\n",
    "\n",
    "It assumes you have already read the following notebooks:\n",
    "- [Install Apache Spark Prerequisites](Install%20Apache%20Spark%20Prerequisites.ipynb)\n",
    "- [Running Apache Spark On Kubernetes](Running%20Apache%20Spark%20On%20Kubernetes.ipynb)\n",
    "\n",
    "The instructions are basically the same as [Create A SparkContext For Locally Hosted Cluster](Create%20A%20SparkContext%20For%20Locally%20Hosted%20Cluster.ipynb)\n",
    "\n",
    "## Adjenda\n",
    "1. Create SparkContext\n",
    "2. Cleanup Spark and Kubernetes\n",
    "3. Package This As A Helper Module\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Create SparKContext Object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are no pods running on our kubernetes cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl -n spark get pod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some vars to specify where the kubernetes master is\n",
    "kubernetes_master_ip = \"15.4.7.11\"\n",
    "kubernetes_master_port = \"6443\"\n",
    "spark_master_url = \"k8s://https://{0}:{1}\".format(kubernetes_master_ip, kubernetes_master_port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ip was detected as: 15.4.12.12\n"
     ]
    }
   ],
   "source": [
    "# Determine the ip address of the machine\n",
    "import netifaces\n",
    "import re\n",
    "nic_uuid = netifaces.gateways()['default'][netifaces.AF_INET][1]\n",
    "nic_details = netifaces.ifaddresses(nic_uuid)\n",
    "ip_address = None\n",
    "for i, nic_detail, in nic_details.items():\n",
    "    if all([key in nic_detail[0].keys() for key in [\"addr\", \"netmask\", \"broadcast\"]]):\n",
    "        if re.match(\"([0-9]+\\\\.)+\", nic_detail[0][\"addr\"]):\n",
    "            ip_address = nic_detail[0][\"addr\"]\n",
    "            break\n",
    "print(\"The ip was detected as: {0}\".format(ip_address))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x7fc9eff9dca0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wire up the SparkConf object\n",
    "sparkConf = pyspark.SparkConf()\n",
    "sparkConf.setMaster(spark_master_url)\n",
    "\n",
    "sparkConf.setAppName(\"spark-jupyter-win\")\n",
    "\n",
    "sparkConf.set(\"spark.submit.deploy.mode\", \"cluster\")\n",
    "sparkConf.set(\"spark.kubernetes.container.image\", \"tschneider/apache-spark-k8:v7\") \n",
    "sparkConf.set(\"spark.kubernetes.namespace\", \"spark\")\n",
    "sparkConf.set(\"spark.kubernetes.pyspark.pythonVersion\", \"3\")\n",
    "sparkConf.set(\"spark.kubernetes.authenticate.driver.serviceAccountName\", \"spark-sa\")\n",
    "sparkConf.set(\"spark.kubernetes.authenticate.serviceAccountName\", \"spark-sa\")\n",
    "\n",
    "sparkConf.set(\"spark.executor.instances\", \"3\")\n",
    "sparkConf.set(\"spark.executor.cores\", \"2\")\n",
    "sparkConf.set(\"spark.executor.memory\", \"1024m\")\n",
    "sparkConf.set(\"spark.driver.memory\", \"1024m\")\n",
    "\n",
    "# If we are not using a hostname registered with a dns server, we need to set this parameter\n",
    "sparkConf.set(\"spark.driver.host\", ip_address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('spark.master', 'k8s://https://15.4.7.11:6443'), ('spark.app.name', 'spark-jupyter-win'), ('spark.submit.deploy.mode', 'cluster'), ('spark.kubernetes.container.image', 'tschneider/apache-spark-k8:v7'), ('spark.kubernetes.namespace', 'spark'), ('spark.kubernetes.pyspark.pythonVersion', '3'), ('spark.kubernetes.authenticate.driver.serviceAccountName', 'spark-sa'), ('spark.kubernetes.authenticate.serviceAccountName', 'spark-sa'), ('spark.executor.instances', '3'), ('spark.executor.cores', '2'), ('spark.executor.memory', '1024m'), ('spark.driver.memory', '1024m'), ('spark.driver.host', '15.4.12.12')])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkConf.getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/15 17:18:27 WARN Utils: Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 15.4.12.12 instead (on interface eth0)\n",
      "22/01/15 17:18:27 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/lib/spark-3.1.1-bin-hadoop2.7/jars/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/01/15 17:18:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at kubernetes to see that out worker nodes were created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                        READY     STATUS    RESTARTS   AGE\n",
      "spark-jupyter-win-f6bab17e5ebf7511-exec-1   1/1       Running   0          26m\n",
      "spark-jupyter-win-f6bab17e5ebf7511-exec-2   1/1       Running   0          26m\n",
      "spark-jupyter-win-f6bab17e5ebf7511-exec-3   1/1       Running   0          26m\n"
     ]
    }
   ],
   "source": [
    "! kubectl -n spark get pod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Cleanup Spark Cluster On Kubernetes\n",
    "When done working with spark we need to cleanup we kubernetes objects that were dynamically created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                        READY     STATUS        RESTARTS   AGE\n",
      "spark-jupyter-win-f6bab17e5ebf7511-exec-1   1/1       Terminating   0          26m\n",
      "spark-jupyter-win-f6bab17e5ebf7511-exec-2   1/1       Terminating   0          26m\n",
      "spark-jupyter-win-f6bab17e5ebf7511-exec-3   1/1       Terminating   0          26m\n"
     ]
    }
   ],
   "source": [
    "! kubectl -n spark get pod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Package This As A Helper Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have packaged the code above into a helper module. We can include this module as a way to have this code execute in a neat and standard way. Simply execute the following in a cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a helper module\n",
    "import importlib.util\n",
    "spec = importlib.util.spec_from_file_location(\"spark_helper\", \"../../../Utilities/spark_helper.py\")\n",
    "spark_helper = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(spark_helper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting SPARK_HOME\n",
      "/usr/lib/spark-3.1.1-bin-hadoop2.7\n",
      "\n",
      "Running findspark.init() function\n",
      "['/usr/lib/spark-3.1.1-bin-hadoop2.7/python', '/usr/lib/spark-3.1.1-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip', '/ml-training-jupyter-notebooks/Machine Learning/Big Data And Big Compute/Apache Spark', '/tmp/spark-b3403cd8-7c96-44ec-bcdc-e435cd31216e/userFiles-4d235b3e-7195-4ca3-8159-d24ec4ddbd04', '/usr/local/lib/python39.zip', '/usr/local/lib/python3.9', '/usr/local/lib/python3.9/lib-dynload', '', '/usr/local/lib/python3.9/site-packages']\n",
      "\n",
      "Setting PYSPARK_PYTHON\n",
      "/usr/local/bin/python3\n",
      "\n",
      "Configuring URL for kubernetes master\n",
      "k8s://https://15.4.7.11:6443\n",
      "\n",
      "Determining IP Of Server\n",
      "The ip was detected as: 15.4.12.12\n",
      "\n",
      "Creating SparkConf Object\n",
      "('spark.executor.instances', '3')\n",
      "('spark.kubernetes.container.image', 'tschneider/pyspark:v5')\n",
      "('spark.app.name', 'spark-jupyter-win')\n",
      "('spark.driver.host', '15.4.12.12')\n",
      "('spark.executor.memoryOverhead', '1024m')\n",
      "('spark.driver.memory', '1024m')\n",
      "('spark.executor.cores', '2')\n",
      "('spark.kubernetes.pyspark.pythonVersion', '3')\n",
      "('spark.executor.memory', '4096m')\n",
      "('spark.kubernetes.namespace', 'spark')\n",
      "('spark.kubernetes.authenticate.serviceAccountName', 'spark-sa')\n",
      "('spark.submit.deploy.mode', 'cluster')\n",
      "('spark.master', 'k8s://https://15.4.7.11:6443')\n",
      "('spark.submit.pyFiles', '')\n",
      "('spark.files.overwrite', 'true')\n",
      "('spark.submit.deployMode', 'client')\n",
      "('spark.kubernetes.authenticate.driver.serviceAccountName', 'spark-sa')\n",
      "('spark.ui.showConsoleProgress', 'true')\n",
      "('spark.files.useFetchCache', 'false')\n",
      "\n",
      "Creating SparkSession Object\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "spark_app_name = \"spark-jupyter-win\"\n",
    "docker_image = \"tschneider/pyspark:v5\"\n",
    "k8_master_ip = \"15.4.7.11\"\n",
    "spark_session = spark_helper.create_spark_session(spark_app_name, docker_image, k8_master_ip)\n",
    "sc = spark_session.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When done working with spark we need to cleanup we kubernetes objects that were dynamically created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
