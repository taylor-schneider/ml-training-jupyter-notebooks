{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "In this notebook we are going to create a SparkContext object. As we will see, this object is our communication channel with the Apache Spark cluster. It allows us to load data and execute code on the Spark cluster.\n",
    "\n",
    "As discussed in the [README](README.md) we will see that creating the SparkContext that is configured to use kubernetes will automagically spin up a set of spark workers which run on the kubernetes cluster. The driver will run locally in \"client mode\".\n",
    "\n",
    "Spark provides a Dashboard for monitoring the work being executed on the cluster. When we create the sparkContext from our jupyter notebook, a service will be spun up to listen on port 4040 (on the machine hosting the nupyter notebook. In my case, the dashboard was available at the following URL http://15.1.1.23:4040 but we will see that we can query this information from our spark objects once they are created.\n",
    "\n",
    "When the cluster is first created, we can expand the event timeline to see when the driver and executors are added to the cluster.\n",
    "\n",
    "<center><img src=\"images/spark_dashboard_event_timeline.png\" width=\"600px\"/></center>\n",
    "\n",
    "Once jobs have been submitted we can see the jobs that are/have run. It resembles the following:\n",
    "\n",
    "<center><img src=\"images/Apache%20Spark%20Dashboard.png\" width=\"600px\"/></center>\n",
    "\n",
    "It assumes you have already read the following notebooks:\n",
    "- [Install Apache Spark Prerequisites](Install%20Apache%20Spark%20Prerequisites.ipynb)\n",
    "- [Running Apache Spark On Kubernetes](Running%20Apache%20Spark%20On%20Kubernetes.ipynb)\n",
    "\n",
    "The instructions are basically the same as [Create A SparkContext For Locally Hosted Cluster](Create%20A%20SparkContext%20For%20Locally%20Hosted%20Cluster.ipynb)\n",
    "\n",
    "## Adjenda\n",
    "1. Create SparkContext\n",
    "2. Cleanup Spark and Kubernetes\n",
    "3. Package This As A Helper Module\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Create SparKContext Object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we are running spark on kubernetes. As such we will need to check whether kubernetes is running any spark pods already. Sometimes kubernetes will be too busy to do work for us because someone else is already using the cluster. We can check using the CLI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl -n spark get pod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command above output no text which tells us there are no pods running in the spark namespace. This is good. We are ready to create the spark context.\n",
    "\n",
    "Before creating the spark context we will need to set/optain a few variables to help configure our connection to the kubernetes cluster and the container we would like to use for our spark workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some vars to specify where the kubernetes master is\n",
    "kubernetes_master_ip = \"15.4.7.11\"\n",
    "kubernetes_master_port = \"6443\"\n",
    "spark_master_url = \"k8s://https://{0}:{1}\".format(kubernetes_master_ip, kubernetes_master_port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ip was detected as: 15.4.12.12\n"
     ]
    }
   ],
   "source": [
    "# Determine the ip address of the machine\n",
    "import netifaces\n",
    "import re\n",
    "nic_uuid = netifaces.gateways()['default'][netifaces.AF_INET][1]\n",
    "nic_details = netifaces.ifaddresses(nic_uuid)\n",
    "ip_address = None\n",
    "for i, nic_detail, in nic_details.items():\n",
    "    if all([key in nic_detail[0].keys() for key in [\"addr\", \"netmask\", \"broadcast\"]]):\n",
    "        if re.match(\"([0-9]+\\\\.)+\", nic_detail[0][\"addr\"]):\n",
    "            ip_address = nic_detail[0][\"addr\"]\n",
    "            break\n",
    "print(\"The ip was detected as: {0}\".format(ip_address))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have this information we can create the configuration object which will configure our spark context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x7faaa6c6b0d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wire up the SparkConf object\n",
    "sparkConf = pyspark.SparkConf()\n",
    "sparkConf.setMaster(spark_master_url)\n",
    "\n",
    "sparkConf.setAppName(\"spark-jupyter-win\")\n",
    "\n",
    "sparkConf.set(\"spark.submit.deploy.mode\", \"cluster\")\n",
    "sparkConf.set(\"spark.kubernetes.container.image\", \"tschneider/apache-spark-k8:v7\") \n",
    "sparkConf.set(\"spark.kubernetes.namespace\", \"spark\")\n",
    "sparkConf.set(\"spark.kubernetes.pyspark.pythonVersion\", \"3\")\n",
    "sparkConf.set(\"spark.kubernetes.authenticate.driver.serviceAccountName\", \"spark-sa\")\n",
    "sparkConf.set(\"spark.kubernetes.authenticate.serviceAccountName\", \"spark-sa\")\n",
    "\n",
    "sparkConf.set(\"spark.executor.instances\", \"3\")\n",
    "sparkConf.set(\"spark.executor.cores\", \"2\")\n",
    "sparkConf.set(\"spark.executor.memory\", \"1024m\")\n",
    "sparkConf.set(\"spark.driver.memory\", \"1024m\")\n",
    "\n",
    "# If we are not using a hostname registered with a dns server, we need to set this parameter\n",
    "sparkConf.set(\"spark.driver.host\", ip_address)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the spark configuration item to create a spark session and a spark context.\n",
    "\n",
    "**Note**: This step may take some time. It is going to instanciate containers on the kubernetes cluster and start the spark service in them. If we haven't downloaded the containers to the worker before (ie. `docker pull <my container >`) then we will have to wait while the image is pulled etc. After the initial pull we should only wait about a minute or so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/19 15:03:48 WARN Utils: Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 15.4.12.12 instead (on interface eth0)\n",
      "22/01/19 15:03:48 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/lib/spark-3.1.1-bin-hadoop2.7/jars/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/01/19 15:03:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we ever need to recall what configurations we set for the spark context we can programatically query that information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('spark.kubernetes.container.image', 'tschneider/apache-spark-k8:v7')\n",
      "('spark.executor.instances', '3')\n",
      "('spark.sql.warehouse.dir', 'file:/ml-training-jupyter-notebooks/Machine%20Learning/Big%20Data%20And%20Big%20Compute/Apache%20Spark/spark-warehouse')\n",
      "('spark.app.name', 'spark-jupyter-win')\n",
      "('spark.driver.host', '15.4.12.12')\n",
      "('spark.executor.id', 'driver')\n",
      "('spark.driver.memory', '1024m')\n",
      "('spark.executor.cores', '2')\n",
      "('spark.kubernetes.pyspark.pythonVersion', '3')\n",
      "('spark.kubernetes.namespace', 'spark')\n",
      "('spark.kubernetes.authenticate.serviceAccountName', 'spark-sa')\n",
      "('spark.submit.deploy.mode', 'cluster')\n",
      "('spark.app.id', 'spark-application-1642604633922')\n",
      "('spark.executor.memory', '1024m')\n",
      "('spark.driver.port', '39790')\n",
      "('spark.rdd.compress', 'True')\n",
      "('spark.app.startTime', '1642604630530')\n",
      "('spark.serializer.objectStreamReset', '100')\n",
      "('spark.master', 'k8s://https://15.4.7.11:6443')\n",
      "('spark.submit.pyFiles', '')\n",
      "('spark.kubernetes.authenticate.driver.serviceAccountName', 'spark-sa')\n",
      "('spark.submit.deployMode', 'client')\n",
      "('spark.kubernetes.executor.podNamePrefix', 'spark-jupyter-win-12c7727e72dd9aa2')\n",
      "('spark.ui.showConsoleProgress', 'true')\n"
     ]
    }
   ],
   "source": [
    "for item in spark.sparkContext.getConf().getAll():\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to see the url for our spark console we can again query this programatically from the spark context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://15.4.12.12:4040'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.uiWebUrl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at kubernetes to see that out worker nodes were created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                        READY     STATUS    RESTARTS   AGE\n",
      "spark-jupyter-win-12c7727e72dd9aa2-exec-1   1/1       Running   0          20s\n",
      "spark-jupyter-win-12c7727e72dd9aa2-exec-2   1/1       Running   0          19s\n",
      "spark-jupyter-win-12c7727e72dd9aa2-exec-3   1/1       Running   0          19s\n"
     ]
    }
   ],
   "source": [
    "! kubectl -n spark get pod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Cleanup Spark Cluster On Kubernetes\n",
    "When done working with spark we need to cleanup we kubernetes objects that were dynamically created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/19 15:04:15 WARN ExecutorPodsWatchSnapshotSource: Kubernetes client has been closed (this is expected if the application is shutting down.)\n"
     ]
    }
   ],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                        READY     STATUS        RESTARTS   AGE\n",
      "spark-jupyter-win-12c7727e72dd9aa2-exec-1   1/1       Terminating   0          23s\n",
      "spark-jupyter-win-12c7727e72dd9aa2-exec-2   1/1       Terminating   0          22s\n",
      "spark-jupyter-win-12c7727e72dd9aa2-exec-3   1/1       Terminating   0          22s\n"
     ]
    }
   ],
   "source": [
    "! kubectl -n spark get pod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Package This As A Helper Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have packaged the code above into a helper module. We can include this module as a way to have this code execute in a neat and standard way. Simply execute the following in a cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load a helper module\n",
    "import spark_helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting SPARK_HOME\n",
      "/usr/lib/spark-3.1.1-bin-hadoop2.7\n",
      "\n",
      "Running findspark.init() function\n",
      "['/usr/lib/spark-3.1.1-bin-hadoop2.7/python', '/usr/lib/spark-3.1.1-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip', '/ml-training-jupyter-notebooks/Machine Learning/Big Data And Big Compute/Apache Spark', '/tmp/spark-4d15631e-545e-4f65-a7e8-deac58eeef4f/userFiles-78383417-3ebb-4d42-a49d-49287e61f985', '/usr/local/lib/python39.zip', '/usr/local/lib/python3.9', '/usr/local/lib/python3.9/lib-dynload', '', '/usr/local/lib/python3.9/site-packages', '/ml-training-jupyter-notebooks/Utilities']\n",
      "\n",
      "Setting PYSPARK_PYTHON\n",
      "/usr/local/bin/python3\n",
      "\n",
      "Configuring URL for kubernetes master\n",
      "k8s://https://15.4.7.11:6443\n",
      "\n",
      "Determining IP Of Server\n",
      "The ip was detected as: 15.4.12.12\n",
      "\n",
      "Creating SparkConf Object\n",
      "('spark.executor.instances', '3')\n",
      "('spark.kubernetes.container.image', 'tschneider/pyspark:v5')\n",
      "('spark.app.name', 'spark-jupyter-win')\n",
      "('spark.driver.host', '15.4.12.12')\n",
      "('spark.executor.memoryOverhead', '1024m')\n",
      "('spark.driver.memory', '1024m')\n",
      "('spark.executor.cores', '2')\n",
      "('spark.kubernetes.pyspark.pythonVersion', '3')\n",
      "('spark.executor.memory', '4096m')\n",
      "('spark.kubernetes.namespace', 'spark')\n",
      "('spark.kubernetes.authenticate.serviceAccountName', 'spark-sa')\n",
      "('spark.submit.deploy.mode', 'cluster')\n",
      "('spark.master', 'k8s://https://15.4.7.11:6443')\n",
      "('spark.submit.pyFiles', '')\n",
      "('spark.files.overwrite', 'true')\n",
      "('spark.submit.deployMode', 'client')\n",
      "('spark.kubernetes.authenticate.driver.serviceAccountName', 'spark-sa')\n",
      "('spark.ui.showConsoleProgress', 'true')\n",
      "('spark.files.useFetchCache', 'false')\n",
      "\n",
      "Creating SparkSession Object\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "spark_app_name = \"spark-jupyter-win\"\n",
    "docker_image = \"tschneider/pyspark:v5\"\n",
    "k8_master_ip = \"15.4.7.11\"\n",
    "spark_session = spark_helper.create_spark_session(spark_app_name, docker_image, k8_master_ip)\n",
    "sc = spark_session.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When done working with spark we need to cleanup we kubernetes objects that were dynamically created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
