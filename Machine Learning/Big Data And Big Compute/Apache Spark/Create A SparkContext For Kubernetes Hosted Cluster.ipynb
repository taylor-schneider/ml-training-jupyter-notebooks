{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "In this notebook we are going to create a SparkContext object. As we will see, this object is our communication channel with the Apache Spark cluster. It allows us to load data and execute code on the Spark cluster.\n",
    "\n",
    "As discussed in the [README](README.md) we will see that creating the SparkContext that is configured to use kubernetes will automagically spin up a set of spark workers which run on the kubernetes cluster. The driver will run locally in \"client mode\".\n",
    "\n",
    "Spark provides a Dashboard at the following URL http://15.1.1.23:4040. Here we can see the jobs that are/have run. It resembles the following:\n",
    "\n",
    "<center><img src=\"images/Apache%20spark%20Dashboard.png\" width=\"600px\"/></center>\n",
    "\n",
    "It assumes you have already read the following notebooks:\n",
    "- [Install Apache Spark Prerequisites](Install%20Apache%20Spark%20Prerequisites.ipynb)\n",
    "- [Running Apache Spark On Kubernetes](Running%20Apache%20Spark%20On%20Kubernetes.ipynb)\n",
    "\n",
    "The instructions are basically the same as [Create A SparkContext For Locally Hosted Cluster](Create%20A%20SparkContext%20For%20Locally%20Hosted%20Cluster.ipynb)\n",
    "\n",
    "## Adjenda\n",
    "1. Create SparkContext\n",
    "2. Cleanup Spark and Kubernetes\n",
    "3. Package This As A Helper Module\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Create SparKContext Object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are no pods running on our kubernetes cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No resources found in spark namespace.\n"
     ]
    }
   ],
   "source": [
    "!kubectl -n spark get pod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some vars to specify where the kubernetes master is\n",
    "kubernetes_master_ip = \"15.4.7.11\"\n",
    "kubernetes_master_port = \"6443\"\n",
    "spark_master_url = \"k8s://https://{0}:{1}\".format(kubernetes_master_ip, kubernetes_master_port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ip was detected as: 15.1.1.23\n"
     ]
    }
   ],
   "source": [
    "# Determine the ip address of the machine\n",
    "import netifaces\n",
    "import re\n",
    "nic_uuid = netifaces.gateways()['default'][netifaces.AF_INET][1]\n",
    "nic_details = netifaces.ifaddresses(nic_uuid)\n",
    "ip_address = None\n",
    "for i, nic_detail, in nic_details.items():\n",
    "    if all([key in nic_detail[0].keys() for key in [\"addr\", \"netmask\", \"broadcast\"]]):\n",
    "        if re.match(\"([0-9]+\\\\.)+\", nic_detail[0][\"addr\"]):\n",
    "            ip_address = nic_detail[0][\"addr\"]\n",
    "            break\n",
    "print(\"The ip was detected as: {0}\".format(ip_address))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x6d59a58>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wire up the SparkConf object\n",
    "sparkConf = pyspark.SparkConf()\n",
    "sparkConf.setMaster(spark_master_url)\n",
    "\n",
    "sparkConf.setAppName(\"spark-jupyter-win\")\n",
    "\n",
    "sparkConf.set(\"spark.submit.deploy.mode\", \"cluster\")\n",
    "sparkConf.set(\"spark.kubernetes.container.image\", \"tschneider/pyspark:v2\") \n",
    "sparkConf.set(\"spark.kubernetes.namespace\", \"spark\")\n",
    "sparkConf.set(\"spark.kubernetes.pyspark.pythonVersion\", \"3\")\n",
    "sparkConf.set(\"spark.kubernetes.authenticate.driver.serviceAccountName\", \"spark-sa\")\n",
    "sparkConf.set(\"spark.kubernetes.authenticate.serviceAccountName\", \"spark-sa\")\n",
    "\n",
    "sparkConf.set(\"spark.executor.instances\", \"3\")\n",
    "sparkConf.set(\"spark.executor.cores\", \"2\")\n",
    "sparkConf.set(\"spark.executor.memory\", \"1024m\")\n",
    "sparkConf.set(\"spark.driver.memory\", \"1024m\")\n",
    "\n",
    "# If we are not using a hostname registered with a dns server, we need to set this parameter\n",
    "sparkConf.set(\"spark.driver.host\", ip_address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('spark.master', 'k8s://https://15.4.7.11:6443'), ('spark.app.name', 'spark-jupyter-win'), ('spark.submit.deploy.mode', 'cluster'), ('spark.kubernetes.container.image', 'tschneider/pyspark:v2'), ('spark.kubernetes.namespace', 'spark'), ('spark.kubernetes.pyspark.pythonVersion', '3'), ('spark.kubernetes.authenticate.driver.serviceAccountName', 'spark-sa'), ('spark.kubernetes.authenticate.serviceAccountName', 'spark-sa'), ('spark.executor.instances', '3'), ('spark.executor.cores', '2'), ('spark.executor.memory', '1024m'), ('spark.driver.memory', '1024m'), ('spark.driver.host', '15.1.1.23')])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkConf.getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at kubernetes to see that out worker nodes were created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                        READY   STATUS    RESTARTS   AGE\n",
      "spark-jupyter-win-e27df879916bbe2e-exec-1   1/1     Running   0          31s\n",
      "spark-jupyter-win-e27df879916bbe2e-exec-2   1/1     Running   0          31s\n",
      "spark-jupyter-win-e27df879916bbe2e-exec-3   1/1     Running   0          31s\n"
     ]
    }
   ],
   "source": [
    "! kubectl -n spark get pod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Cleanup Spark Cluster On Kubernetes\n",
    "When done working with spark we need to cleanup we kubernetes objects that were dynamically created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                        READY   STATUS        RESTARTS   AGE\n",
      "spark-jupyter-win-e27df879916bbe2e-exec-1   1/1     Terminating   0          34s\n",
      "spark-jupyter-win-e27df879916bbe2e-exec-2   1/1     Terminating   0          34s\n",
      "spark-jupyter-win-e27df879916bbe2e-exec-3   1/1     Terminating   0          34s\n"
     ]
    }
   ],
   "source": [
    "! kubectl -n spark get pod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Package This As A Helper Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have packaged the code above into a helper module. We can include this module as a way to have this code execute in a neat and standard way. Simply execute the following in a cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting SPARK_HOME\n",
      "c:\\spark\\spark-3.1.1-bin-hadoop2.7\n",
      "\n",
      "Running findspark.init() function\n",
      "['c:\\\\spark\\\\spark-3.1.1-bin-hadoop2.7\\\\python', 'c:\\\\spark\\\\spark-3.1.1-bin-hadoop2.7\\\\python\\\\lib\\\\py4j-0.10.9-src.zip', 'c:\\\\spark\\\\spark-3.1.1-bin-hadoop2.7\\\\python', 'C:\\\\Users\\\\Administrator\\\\AppData\\\\Local\\\\Temp\\\\spark-ec717136-dfd3-4084-9c15-d599fa3714e8\\\\userFiles-a6b56e9c-4c78-4ae2-9378-1cc43473adf2', 'c:\\\\spark\\\\spark-3.1.1-bin-hadoop2.7\\\\python\\\\lib\\\\py4j-0.10.9-src.zip', 'c:\\\\program files\\\\python36\\\\python36.zip', 'c:\\\\program files\\\\python36\\\\DLLs', 'c:\\\\program files\\\\python36\\\\lib', 'c:\\\\program files\\\\python36', '', 'c:\\\\program files\\\\python36\\\\lib\\\\site-packages', 'c:\\\\program files\\\\python36\\\\lib\\\\site-packages\\\\win32', 'c:\\\\program files\\\\python36\\\\lib\\\\site-packages\\\\win32\\\\lib', 'c:\\\\program files\\\\python36\\\\lib\\\\site-packages\\\\Pythonwin', 'c:\\\\program files\\\\python36\\\\lib\\\\site-packages\\\\IPython\\\\extensions', 'C:\\\\Users\\\\Administrator\\\\.ipython']\n",
      "\n",
      "Setting PYSPARK_PYTHON\n",
      "/usr/bin/python3\n",
      "\n",
      "Determine IP Of Server\n",
      "The ip was detected as: 15.1.1.23\n",
      "\n",
      "Create SparkContext\n",
      "\n",
      "<SparkContext master=k8s://https://15.4.7.11:6443 appName=spark-jupyter-win>\n"
     ]
    }
   ],
   "source": [
    "from spark_helper import create_spark_context\n",
    "spark_app_name = \"spark-jupyter-win\"\n",
    "docker_image = \"tschneider/pyspark:v5\"\n",
    "k8_master_ip = \"15.4.7.11\"\n",
    "sc = create_spark_context(spark_app_name, docker_image, k8_master_ip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When done working with spark we need to cleanup we kubernetes objects that were dynamically created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
