{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "In this notebook we are going to load data from our machine into a spark cluster.\n",
    "\n",
    "## Prerequisites\n",
    "It assumes you already have a running spark cluster. In our case we have prepared our spark cluster to run on kubernetes. If you haven't done so already, read through the following notebooks to get setup:\n",
    "- [Install Apache Spark Prerequisites](Install%20Apache%20Spark%20Prerequisites.ipynb)\n",
    "- [Running Apache Spark On Kubernetes](Running%20Apache%20Spark%20On%20Kubernetes.ipynb)\n",
    "\n",
    "Note: We will see that the instructions are basically the same as [Load CSV Into Apache Spark Locally](Load%20CSV%20Into%20Apache%20Spark%20Locally.ipynb) once you get the kubernetes stuff setup.\n",
    "\n",
    "## Adjenda\n",
    "1. Setup Datastore\n",
    "2. Create SparkContext\n",
    "3. Create webserver to host data\n",
    "4. Load Data\n",
    "5. Cleanup Spark and Kubernetes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup Datastore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get on with the rest of our notebook. We need to understand two things:\n",
    "\n",
    "1. Spark makes an assumption about how data is accessed and where it is stored. Steming back to the HDFS days (I assume based on inferrences from obscure Stackoverflow articles), spark assumes that every node (client, driver, worker) have identically mounted the same network filesystem. By default (I haven't found how to override) spark assumes that files being referenced are stored at the filesystem root on all nodes. If you try to load a file from your local machine from a location that doesnt exist on the worker, you will have an error saying the file is not found.\n",
    "\n",
    "2. We have a unique setup. We are running in a jupyter notebook as the spark client and we have rolled our own spark (and I am not an expert in spark... yet). As such the problem we mentioned in (1) may go away with a simple config change or a system upgrade. Again, I have not found this yet.\n",
    "\n",
    "Because of this issue, we need to our dataset to be mounted at the root of our filesystem. Again, this is because when we tell the worker to load a file, for some reason, the pyspark framework assumes that file is in the same place as the machine hosting our notebook. \n",
    "\n",
    "Rather than copy the data to the root, we will simply slymlink the file from our repository into the filesystem root. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symlink created as /nasdaq_2019.csv -> /root/ml-training-jupyter-notebooks/Example Data Sets/nasdaq_2019.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "csv_file_name = \"nasdaq_2019.csv\"\n",
    "csv_relative_file_path = \"../../../Example Data Sets/{0}\".format(csv_file_name)\n",
    "csv_absolute_file_path = os.path.abspath(csv_relative_file_path)\n",
    "csv_link_path = \"/{0}\".format(csv_file_name)\n",
    "\n",
    "if os.path.exists(csv_link_path):\n",
    "    if os.path.islink(csv_link_path):\n",
    "        print(\"Symlink exists at {0}\".format(csv_link_path))\n",
    "    elif os.path.isfile(file):\n",
    "        print(\"File exists at {0}\".format(csv_link_path))\n",
    "    else:\n",
    "        raise Exception(\"Something is wrong. An object exists where we want to create a symlink.\")\n",
    "else:\n",
    "    os.symlink(csv_absolute_file_path, csv_link_path)\n",
    "    print(\"Symlink created as {0} -> {1}\".format(csv_link_path, csv_absolute_file_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Create SparkContext\n",
    "The spark context is the object which allows us interact with the spark cluster and submit jobs etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a helper module\n",
    "import importlib.util\n",
    "spec = importlib.util.spec_from_file_location(\"spark_helper\", \"../../../Utilities/spark_helper.py\")\n",
    "spark_helper = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(spark_helper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting SPARK_HOME\n",
      "/opt/spark\n",
      "\n",
      "Running findspark.init() function\n",
      "['/opt/spark/python', '/opt/spark/python/lib/py4j-0.10.9-src.zip', '/opt/spark/python', '/tmp/spark-ed3a5ec9-7217-41a7-92f3-ed41666e7c11/userFiles-ea944a39-75e0-497f-babb-ffa4f6d50fbd', '/tmp/spark-ed3a5ec9-7217-41a7-92f3-ed41666e7c11/userFiles-859a7aff-c4e9-4ccc-8a84-04ea4530703d', '/opt/spark/python/lib/py4j-0.10.9-src.zip', '/usr/lib64/python36.zip', '/usr/lib64/python3.6', '/usr/lib64/python3.6/lib-dynload', '', '/usr/local/lib64/python3.6/site-packages', '/usr/local/lib/python3.6/site-packages', '/usr/lib64/python3.6/site-packages', '/usr/lib/python3.6/site-packages', '/usr/local/lib/python3.6/site-packages/IPython/extensions', '/root/.ipython']\n",
      "\n",
      "Setting PYSPARK_PYTHON\n",
      "/usr/bin/python3\n",
      "\n",
      "Determining IP Of Server\n",
      "The ip was detected as: 15.4.12.12\n",
      "\n",
      "Configuring URL for kubernetes master\n",
      "k8s://https://15.4.7.11:6443\n",
      "\n",
      "Creating Spark Session\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "spark_app_name = \"spark-jupyter-win-demo\"\n",
    "docker_image = \"tschneider/pyspark:v5\"\n",
    "k8_master_ip = \"15.4.7.11\"\n",
    "spark_session = spark_helper.create_spark_session(spark_app_name, docker_image, k8_master_ip)\n",
    "sc = spark_session.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** We can look at kubernetes to see that out worker nodes were created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                             READY     STATUS    RESTARTS   AGE\n",
      "koalas-7fff4f7d8ba1ba74-exec-1   1/1       Running   0          1m\n",
      "koalas-7fff4f7d8ba1ba74-exec-2   1/1       Running   0          1m\n",
      "koalas-7fff4f7d8ba1ba74-exec-3   1/1       Running   0          1m\n"
     ]
    }
   ],
   "source": [
    "! kubectl -n spark get pod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. Create web server to host data\n",
    "Recall that spark is a distributed compute environment; meaning that a group of machines are working together to load data, distribute it accross the cluster nodes, and execute code. In order for for the data to be loaded, it needs to be available across all the nodes; we cannot load it directly from our local filesystem because the spark workers cannot access our local file system directly. There are a number of solutions for making the data available like s3 or hadoop file system. In our case we will take a different approach.\n",
    "\n",
    "We will publish our data to a webserver running in our jupyter notebook. The worker nodes will be able to download the file from a URL.\n",
    "\n",
    "Note: This is just for testing and small scale EDA. This is not intended for production use cases!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Determine the current working directory. \n",
    "\n",
    "Note: There is a trick to doing this inside a jupyter notebook and so we will use a special library to get that information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/ml-training-jupyter-notebooks\n"
     ]
    }
   ],
   "source": [
    "import pyprojroot\n",
    "project_root_dir  = pyprojroot.here()\n",
    "print(project_root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Load the module for the webserver from our utilities directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the module for the web server we wrote\n",
    "import importlib.util\n",
    "spec = importlib.util.spec_from_file_location(\"PythonHttpFileServer\", \"../../../Utilities/PythonHttpFileServer.py\")\n",
    "PythonHttpFileServer = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(PythonHttpFileServer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Configure logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the logger and log level\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Start the webserver in a new thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_sub_dir = \"Example Data Sets\"\n",
    "web_root = os.path.join(project_root_dir, data_sub_dir)\n",
    "\n",
    "if not os.path.exists(web_root):\n",
    "    raise Exception(\"The web root for the server does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Starting server on port 80\n",
      "INFO:root:Web root specified as: /root/ml-training-jupyter-notebooks/Example Data Sets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app 'PythonHttpFileServer' (lazy loading)\n"
     ]
    }
   ],
   "source": [
    "# Start the webserver in a thread so the cell is not stuck in a running state\n",
    "import threading\n",
    "\n",
    "var_exists = 'web_server_thread' in locals() or 'web_server_thread' in globals()\n",
    "if not var_exists:\n",
    "    web_server_port = 80\n",
    "    web_server_args = (web_server_port, web_root)\n",
    "    web_server_thread = threading.Thread(target=PythonHttpFileServer.run_server, args=web_server_args)\n",
    "    web_server_thread.start()\n",
    "else:\n",
    "    print(\"Web Server thread already exists\")\n",
    "    print(\"To kill it you need to restart the kernel.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Load The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data is not as intuitive as one would think. We Instruct the spark cluster to download a file from the web server. But when we do this, the file is not actually downloaded. Remember, spark is lazy. Instead, a link to the url is stored in the spark session object so that the file can be downloaded when we need to perform an operation on it.\n",
    "\n",
    "Later, we will tell spark to return a handle to a dataframe consisting of the data contained in this url. At that point, lazy spark, will download the data file to each worker (at their root), open the file, and distribute the data accross the cluster. We will see proof of this when we call the addFile() function. Our server logs will show that the workers are making web requests to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Add the file using Spark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Get /root/ml-training-jupyter-notebooks/Example Data Sets/nasdaq_2019.csv\n",
      "INFO:werkzeug:15.4.12.12 - - [05/Dec/2021 17:27:44] \"GET /nasdaq_2019.csv HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading file 'http://15.4.12.12:80/nasdaq_2019.csv' to Spark cluster.\n"
     ]
    }
   ],
   "source": [
    "ip_address = spark_helper.determine_ip_address()\n",
    "csv_file_name = \"nasdaq_2019.csv\"\n",
    "csv_file_url = \"http://{0}:{1}/{2}\".format(ip_address, web_server_port, csv_file_name)\n",
    "print(\"Uploading file '{0}' to Spark cluster.\".format(csv_file_url))\n",
    "sc.addFile(csv_file_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Use koalas to open the file on spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the utility function to convert a date string to a datetime object from our utilities module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the utilities module we wrote\n",
    "import importlib.util\n",
    "spec = importlib.util.spec_from_file_location(\"utilities\", \"../../../Utilities/utilities.py\")\n",
    "utilities = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(utilities)\n",
    "\n",
    "# Define a mapping to convert our data field to the correct type\n",
    "converter_mapping = {\n",
    "    \"date\": utilities.convert_date_string_to_date\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Load our OHCLV data Into a koalas dataframe and pull out a single day in the say way we would in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Get /root/ml-training-jupyter-notebooks/Example Data Sets/nasdaq_2019.csv\n",
      "INFO:werkzeug:15.4.7.101 - - [05/Dec/2021 17:28:11] \"GET /nasdaq_2019.csv HTTP/1.1\" 200 -\n",
      "INFO:root:Get /root/ml-training-jupyter-notebooks/Example Data Sets/nasdaq_2019.csv\n",
      "INFO:root:Get /root/ml-training-jupyter-notebooks/Example Data Sets/nasdaq_2019.csv\n",
      "INFO:werkzeug:15.4.7.102 - - [05/Dec/2021 17:28:14] \"GET /nasdaq_2019.csv HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:15.4.7.103 - - [05/Dec/2021 17:28:14] \"GET /nasdaq_2019.csv HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "# Avoid a warning\n",
    "import os\n",
    "os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\n",
    "\n",
    "from databricks import koalas\n",
    "koalas_dataframe = koalas.read_csv(u\"file:////nasdaq_2019.csv\", converters=converter_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should see the workers download the file in the logs. If we log into the nodes we can see the file is located on the filesystem root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                             READY     STATUS    RESTARTS   AGE\n",
      "koalas-7fff4f7d8ba1ba74-exec-1   1/1       Running   0          2m\n",
      "koalas-7fff4f7d8ba1ba74-exec-2   1/1       Running   0          2m\n",
      "koalas-7fff4f7d8ba1ba74-exec-3   1/1       Running   0          2m\n"
     ]
    }
   ],
   "source": [
    "! kubectl -n spark get pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nasdaq_2019.csv\n"
     ]
    }
   ],
   "source": [
    "! kubectl -n spark exec -ti koalas-7fff4f7d8ba1ba74-exec-1 -- find / -name nasdaq_2019.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data loaded into a koalas dataframe we can access the data in the same way we would from a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>interval</th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AABA</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>70.90</td>\n",
       "      <td>71.5200</td>\n",
       "      <td>70.3250</td>\n",
       "      <td>70.57</td>\n",
       "      <td>10234800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAL</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>33.14</td>\n",
       "      <td>33.6632</td>\n",
       "      <td>32.5301</td>\n",
       "      <td>32.88</td>\n",
       "      <td>8995100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAME</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>2.43</td>\n",
       "      <td>2.4300</td>\n",
       "      <td>2.4000</td>\n",
       "      <td>2.40</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAOI</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>10.70</td>\n",
       "      <td>10.8900</td>\n",
       "      <td>10.0100</td>\n",
       "      <td>10.18</td>\n",
       "      <td>883100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAON</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>50.57</td>\n",
       "      <td>50.9850</td>\n",
       "      <td>48.5600</td>\n",
       "      <td>49.73</td>\n",
       "      <td>180200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker interval        date   open     high      low  close    volume\n",
       "0   AABA        D  2019-07-01  70.90  71.5200  70.3250  70.57  10234800\n",
       "1    AAL        D  2019-07-01  33.14  33.6632  32.5301  32.88   8995100\n",
       "2   AAME        D  2019-07-01   2.43   2.4300   2.4000   2.40       500\n",
       "3   AAOI        D  2019-07-01  10.70  10.8900  10.0100  10.18    883100\n",
       "4   AAON        D  2019-07-01  50.57  50.9850  48.5600  49.73    180200"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "koalas_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>interval</th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>93620</th>\n",
       "      <td>AABA</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>57.94</td>\n",
       "      <td>57.94</td>\n",
       "      <td>57.94</td>\n",
       "      <td>57.94</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93621</th>\n",
       "      <td>AAL</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>32.11</td>\n",
       "      <td>32.11</td>\n",
       "      <td>32.11</td>\n",
       "      <td>32.11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93622</th>\n",
       "      <td>AAME</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>2.41</td>\n",
       "      <td>2.41</td>\n",
       "      <td>2.41</td>\n",
       "      <td>2.41</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93623</th>\n",
       "      <td>AAOI</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>15.43</td>\n",
       "      <td>15.43</td>\n",
       "      <td>15.43</td>\n",
       "      <td>15.43</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93624</th>\n",
       "      <td>AAON</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>35.06</td>\n",
       "      <td>35.06</td>\n",
       "      <td>35.06</td>\n",
       "      <td>35.06</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ticker interval        date   open   high    low  close  volume\n",
       "93620   AABA        D  2019-01-01  57.94  57.94  57.94  57.94       0\n",
       "93621    AAL        D  2019-01-01  32.11  32.11  32.11  32.11       0\n",
       "93622   AAME        D  2019-01-01   2.41   2.41   2.41   2.41       0\n",
       "93623   AAOI        D  2019-01-01  15.43  15.43  15.43  15.43       0\n",
       "93624   AAON        D  2019-01-01  35.06  35.06  35.06  35.06       0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort based on the date column\n",
    "koalas_dataframe = koalas_dataframe.sort_values(\"date\")\n",
    "df_01_01_2019 = koalas_dataframe.loc[koalas_dataframe[\"date\"] == '2019-01-01']\n",
    "df_01_01_2019.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Cleanup Spark Cluster On Kubernetes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Remove symlinked data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted symlinked data file\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if os.path.exists(csv_link_path) and os.path.islink(csv_link_path):\n",
    "    os.unlink(csv_link_path)\n",
    "    print(\"Deleted symlinked data file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Cleanup Spark Cluster On Kubernetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No resources found.\n"
     ]
    }
   ],
   "source": [
    "! kubectl -n spark get pod"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
