{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "In this notebook we are going to load data from our machine into a spark cluster.\n",
    "\n",
    "## Prerequisites\n",
    "It assumes you already have a running spark cluster. In our case we have prepared our spark cluster to run on kubernetes. If you haven't done so already, read through the following notebooks to get setup:\n",
    "- [Install Apache Spark Prerequisites](Install%20Apache%20Spark%20Prerequisites.ipynb)\n",
    "- [Running Apache Spark On Kubernetes](Running%20Apache%20Spark%20On%20Kubernetes.ipynb)\n",
    "- [Intro To Koalas](Intro%20To%20Koalas.ipynb)\n",
    "\n",
    "Note: We will see that the instructions are basically the same as [Load CSV Into Apache Spark Locally](Load%20CSV%20Into%20Apache%20Spark%20Locally.ipynb) once you get the kubernetes stuff setup.\n",
    "\n",
    "## Adjenda\n",
    "1. Understand Architecture\n",
    "2. Create SparkContext\n",
    "3. Setup Datastore\n",
    "4. Load Data\n",
    "5. Cleanup Spark and Kubernetes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Understand Atchitecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start working, we need to understand a few things related to architecture.\n",
    "\n",
    "If we think of the spark cluster as a server and our jupyter noteboook as a client we will see that there are two places from which data can be loaded. \n",
    "- Option 1: We load data from a file on the client\n",
    "- Option 2: We load data from a file on the workers\n",
    "\n",
    "Currently the Spark/Koalas framework provides a means for accomplishing both options but each have their own caveats which I will briefly describe. \n",
    "\n",
    "With Option 1, the process of loading the data is to create a pandas dataframe, and then create a koalas dataframe from it. The code would look something like this:\n",
    "```python\n",
    "import pandas\n",
    "from databricks import koalas\n",
    "\n",
    "pdf = pandas.read_csv(...)\n",
    "kdf = koalas.DataFrame(pdf)\n",
    "```\n",
    "\n",
    "But, in order to create the pandas dataframe, we will need to be able to load all the data into memory which is a deal breaker when working with big data; we simply do not have enough memory to load such a large dataset on one machine.\n",
    "\n",
    "With Option 2, the process of loading data is to instruct the spark workers to load a file from their local filesystem. The Caveat is that Spark has made an assumption that the driver and all of the workers have access to the same network file system mounted in the same place. I think this stems back to the HDFS days but I am not 100% sure. In our case, which is often the case, we do not have such a thing setup. Instead we will have to hack some utility function together. First we will need to move data from our *Example Data Sets* directory to the file system root (in our case symlink). Then take advantage of a Spark utility which will run a job which downloads a file from a URL to the root of the local filesystem on all the spark workers. Finally We will setup a simple http server to serve local files to the spark cluster. \n",
    "\n",
    "**Note**: It should be noted that this is strictly for educational purposes and is not inteded for production use. For production use, configure the Spark Worers to mount the network storage solution. This avoids data duplication and cuts down on the wall time because the data has less hops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2. Create SparkContext\n",
    "The spark context is the object which allows us interact with the spark cluster and submit jobs etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a helper module\n",
    "import spark_helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting SPARK_HOME\n",
      "/usr/lib/spark-3.1.1-bin-hadoop2.7\n",
      "\n",
      "Running findspark.init() function\n",
      "['/usr/lib/spark-3.1.1-bin-hadoop2.7/python', '/usr/lib/spark-3.1.1-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip', '/root/ml-training-jupyter-notebooks/Machine Learning/Big Data And Big Compute/Apache Spark', '/usr/local/lib/python39.zip', '/usr/local/lib/python3.9', '/usr/local/lib/python3.9/lib-dynload', '', '/usr/local/lib/python3.9/site-packages', '/root/ml-training-jupyter-notebooks/Utilities']\n",
      "\n",
      "Setting PYSPARK_PYTHON\n",
      "/usr/local/bin/python3\n",
      "\n",
      "Configuring URL for kubernetes master\n",
      "k8s://https://15.4.7.11:6443\n",
      "\n",
      "Determining IP Of Server\n",
      "The ip was detected as: 15.4.12.12\n",
      "\n",
      "Creating SparkConf Object\n",
      "('spark.master', 'k8s://https://15.4.7.11:6443')\n",
      "('spark.app.name', 'spark-jupyter-mlib')\n",
      "('spark.submit.deploy.mode', 'cluster')\n",
      "('spark.kubernetes.container.image', 'tschneider/apache-spark-k8:v7')\n",
      "('spark.kubernetes.namespace', 'spark')\n",
      "('spark.kubernetes.pyspark.pythonVersion', '3')\n",
      "('spark.kubernetes.authenticate.driver.serviceAccountName', 'spark-sa')\n",
      "('spark.kubernetes.authenticate.serviceAccountName', 'spark-sa')\n",
      "('spark.executor.instances', '3')\n",
      "('spark.executor.cores', '2')\n",
      "('spark.executor.memory', '4096m')\n",
      "('spark.executor.memoryOverhead', '1024m')\n",
      "('spark.driver.memory', '1024m')\n",
      "('spark.driver.host', '15.4.12.12')\n",
      "('spark.files.overwrite', 'true')\n",
      "('spark.files.useFetchCache', 'false')\n",
      "\n",
      "Creating SparkSession Object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/02/20 16:59:13 WARN Utils: Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 15.4.12.12 instead (on interface eth0)\n",
      "22/02/20 16:59:13 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/lib/spark-3.1.1-bin-hadoop2.7/jars/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/02/20 16:59:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Create a spark session and spark context\n",
    "spark_app_name = \"spark-jupyter-mlib\"\n",
    "docker_image = \"tschneider/apache-spark-k8:v7\"\n",
    "k8_master_ip = \"15.4.7.11\"\n",
    "spark_session = spark_helper.create_spark_session(spark_app_name, docker_image, k8_master_ip)\n",
    "sc = spark_session.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** We can look at kubernetes to see that out worker nodes were created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                         READY   STATUS    RESTARTS   AGE\n",
      "spark-jupyter-mlib-8519867f1812cafb-exec-1   1/1     Running   0          51s\n",
      "spark-jupyter-mlib-8519867f1812cafb-exec-2   1/1     Running   0          50s\n",
      "spark-jupyter-mlib-8519867f1812cafb-exec-3   1/1     Running   0          49s\n"
     ]
    }
   ],
   "source": [
    "! kubectl -n spark get pod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. Setup Datastore\n",
    "As mentioned earlier, we are going to create a webserver to serve files from our local machine to the spark cluster. In order to be able to run the web server, without having the jupyter cell run forever and prevent us from moving on with out work, we will run it in a separate thread. If you are not familiar with threads etc I suggest reading up. The key things to know are that the web server will run as a separate process and python doesn't provide an out of the box way to kill the thread. If you fudged the configs... restart the kernel to kill the web server.\n",
    "\n",
    "This web server will be configured to serve all files from a given *web_root* directory. In this demo, I set the directory to the *./Example Data Sets* folder at the root of this project.\n",
    "\n",
    "**Note**: This is just for testing and small scale EDA. This is not intended for production use cases!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3.1. Symlink Local Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/ml-training-jupyter-notebooks\n"
     ]
    }
   ],
   "source": [
    "import pyprojroot\n",
    "project_root_dir  = pyprojroot.here()\n",
    "print(project_root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Symlink: /root/ml-training-jupyter-notebooks/Example Data Sets/Test Scores.csv -> /Test Scores.csv\n",
      "Creating Symlink: /root/ml-training-jupyter-notebooks/Example Data Sets/nasdaq_2019.csv -> /nasdaq_2019.csv\n",
      "Creating Symlink: /root/ml-training-jupyter-notebooks/Example Data Sets/.gitignore -> /.gitignore\n",
      "Creating Symlink: /root/ml-training-jupyter-notebooks/Example Data Sets/demo_data.csv -> /demo_data.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "data_dir_name = \"Example Data Sets\"\n",
    "data_dir_path = os.path.join(project_root_dir, data_dir_name)\n",
    "spark_helper.symlink_dir_to_root(data_dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Load Web Server Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the module for the web server we wrote\n",
    "import PythonHttpFileServer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Start Web Server\n",
    "**Note**: When setting a web root, or working with files, keep in mind that URLs need to escape special characters. I have set my webroot to the Example Data Directory rather than the project root so I dont have to escape anything. That is also why I have named the file with the characters I that I did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Starting server on port 80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "web root: /root/ml-training-jupyter-notebooks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Web root specified as: /root/ml-training-jupyter-notebooks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app 'PythonHttpFileServer' (lazy loading)\n",
      " * Environment: production\n"
     ]
    }
   ],
   "source": [
    "# Import the library\n",
    "import threading\n",
    "\n",
    "# Configure the logger and log level (incase we need/want to debug)\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Create and start the thread if it doesnt exist\n",
    "web_root = project_root_dir\n",
    "print(\"web root: {0}\".format(web_root))\n",
    "var_exists = 'web_server_thread' in locals() or 'web_server_thread' in globals()\n",
    "if not var_exists:\n",
    "    web_server_port = 80\n",
    "    web_server_args = (web_server_port, web_root)\n",
    "    web_server_thread = threading.Thread(target=PythonHttpFileServer.run_server, args=web_server_args)\n",
    "    web_server_thread.start()\n",
    "else:\n",
    "    print(\"Web Server thread already exists\")\n",
    "    print(\"To kill it you need to restart the kernel.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Load The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data is not as intuitive as one would think. We Instruct the spark cluster to download a file from the web server. But when we do this, the file is not actually downloaded. Remember, spark is lazy. Instead, a link to the url is stored in the spark session object so that the file can be downloaded when we need to perform an operation on it.\n",
    "\n",
    "Later, we will tell spark to return a handle to a dataframe consisting of the data contained in this url. At that point, lazy spark, will download the data file to each worker (at their root), open the file, and distribute the data accross the cluster. We will see proof of this when we call the addFile() function. Our server logs will show that the workers are making web requests to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4.1. Add the file using Spark Context\n",
    "We will use the *addFiles()* function available on the SparkContect object to download a file to the workers. Behind the scenes, this file submits a job to the cluster. So the file isnt actually downloaded until I do some work. This is made apparent when the server logs pop up in a jupyter cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:werkzeug: * Running on all addresses.\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "INFO:werkzeug: * Running on http://15.4.12.12:80/ (Press CTRL+C to quit)\n",
      "INFO:root:Get /root/ml-training-jupyter-notebooks/Example Data Sets/demo_data.csv\n",
      "INFO:werkzeug:15.4.12.12 - - [20/Feb/2022 17:00:22] \"GET /Example%2520Data%2520Sets/demo_data.csv HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "import urllib.parse\n",
    "csv_file_name = \"demo_data.csv\"\n",
    "data_dir_name = \"Example Data Sets\"\n",
    "ip_address = spark_helper.determine_ip_address()\n",
    "csv_file_url = \"http://{0}:{1}/{2}/{3}\".format(\n",
    "    ip_address, \n",
    "    web_server_port, \n",
    "    urllib.parse.quote(data_dir_name), \n",
    "    urllib.parse.quote(csv_file_name))\n",
    "sc.addFile(csv_file_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Use koalas to open the file on spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we load the date we want to set a few environment variables for our convenience. We dont want pyspark complaining about our timezone and we dont wnat koalas auto upgrading spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoid a warning\n",
    "import os\n",
    "os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\n",
    "os.environ[\"SPARK_KOALAS_AUTOPATCH\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Get /root/ml-training-jupyter-notebooks/Example Data Sets/demo_data.csv\n",
      "INFO:werkzeug:15.4.7.103 - - [20/Feb/2022 17:00:35] \"GET /Example%20Data%20Sets/demo_data.csv HTTP/1.1\" 200 -\n",
      "22/02/20 17:00:44 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/02/20 17:00:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "INFO:root:Get /root/ml-training-jupyter-notebooks/Example Data Sets/demo_data.csv\n",
      "INFO:werkzeug:15.4.7.102 - - [20/Feb/2022 17:00:48] \"GET /Example%20Data%20Sets/demo_data.csv HTTP/1.1\" 200 -\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A  B  C\n",
       "0  1  2  3\n",
       "1  4  5  6\n",
       "2  7  8  9"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from databricks import koalas\n",
    "demo_df = koalas.read_csv(u\"file:///{0}\".format(csv_file_name))\n",
    "demo_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should see the workers download the file in the logs for the cells above.\n",
    "\n",
    "Using the kubectl CLI we can log into the kubernetes pods running our worker nodes. This is useful for debugging purposes. For example, if we log into the nodes we can search for the file we downloaded. Below we cant see the file is located on the filesystem root:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                         READY   STATUS    RESTARTS   AGE\n",
      "spark-jupyter-mlib-8519867f1812cafb-exec-1   1/1     Running   0          92s\n",
      "spark-jupyter-mlib-8519867f1812cafb-exec-2   1/1     Running   0          91s\n",
      "spark-jupyter-mlib-8519867f1812cafb-exec-3   1/1     Running   0          90s\n"
     ]
    }
   ],
   "source": [
    "! kubectl -n spark get pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error from server (NotFound): pods \"spark-jupyter-mlib-f9a4ff7e756bc78c-exec-1\" not found\n"
     ]
    }
   ],
   "source": [
    "! kubectl -n spark exec -ti spark-jupyter-mlib-f9a4ff7e756bc78c-exec-1 -- find / -name demo_data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Updating Data Files On Workers\n",
    "\n",
    "In some cases we may want to update data files we previously loaded onto our workers. \n",
    "\n",
    "The problem we run into is that at this point in time, the pyspark API does not have the functionality to do this for us out of the box (despite the documentation claiming it does... I have proved that it does not and opened a [bug](https://issues.apache.org/jira/browse/SPARK-37958) in the project's issue tracker).\n",
    "\n",
    "No worries... I have written some simple utilities (hacks) to help with this effort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets update our data and save it to our local file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/02/20 17:00:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/02/20 17:00:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A  B  C\n",
       "0  1  2  3\n",
       "1  4  5  6\n",
       "2  7  8  9\n",
       "0  1  2  3\n",
       "1  4  5  6\n",
       "2  7  8  9"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_def = demo_df.append(demo_df).to_pandas()\n",
    "new_def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = os.path.join(web_root, csv_file_name)\n",
    "if os.path.exists(csv_file_path):\n",
    "    os.remove(csv_file_path)\n",
    "new_def.to_csv(csv_file_path, mode='a', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Try To Reload Previously Added Data File\n",
    "If we try to load the new data by adding the data file again, spark will complain with a warning message and reload the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Get /root/ml-training-jupyter-notebooks/Example Data Sets/demo_data.csv\n",
      "INFO:werkzeug:15.4.7.101 - - [20/Feb/2022 17:00:59] \"GET /Example%20Data%20Sets/demo_data.csv HTTP/1.1\" 200 -\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reloaded_df = koalas.read_csv(u\"file:///{0}\".format(csv_file_name))\n",
    "reloaded_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the reloaded dataframe has the same shape as the original!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Write A Workaround Utility\n",
    "Ok, so what is the workaround? First we need to explore the other parts of the pyspark api.\n",
    "\n",
    "We can get an absolute path to a file on the driver (whether it exists or not) by using the SparkFiles API. We can combine this api with the `os` module to check if a file really exists. If it does, we know the file was added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_added_to_spark(file_name):\n",
    "    \n",
    "    import os\n",
    "    from  pyspark import SparkFiles\n",
    "    file_path = SparkFiles.get(file_name)\n",
    "    if os.path.exists(file_path):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_added_to_spark(csv_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_added_to_spark(\"does_not_exist.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can reliably tell if a file exists on the driver using the function above.\n",
    "\n",
    "The next part of the workaround involves the worker. In order to update the data on the worker we will tell each worker to delete their copy of the file, then redownload the file for them, then create a new dataframe.\n",
    "\n",
    "But how do we directly tell all the workers to do something? One method is to use the builtin parallelize() function available on the SparkContext. The function will run a function N times in parallel across the cluster. The trick is getting N to be the number of workers. The following code snippet illustrates how this works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['spark-jupyter-mlib-8519867f1812cafb-exec-3',\n",
       " 'spark-jupyter-mlib-8519867f1812cafb-exec-2',\n",
       " 'spark-jupyter-mlib-8519867f1812cafb-exec-1']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_hostname(var):\n",
    "    import socket\n",
    "    return socket.gethostname()\n",
    "    \n",
    "worker_count = int(spark_session.sparkContext.getConf().get('spark.executor.instances'))\n",
    "rdd = spark_session.sparkContext.parallelize(range(worker_count)).map(get_hostname)\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the hostnames for all the cluster nodes above!\n",
    "\n",
    "Now we just need to modify the function that will run on the workers. We can package this function, and the logic up in a nuce utility function. The code below accomplishes this goal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Get /root/ml-training-jupyter-notebooks/Example Data Sets/demo_data.csv\n",
      "INFO:werkzeug:15.4.12.12 - - [20/Feb/2022 17:01:16] \"GET /Example%20Data%20Sets/demo_data.csv HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating file on driver.\n",
      "Updating file on workers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Get /root/ml-training-jupyter-notebooks/Example Data Sets/demo_data.csv\n",
      "INFO:werkzeug:15.4.7.102 - - [20/Feb/2022 17:01:17] \"GET /Example%20Data%20Sets/demo_data.csv HTTP/1.1\" 200 -\n",
      "INFO:root:Get /root/ml-training-jupyter-notebooks/Example Data Sets/demo_data.csv\n",
      "INFO:werkzeug:15.4.7.103 - - [20/Feb/2022 17:01:17] \"GET /Example%20Data%20Sets/demo_data.csv HTTP/1.1\" 200 -\n",
      "INFO:root:Get /root/ml-training-jupyter-notebooks/Example Data Sets/demo_data.csv\n",
      "INFO:werkzeug:15.4.7.101 - - [20/Feb/2022 17:01:17] \"GET /Example%20Data%20Sets/demo_data.csv HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark-jupyter-mlib-8519867f1812cafb-exec-1 -> Deleted. Downloaded.\n",
      "spark-jupyter-mlib-8519867f1812cafb-exec-3 -> Deleted. Downloaded.\n",
      "spark-jupyter-mlib-8519867f1812cafb-exec-2 -> Deleted. Downloaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def update_file_on_worker(file_url):\n",
    "        \n",
    "    # Determine the hostname of the current worker node\n",
    "    import socket\n",
    "    hostname = socket.gethostname()\n",
    "\n",
    "    # Create a message to inform the driver what has happened\n",
    "    update_result = hostname + \" -> \"\n",
    "    \n",
    "    # Determine the name of the file\n",
    "    import urllib.parse as parse\n",
    "    file_name = os.path.basename(parse.urlparse(file_url).path)\n",
    "    \n",
    "    # Delete the file if it exits\n",
    "    local_file_path = \"/{0}\".format(file_name)\n",
    "    if os.path.exists(local_file_path):\n",
    "        update_result += \"Deleted. \"\n",
    "        os.remove(local_file_path)\n",
    "     \n",
    "    # Determine the file name\n",
    "    import urllib.parse as parse\n",
    "    file_name = os.path.basename(parse.urlparse(file_url).path)\n",
    "    \n",
    "    # Download the file\n",
    "    import urllib.request\n",
    "    urllib.request.urlretrieve(file_url, local_file_path)\n",
    "    \n",
    "    return update_result + \"Downloaded.\"\n",
    "\n",
    "def add_file_to_cluster(spark_session, file_url):\n",
    "    if file_added_to_spark:\n",
    "        print(\"Updating file on driver.\")\n",
    "        import os\n",
    "        import urllib.parse as parse\n",
    "        file_name = os.path.basename(parse.urlparse(file_url).path)\n",
    "        import pyspark\n",
    "        local_file_path = pyspark.SparkFiles.get(file_name)\n",
    "        if os.path.exists(local_file_path):\n",
    "            os.remove(local_file_path)\n",
    "        import urllib.request\n",
    "        urllib.request.urlretrieve(file_url, local_file_path)\n",
    "    else:\n",
    "        print(\"Adding file to driver.\")\n",
    "        spark_session.sparkContext.addFile(file_url)\n",
    "    print(\"Updating file on workers:\")\n",
    "    worker_count = int(spark_session.sparkContext.getConf().get('spark.executor.instances'))\n",
    "    rdd = spark_session.sparkContext.parallelize(range(worker_count)).map(lambda var: update_file_on_worker(file_url))\n",
    "    results = rdd.collect()\n",
    "    for result in results:\n",
    "        print(result)\n",
    "\n",
    "add_file_to_cluster(spark_session, csv_file_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: I have added the functions listed above to the spark_helper module so that they can be reused going forward. Because the functions are defined in a module however, we will need the module to be present on the workers. As such we will need to add our module to the worker in the same way that we add our data files before we can use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Symlink: /root/ml-training-jupyter-notebooks/Utilities/.ipynb_checkpoints -> /.ipynb_checkpoints\n",
      "Creating Symlink: /root/ml-training-jupyter-notebooks/Utilities/PythonHttpFileServer.py -> /PythonHttpFileServer.py\n",
      "Creating Symlink: /root/ml-training-jupyter-notebooks/Utilities/Using Progressbars.ipynb -> /Using Progressbars.ipynb\n",
      "Creating Symlink: /root/ml-training-jupyter-notebooks/Utilities/parallelization.py -> /parallelization.py\n",
      "Creating Symlink: /root/ml-training-jupyter-notebooks/Utilities/spark_helper.py -> /spark_helper.py\n",
      "Creating Symlink: /root/ml-training-jupyter-notebooks/Utilities/utilities.py -> /utilities.py\n",
      "Creating Symlink: /root/ml-training-jupyter-notebooks/Utilities/Utilities.egg-info -> /Utilities.egg-info\n",
      "Creating Symlink: /root/ml-training-jupyter-notebooks/Utilities/__pycache__ -> /__pycache__\n"
     ]
    }
   ],
   "source": [
    "import spark_helper\n",
    "utilities_dir_path = os.path.join(project_root_dir, \"Utilities\")\n",
    "spark_helper.symlink_dir_to_root(utilities_dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Get /root/ml-training-jupyter-notebooks/Utilities/spark_helper.py\n",
      "INFO:werkzeug:15.4.12.12 - - [20/Feb/2022 17:01:18] \"GET /Utilities/spark_helper.py HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "python_module_name = \"spark_helper.py\"\n",
    "python_module_url = \"http://{0}:{1}/{2}/{3}\".format(\n",
    "    ip_address, \n",
    "    web_server_port,\n",
    "    urllib.parse.quote(\"Utilities\"), \n",
    "    urllib.parse.quote(python_module_name))\n",
    "sc.addFile(python_module_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we go ahead and re-add the file and re-read into a csv we will see the content update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Get /root/ml-training-jupyter-notebooks/Example Data Sets/demo_data.csv\n",
      "INFO:werkzeug:15.4.12.12 - - [20/Feb/2022 17:01:21] \"GET /Example%20Data%20Sets/demo_data.csv HTTP/1.1\" 200 -\n",
      "INFO:root:Get /root/ml-training-jupyter-notebooks/Utilities/spark_helper.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating file on driver.\n",
      "Updating file on workers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Get /root/ml-training-jupyter-notebooks/Utilities/spark_helper.py\n",
      "INFO:werkzeug:15.4.7.101 - - [20/Feb/2022 17:01:22] \"GET /Utilities/spark_helper.py HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:15.4.7.103 - - [20/Feb/2022 17:01:22] \"GET /Utilities/spark_helper.py HTTP/1.1\" 200 -\n",
      "INFO:root:Get /root/ml-training-jupyter-notebooks/Utilities/spark_helper.py\n",
      "INFO:werkzeug:15.4.7.102 - - [20/Feb/2022 17:01:22] \"GET /Utilities/spark_helper.py HTTP/1.1\" 200 -\n",
      "INFO:root:Get /root/ml-training-jupyter-notebooks/Example Data Sets/demo_data.csv\n",
      "INFO:werkzeug:15.4.7.103 - - [20/Feb/2022 17:01:22] \"GET /Example%20Data%20Sets/demo_data.csv HTTP/1.1\" 200 -\n",
      "INFO:root:Get /root/ml-training-jupyter-notebooks/Example Data Sets/demo_data.csv\n",
      "INFO:werkzeug:15.4.7.102 - - [20/Feb/2022 17:01:22] \"GET /Example%20Data%20Sets/demo_data.csv HTTP/1.1\" 200 -\n",
      "INFO:root:Get /root/ml-training-jupyter-notebooks/Example Data Sets/demo_data.csv\n",
      "INFO:werkzeug:15.4.7.101 - - [20/Feb/2022 17:01:22] \"GET /Example%20Data%20Sets/demo_data.csv HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark-jupyter-mlib-8519867f1812cafb-exec-2 -> Deleted. Downloaded.\n",
      "spark-jupyter-mlib-8519867f1812cafb-exec-3 -> Deleted. Downloaded.\n",
      "spark-jupyter-mlib-8519867f1812cafb-exec-1 -> Deleted. Downloaded.\n"
     ]
    }
   ],
   "source": [
    "spark_helper.add_file_to_cluster(spark_session, csv_file_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_df_2 = koalas.read_csv(u\"file:////{0}\".format(csv_file_name))\n",
    "demo_df_2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the new dataframe's dimensions match the updated dimensions.\n",
    "\n",
    "For housekeeping we will restore the original data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/02/20 17:01:24 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "csv_file_path = os.path.join(web_root, csv_file_name)\n",
    "if os.path.exists(csv_file_path):\n",
    "    os.remove(csv_file_path)\n",
    "demo_df.to_pandas().to_csv(csv_file_path, mode='a', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/02/20 17:01:25 WARN ExecutorPodsWatchSnapshotSource: Kubernetes client has been closed (this is expected if the application is shutting down.)\n"
     ]
    }
   ],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                         READY   STATUS        RESTARTS   AGE\n",
      "spark-jupyter-mlib-8519867f1812cafb-exec-1   1/1     Terminating   0          2m15s\n",
      "spark-jupyter-mlib-8519867f1812cafb-exec-2   1/1     Terminating   0          2m14s\n",
      "spark-jupyter-mlib-8519867f1812cafb-exec-3   0/1     Terminating   0          2m13s\n"
     ]
    }
   ],
   "source": [
    "! kubectl -n spark get pod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing Symlink: /root/ml-training-jupyter-notebooks/Example Data Sets/Test Scores.csv -> /Test Scores.csv\n",
      "Removing Symlink: /root/ml-training-jupyter-notebooks/Example Data Sets/nasdaq_2019.csv -> /nasdaq_2019.csv\n",
      "Removing Symlink: /root/ml-training-jupyter-notebooks/Example Data Sets/.gitignore -> /.gitignore\n",
      "Removing Symlink: /root/ml-training-jupyter-notebooks/Example Data Sets/demo_data.csv -> /demo_data.csv\n"
     ]
    }
   ],
   "source": [
    "spark_helper.unlink_dir_from_root(data_dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing Symlink: /root/ml-training-jupyter-notebooks/Utilities/.ipynb_checkpoints -> /.ipynb_checkpoints\n",
      "Removing Symlink: /root/ml-training-jupyter-notebooks/Utilities/PythonHttpFileServer.py -> /PythonHttpFileServer.py\n",
      "Removing Symlink: /root/ml-training-jupyter-notebooks/Utilities/Using Progressbars.ipynb -> /Using Progressbars.ipynb\n",
      "Removing Symlink: /root/ml-training-jupyter-notebooks/Utilities/parallelization.py -> /parallelization.py\n",
      "Removing Symlink: /root/ml-training-jupyter-notebooks/Utilities/spark_helper.py -> /spark_helper.py\n",
      "Removing Symlink: /root/ml-training-jupyter-notebooks/Utilities/utilities.py -> /utilities.py\n",
      "Removing Symlink: /root/ml-training-jupyter-notebooks/Utilities/Utilities.egg-info -> /Utilities.egg-info\n",
      "Removing Symlink: /root/ml-training-jupyter-notebooks/Utilities/__pycache__ -> /__pycache__\n"
     ]
    }
   ],
   "source": [
    "spark_helper.unlink_dir_from_root(utilities_dir_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
