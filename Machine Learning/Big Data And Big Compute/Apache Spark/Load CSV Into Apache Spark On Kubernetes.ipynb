{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "In this notebook we are going to load data from our machine into a spark cluster.\n",
    "\n",
    "## Prerequisites\n",
    "It assumes you already have a running spark cluster. In our case we have prepared our spark cluster to run on kubernetes. If you haven't done so already, read through the following notebooks to get setup:\n",
    "- [Install Apache Spark Prerequisites](Install%20Apache%20Spark%20Prerequisites.ipynb)\n",
    "- [Running Apache Spark On Kubernetes](Running%20Apache%20Spark%20On%20Kubernetes.ipynb)\n",
    "- [Intro To Koalas](Intro%20To%20Koalas.ipynb)\n",
    "\n",
    "Note: We will see that the instructions are basically the same as [Load CSV Into Apache Spark Locally](Load%20CSV%20Into%20Apache%20Spark%20Locally.ipynb) once you get the kubernetes stuff setup.\n",
    "\n",
    "## Adjenda\n",
    "1. Understand Architecture\n",
    "2. Create SparkContext\n",
    "3. Setup Datastore\n",
    "4. Load Data\n",
    "5. Cleanup Spark and Kubernetes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Understand Atchitecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start working, we need to understand a few things related to architecture.\n",
    "\n",
    "If we think of the spark cluster as a server and our jupyter noteboook as a client we will see that there are two places from which data can be loaded. \n",
    "- Option 1: We load data from a file on the client\n",
    "- Option 2: We load data from a file on the workers\n",
    "\n",
    "Currently the Spark/Koalas framework provides a means for accomplishing both options but each have their own caveats which I will briefly describe. \n",
    "\n",
    "With Option 1, the process of loading the data is to create a pandas dataframe, and then create a koalas dataframe from it. The code would look something like this:\n",
    "```python\n",
    "import pandas\n",
    "from databricks import koalas\n",
    "\n",
    "pdf = pandas.read_csv(...)\n",
    "kdf = koalas.DataFrame(pdf)\n",
    "```\n",
    "\n",
    "But, in order to create the pandas dataframe, we will need to be able to load all the data into memory which is a deal breaker when working with big data; we simply do not have enough memory to load such a large dataset on one machine.\n",
    "\n",
    "With Option 2, the process of loading data is to instruct the spark workers to load a file from their local filesystem. The Caveat is that Spark has made an assumption that the driver and all of the workers have access to the same network file system mounted in the same place. I think this stems back to the HDFS days but I am not 100% sure. In our case, which is often the case, we do not have such a thing setup. Instead we will have to hack some utility function together. First we will need to move data from our *Example Data Sets* directory to the file system root (in our case symlink). Then take advantage of a Spark utility which will run a job which downloads a file from a URL to the root of the local filesystem on all the spark workers. Finally We will setup a simple http server to serve local files to the spark cluster. \n",
    "\n",
    "**Note**: It should be noted that this is strictly for educational purposes and is not inteded for production use. For production use, configure the Spark Worers to mount the network storage solution. This avoids data duplication and cuts down on the wall time because the data has less hops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2. Create SparkContext\n",
    "The spark context is the object which allows us interact with the spark cluster and submit jobs etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/ml-training-jupyter-notebooks\n"
     ]
    }
   ],
   "source": [
    "import pyprojroot\n",
    "project_root_dir  = pyprojroot.here()\n",
    "print(project_root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading module: /root/ml-training-jupyter-notebooks/Utilities/spark_helper.py\n"
     ]
    }
   ],
   "source": [
    "# Load a helper module\n",
    "import os\n",
    "import importlib.util\n",
    "module_name = \"spark_helper\"\n",
    "module_dir = os.path.join(project_root_dir, \"Utilities\", \"{0}.py\".format(module_name))\n",
    "if not os.path.exists(module_dir):\n",
    "    print(\"The helper module does not exist\")\n",
    "print(\"Loading module: {0}\".format(module_dir))\n",
    "spec = importlib.util.spec_from_file_location(module_name, module_dir)\n",
    "spark_helper = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(spark_helper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting SPARK_HOME\n",
      "/opt/spark\n",
      "\n",
      "Running findspark.init() function\n",
      "['/opt/spark/python', '/opt/spark/python/lib/py4j-0.10.9-src.zip', '/usr/lib64/python36.zip', '/usr/lib64/python3.6', '/usr/lib64/python3.6/lib-dynload', '', '/usr/local/lib64/python3.6/site-packages', '/usr/local/lib/python3.6/site-packages', '/usr/lib64/python3.6/site-packages', '/usr/lib/python3.6/site-packages', '/usr/local/lib/python3.6/site-packages/IPython/extensions', '/root/.ipython']\n",
      "\n",
      "Setting PYSPARK_PYTHON\n",
      "/usr/bin/python3\n",
      "\n",
      "Configuring URL for kubernetes master\n",
      "k8s://https://15.4.7.11:6443\n",
      "\n",
      "Determining IP Of Server\n",
      "The ip was detected as: 15.4.12.12\n",
      "\n",
      "Creating SparkConf Object\n",
      "('spark.master', 'k8s://https://15.4.7.11:6443')\n",
      "('spark.app.name', 'spark-jupyter-mlib')\n",
      "('spark.submit.deploy.mode', 'cluster')\n",
      "('spark.kubernetes.container.image', 'tschneider/pyspark:v5')\n",
      "('spark.kubernetes.namespace', 'spark')\n",
      "('spark.kubernetes.pyspark.pythonVersion', '3')\n",
      "('spark.kubernetes.authenticate.driver.serviceAccountName', 'spark-sa')\n",
      "('spark.kubernetes.authenticate.serviceAccountName', 'spark-sa')\n",
      "('spark.executor.instances', '3')\n",
      "('spark.executor.cores', '2')\n",
      "('spark.executor.memory', '4096m')\n",
      "('spark.executor.memoryOverhead', '1024m')\n",
      "('spark.driver.memory', '1024m')\n",
      "('spark.driver.host', '15.4.12.12')\n",
      "('spark.files.overwrite', 'true')\n",
      "\n",
      "Creating SparkSession Object\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "spark_app_name = \"spark-jupyter-mlib\"\n",
    "docker_image = \"tschneider/pyspark:v5\"\n",
    "k8_master_ip = \"15.4.7.11\"\n",
    "spark_session = spark_helper.create_spark_session(spark_app_name, docker_image, k8_master_ip)\n",
    "sc = spark_session.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** We can look at kubernetes to see that out worker nodes were created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                         READY     STATUS    RESTARTS   AGE\n",
      "spark-jupyter-mlib-f768867dc4b6db7c-exec-1   1/1       Running   0          18s\n",
      "spark-jupyter-mlib-f768867dc4b6db7c-exec-2   1/1       Running   0          18s\n",
      "spark-jupyter-mlib-f768867dc4b6db7c-exec-3   1/1       Running   0          18s\n"
     ]
    }
   ],
   "source": [
    "! kubectl -n spark get pod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. Setup Datastore\n",
    "As mentioned earlier, we are going to create a webserver to serve files from our local machine to the spark cluster. In order to be able to run the web server, without having the jupyter cell run forever and prevent us from moving on with out work, we will run it in a separate thread. If you are not familiar with threads etc I suggest reading up. The key things to know are that the web server will run as a separate process and python doesn't provide an out of the box way to kill the thread. If you fudged the configs... restart the kernel to kill the web server.\n",
    "\n",
    "This web server will be configured to serve all files from a given *web_root* directory. In this demo, I set the directory to the *./Example Data Sets* folder at the root of this project.\n",
    "\n",
    "**Note**: This is just for testing and small scale EDA. This is not intended for production use cases!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Symlink Local Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Symlink: /root/ml-training-jupyter-notebooks/Example Data Sets/Test Scores.csv -> /Test Scores.csv\n",
      "Creating Symlink: /root/ml-training-jupyter-notebooks/Example Data Sets/nasdaq_2019.csv -> /nasdaq_2019.csv\n",
      "Creating Symlink: /root/ml-training-jupyter-notebooks/Example Data Sets/results.csv -> /results.csv\n"
     ]
    }
   ],
   "source": [
    "data_dir_name = \"Example Data Sets\"\n",
    "data_dir_path = os.path.join(project_root_dir, data_dir_name)\n",
    "spark_helper.link_data_dir_to_root(data_dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Load Web Server Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the module for the web server we wrote\n",
    "import importlib.util\n",
    "spec = importlib.util.spec_from_file_location(\"PythonHttpFileServer\", \"../../../Utilities/PythonHttpFileServer.py\")\n",
    "PythonHttpFileServer = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(PythonHttpFileServer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Start Web Server\n",
    "**Note**: When setting a web root, or working with files, keep in mind that URLs need to escape special characters. I have set my webroot to the Example Data Directory rather than the project root so I dont have to escape anything. That is also why I have named the file with the characters I that I did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_dir_name = \"Example Data Sets\"\n",
    "web_root = os.path.join(project_root_dir, data_dir_name)\n",
    "\n",
    "if not os.path.exists(web_root):\n",
    "    raise Exception(\"The web root for the server does not exist.\")\n",
    "\n",
    "csv_file_name = \"nasdaq_2019.csv\"\n",
    "csv_file_path = os.path.join(web_root, csv_file_name)\n",
    "\n",
    "if not os.path.exists(csv_file_path):\n",
    "    raise Exception(\"The data file does not exist.\")\n",
    "    \n",
    "print(\"Web root and data file exist!\")\n",
    "print(\"web root: {0}\".format(web_root))\n",
    "print(\"data file: {0}\".format(csv_file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the library\n",
    "import threading\n",
    "\n",
    "# Configure the logger and log level (incase we need/want to debug)\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Create and start the thread if it doesnt exist\n",
    "var_exists = 'web_server_thread' in locals() or 'web_server_thread' in globals()\n",
    "if not var_exists:\n",
    "    web_server_port = 80\n",
    "    web_server_args = (web_server_port, web_root)\n",
    "    web_server_thread = threading.Thread(target=PythonHttpFileServer.run_server, args=web_server_args)\n",
    "    web_server_thread.start()\n",
    "else:\n",
    "    print(\"Web Server thread already exists\")\n",
    "    print(\"To kill it you need to restart the kernel.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Load The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data is not as intuitive as one would think. We Instruct the spark cluster to download a file from the web server. But when we do this, the file is not actually downloaded. Remember, spark is lazy. Instead, a link to the url is stored in the spark session object so that the file can be downloaded when we need to perform an operation on it.\n",
    "\n",
    "Later, we will tell spark to return a handle to a dataframe consisting of the data contained in this url. At that point, lazy spark, will download the data file to each worker (at their root), open the file, and distribute the data accross the cluster. We will see proof of this when we call the addFile() function. Our server logs will show that the workers are making web requests to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Add the file using Spark Context\n",
    "We will use the *addFiles()* function available on the SparkContect object to download a file to the workers. Behind the scenes, this file submits a job to the cluster. So the file isnt actually downloaded until I do some work. This is made apparent when the server logs pop up in a jupyter cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_address = spark_helper.determine_ip_address()\n",
    "csv_file_url = \"http://{0}:{1}/{2}\".format(ip_address, web_server_port, csv_file_name)\n",
    "print(\"Uploading file '{0}' to Spark cluster.\".format(csv_file_url))\n",
    "sc.addFile(csv_file_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Use koalas to open the file on spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the utility function to convert a date string to a datetime object from our utilities module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the utilities module we wrote\n",
    "import importlib.util\n",
    "spec = importlib.util.spec_from_file_location(\"utilities\", \"../../../Utilities/utilities.py\")\n",
    "utilities = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(utilities)\n",
    "\n",
    "# Define a mapping to convert our data field to the correct type\n",
    "converter_mapping = {\n",
    "    \"date\": utilities.convert_date_string_to_date\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Load our OHCLV data Into a koalas dataframe and pull out a single day in the say way we would in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoid a warning\n",
    "import os\n",
    "os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\n",
    "\n",
    "from databricks import koalas\n",
    "koalas_dataframe = koalas.read_csv(u\"file:///nasdaq_2019.csv\", converters=converter_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "koalas_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should see the workers download the file in the logs. If we log into the nodes we can see the file is located on the filesystem root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! kubectl -n spark get pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! kubectl -n spark exec -ti spark-jupyter-mlib-51eab07dc4a02d17-exec-1 -- find / -name nasdaq_2019.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! kubectl -n spark get pod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_helper.unlink_data_dir_from_root(data_dir_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
