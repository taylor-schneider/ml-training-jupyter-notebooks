{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "Previously we have seen examples of running the k-means algorithm provided by scikit-learn. In this notebook we are going to look at the Apache Spark MLib implimentation instead. Unlike the models packaged with scikit-learn, Apache Spark models are built to be distributed and can parallelize calculations. This fact can cause some headaches due to the assumptions/design that the spark framework asserts. We will cover this is the gotcha section.\n",
    "\n",
    "It assumes you have already read the following notebooks:\n",
    "- [Install Apache Spark Prerequisites](Install%20Apache%20Spark%20Prerequisites.ipynb)\n",
    "- [Spark Pi - The Hello World Example For Apache spark](Spark%20Pi%20-%20The%20Hello%20World%20Example%20For%20Apache%20spark.ipynb)\n",
    "- [Intro To Koalas](Intro%20To%20Koalas.ipynb)\n",
    "- [K-Means](../../Algorithms/Unsupervised%20Learning/Cluster%20Analysis/K-Means.ipynb)\n",
    "- [Load CSV Into Apache Spark On Kubernetes](Load%20CSV%20Into%20Apache%20Spark%20On%20Kubernetes.ipynb)\n",
    "\n",
    "The instructions are basically the same as [Running Scikit-Learn Apache Spark](Running%20Scikit-Learn%20Apache%20Spark.ipynb) once you get the kubernetes stuff setup.\n",
    "\n",
    "## Adjenda\n",
    "1. Create SparkContext\n",
    "2. Create Web Server To Host Data\n",
    "3. Load The Data\n",
    "8. Prepare Worker Nodes\n",
    "9. Submit Python Code To Spark Cluster\n",
    "10. Cleanup Spark and Kubernetes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gotchas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Doesnt Support Nested Parallelism\n",
    "\n",
    "Apache Spark doesn't support any form of nesting in terms of spark managed parallelism. Distributed operations can be initialized only by the driver (ie. not in a worker process created by the driver). This includes access to distributed data structures, like Spark DataFrame, and execution of parallel processing functions, like training an MLlib algorithm.\n",
    "\n",
    "If you tried to have a spark worker create and train an MLlib algorithm you would see the following error pop up:\n",
    "\n",
    "```\n",
    "AttributeError: Cannot load _jvm from SparkContext. Is SparkContext initialized?\n",
    "```\n",
    "\n",
    "This took me a while to figure out, but it is pointed out [here](https://coderedirect.com/questions/310003/run-ml-algorithm-inside-map-function-in-spark)\n",
    "\n",
    "This is inconvenient as spark provides a number of useful mechanism for kicking off parallel processes based on our dataset. For example, in the previous notebook we use the groupby(criteria).apply(func) function to kick off a training job for each group of data in parallel. We would not be able to use this api with mlib as the function func is executed on the worker where a spark context is not found.\n",
    "\n",
    "We can however manage the parallelism outside spark from the driver node. We can do this using multithreading or multiprocessing. Before we get into those topics, we need a refresher on operating system design: An operating system is in charge of running processes. The OS allocates memory and CPU resources for the process. Once allocated the process can utilize the resources as it likes. Multiprocessing is when a program asks the OS to spin up multiple \"subprocesses\" each with their own separate memory and cpu allocations. Multithreading is when a single process created multiple threads to execute work in parrallel instead of multiple processes. Multithreading allows for threads to share memory and cpu resources allocated to the process. Multiprocessing does not.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Create SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/ml-training-jupyter-notebooks\n"
     ]
    }
   ],
   "source": [
    "import pyprojroot\n",
    "project_root_dir  = pyprojroot.here()\n",
    "print(project_root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading module: /root/ml-training-jupyter-notebooks/Utilities/spark_helper.py\n"
     ]
    }
   ],
   "source": [
    "# Load a helper module\n",
    "import os\n",
    "import importlib.util\n",
    "module_name = \"spark_helper\"\n",
    "module_dir = os.path.join(project_root_dir, \"Utilities\", \"{0}.py\".format(module_name))\n",
    "if not os.path.exists(module_dir):\n",
    "    print(\"The helper module does not exist\")\n",
    "print(\"Loading module: {0}\".format(module_dir))\n",
    "spec = importlib.util.spec_from_file_location(module_name, module_dir)\n",
    "spark_helper = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(spark_helper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting SPARK_HOME\n",
      "/opt/spark\n",
      "\n",
      "Running findspark.init() function\n",
      "['/opt/spark/python', '/opt/spark/python/lib/py4j-0.10.9-src.zip', '/usr/lib64/python36.zip', '/usr/lib64/python3.6', '/usr/lib64/python3.6/lib-dynload', '', '/usr/local/lib64/python3.6/site-packages', '/usr/local/lib/python3.6/site-packages', '/usr/lib64/python3.6/site-packages', '/usr/lib/python3.6/site-packages', '/usr/local/lib/python3.6/site-packages/IPython/extensions', '/root/.ipython']\n",
      "\n",
      "Setting PYSPARK_PYTHON\n",
      "/usr/bin/python3\n",
      "\n",
      "Configuring URL for kubernetes master\n",
      "k8s://https://15.4.7.11:6443\n",
      "\n",
      "Determining IP Of Server\n",
      "The ip was detected as: 15.4.12.12\n",
      "\n",
      "Creating SparkConf Object\n",
      "('spark.master', 'k8s://https://15.4.7.11:6443')\n",
      "('spark.app.name', 'spark-jupyter-mlib')\n",
      "('spark.submit.deploy.mode', 'cluster')\n",
      "('spark.kubernetes.container.image', 'tschneider/pyspark:v5')\n",
      "('spark.kubernetes.namespace', 'spark')\n",
      "('spark.kubernetes.pyspark.pythonVersion', '3')\n",
      "('spark.kubernetes.authenticate.driver.serviceAccountName', 'spark-sa')\n",
      "('spark.kubernetes.authenticate.serviceAccountName', 'spark-sa')\n",
      "('spark.executor.instances', '3')\n",
      "('spark.executor.cores', '2')\n",
      "('spark.executor.memory', '4096m')\n",
      "('spark.executor.memoryOverhead', '1024m')\n",
      "('spark.driver.memory', '1024m')\n",
      "('spark.driver.host', '15.4.12.12')\n",
      "('spark.files.overwrite', 'true')\n",
      "('spark.files.useFetchCache', 'false')\n",
      "\n",
      "Creating SparkSession Object\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "spark_app_name = \"spark-jupyter-mlib\"\n",
    "docker_image = \"tschneider/pyspark:v5\"\n",
    "k8_master_ip = \"15.4.7.11\"\n",
    "spark_session = spark_helper.create_spark_session(spark_app_name, docker_image, k8_master_ip)\n",
    "sc = spark_session.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                         READY     STATUS    RESTARTS   AGE\n",
      "spark-jupyter-mlib-5a59ba7dc9fe5110-exec-1   1/1       Running   0          33s\n",
      "spark-jupyter-mlib-5a59ba7dc9fe5110-exec-2   1/1       Running   0          33s\n",
      "spark-jupyter-mlib-5a59ba7dc9fe5110-exec-3   1/1       Running   0          33s\n"
     ]
    }
   ],
   "source": [
    "! kubectl -n spark get pods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Setup Datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_name = \"Example Data Sets\"\n",
    "data_dir_path = os.path.join(project_root_dir, data_dir_name)\n",
    "spark_helper.link_data_dir_to_root(data_dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the module for the web server we wrote\n",
    "import importlib.util\n",
    "spec = importlib.util.spec_from_file_location(\"PythonHttpFileServer\", \"../../../Utilities/PythonHttpFileServer.py\")\n",
    "PythonHttpFileServer = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(PythonHttpFileServer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Web root and data file exist!\n",
      "web root: /root/ml-training-jupyter-notebooks/Example Data Sets\n",
      "data file: /root/ml-training-jupyter-notebooks/Example Data Sets/nasdaq_2019.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "data_dir_name = \"Example Data Sets\"\n",
    "web_root = os.path.join(project_root_dir, data_dir_name)\n",
    "\n",
    "if not os.path.exists(web_root):\n",
    "    raise Exception(\"The web root for the server does not exist.\")\n",
    "\n",
    "csv_file_name = \"nasdaq_2019.csv\"\n",
    "csv_file_path = os.path.join(web_root, csv_file_name)\n",
    "\n",
    "if not os.path.exists(csv_file_path):\n",
    "    raise Exception(\"The data file does not exist.\")\n",
    "    \n",
    "print(\"Web root and data file exist!\")\n",
    "print(\"web root: {0}\".format(web_root))\n",
    "print(\"data file: {0}\".format(csv_file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Starting server on port 80\n",
      "INFO:root:Web root specified as: /root/ml-training-jupyter-notebooks/Example Data Sets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app 'PythonHttpFileServer' (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:werkzeug: * Running on all addresses.\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "INFO:werkzeug: * Running on http://15.4.12.12:80/ (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "# Import the library\n",
    "import threading\n",
    "\n",
    "# Configure the logger and log level (incase we need/want to debug)\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Create and start the thread if it doesnt exist\n",
    "var_exists = 'web_server_thread' in locals() or 'web_server_thread' in globals()\n",
    "if not var_exists:\n",
    "    web_server_port = 80\n",
    "    web_server_args = (web_server_port, web_root)\n",
    "    web_server_thread = threading.Thread(target=PythonHttpFileServer.run_server, args=web_server_args)\n",
    "    web_server_thread.start()\n",
    "else:\n",
    "    print(\"Web Server thread already exists\")\n",
    "    print(\"To kill it you need to restart the kernel.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Load The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Add File To Spark Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Get /root/ml-training-jupyter-notebooks/Example Data Sets/nasdaq_2019.csv\n",
      "INFO:werkzeug:15.4.12.12 - - [17/Dec/2021 20:08:09] \"GET /nasdaq_2019.csv HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading file 'http://15.4.12.12:80/nasdaq_2019.csv' to Spark cluster.\n"
     ]
    }
   ],
   "source": [
    "ip_address = spark_helper.determine_ip_address()\n",
    "csv_file_name = \"nasdaq_2019.csv\"\n",
    "csv_file_url = \"http://{0}:{1}/{2}\".format(ip_address, web_server_port, csv_file_name)\n",
    "print(\"Uploading file '{0}' to Spark cluster.\".format(csv_file_url))\n",
    "sc.addFile(csv_file_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Use Koalas To Load Data File Into DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the utility function to convert a date string to a datetime object from our utilities module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the utilities module we wrote\n",
    "import importlib.util\n",
    "spec = importlib.util.spec_from_file_location(\"utilities\", \"../../../Utilities/utilities.py\")\n",
    "utilities = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(utilities)\n",
    "\n",
    "# Define a mapping to convert our data field to the correct type\n",
    "converter_mapping = {\n",
    "    \"date\": utilities.convert_date_string_to_date\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Load our OHCLV data Into a koalas dataframe and pull out a single day in the say way we would in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Get /root/ml-training-jupyter-notebooks/Example Data Sets/nasdaq_2019.csv\n",
      "INFO:werkzeug:15.4.7.101 - - [17/Dec/2021 20:08:12] \"GET /nasdaq_2019.csv HTTP/1.1\" 200 -\n",
      "INFO:root:Get /root/ml-training-jupyter-notebooks/Example Data Sets/nasdaq_2019.csv\n",
      "INFO:werkzeug:15.4.7.103 - - [17/Dec/2021 20:08:13] \"GET /nasdaq_2019.csv HTTP/1.1\" 200 -\n",
      "INFO:root:Get /root/ml-training-jupyter-notebooks/Example Data Sets/nasdaq_2019.csv\n",
      "INFO:werkzeug:15.4.7.102 - - [17/Dec/2021 20:08:13] \"GET /nasdaq_2019.csv HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "# Avoid a warning\n",
    "import os\n",
    "os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\n",
    "os.environ[\"SPARK_KOALAS_AUTOPATCH\"] = \"0\"\n",
    "\n",
    "from databricks import koalas\n",
    "koalas_dataframe = koalas.read_csv(u\"file:////nasdaq_2019.csv\", converters=converter_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>interval</th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AABA</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>70.90</td>\n",
       "      <td>71.5200</td>\n",
       "      <td>70.3250</td>\n",
       "      <td>70.57</td>\n",
       "      <td>10234800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAL</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>33.14</td>\n",
       "      <td>33.6632</td>\n",
       "      <td>32.5301</td>\n",
       "      <td>32.88</td>\n",
       "      <td>8995100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAME</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>2.43</td>\n",
       "      <td>2.4300</td>\n",
       "      <td>2.4000</td>\n",
       "      <td>2.40</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAOI</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>10.70</td>\n",
       "      <td>10.8900</td>\n",
       "      <td>10.0100</td>\n",
       "      <td>10.18</td>\n",
       "      <td>883100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAON</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>50.57</td>\n",
       "      <td>50.9850</td>\n",
       "      <td>48.5600</td>\n",
       "      <td>49.73</td>\n",
       "      <td>180200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker interval        date   open     high      low  close    volume\n",
       "0   AABA        D  2019-07-01  70.90  71.5200  70.3250  70.57  10234800\n",
       "1    AAL        D  2019-07-01  33.14  33.6632  32.5301  32.88   8995100\n",
       "2   AAME        D  2019-07-01   2.43   2.4300   2.4000   2.40       500\n",
       "3   AAOI        D  2019-07-01  10.70  10.8900  10.0100  10.18    883100\n",
       "4   AAON        D  2019-07-01  50.57  50.9850  48.5600  49.73    180200"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "koalas_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Submit Python Code To Spark Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section of the notebook we are going to apply the kmeans algorithm from Spark MLlib to each date in our koalas_dataframe object.\n",
    "\n",
    "To do this, we are going to write a function that applies the algorithm to a dataframe; the assumption being the dataframe only contains data related to the same date.\n",
    "\n",
    "**Note**: Most of this is a review and reworking of the content contained in the [K-Means Notebook](../../Algorithms/Unsupervised%20Learning/Cluster%20Analysis/K-Means.ipynb).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Setup And Test  Utility Function\n",
    "We create our data frame for testing based on a subset of our real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_01_02_2019 = koalas_dataframe.loc[koalas_dataframe[\"date\"] == '2019-01-02'].copy()\n",
    "df_01_02_2019.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then write and test our function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import pyspark\n",
    "from pyspark.sql.functions import pandas_udf, udf\n",
    "from pyspark.sql.types import *\n",
    "from databricks import koalas\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "koalas.set_option(\"compute.ops_on_diff_frames\", True)\n",
    "\n",
    "def perform_kmeans_on_dataframe(df, column_names):\n",
    "    \n",
    "    # Create a copy of our dataframe so we can play around\n",
    "    tmp = df.copy()\n",
    "    columns = column_names.copy()\n",
    "    \n",
    "    # Create our model\n",
    "    model = KMeans().setK(5).setSeed(42)\n",
    "\n",
    "    # Do some magic to get the data in the right format for the spark model\n",
    "    from pyspark.ml.feature import VectorAssembler\n",
    "    assembler = VectorAssembler(inputCols=column_names, outputCol=\"features\")\n",
    "    if type(tmp) == koalas.frame.DataFrame:\n",
    "        model_parameters = assembler.transform(tmp[[*columns]].to_spark())\n",
    "    elif type(tmp) == pandas.DataFrame:\n",
    "        tmp = koalas.DataFrame(tmp)\n",
    "        model_parameters = assembler.transform(tmp[[*columns]].to_spark())\n",
    "\n",
    "    # Train the model\n",
    "    trained_model = model.fit(model_parameters)\n",
    "    \n",
    "    # Extract the cluster information for the training data\n",
    "    predictions = trained_model.transform(model_parameters)\n",
    "    cluster_indices = predictions.select(\"prediction\")\n",
    "    cluster_indices = koalas.DataFrame(cluster_indices).to_numpy().reshape(-1)\n",
    "    cluster_indices = koalas.Series(cluster_indices, index=tmp.index.to_numpy())\n",
    "    tmp[\"cluster_indices\"] = cluster_indices\n",
    "    cluster_centroids = trained_model.clusterCenters()\n",
    "    tmp[\"cluster_centroids\"] = tmp[\"cluster_indices\"].apply(lambda i: str(cluster_centroids[i]))\n",
    "\n",
    "    return tmp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_kmeans_on_dataframe(df_01_02_2019, column_names=[\"open\", \"close\"]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: The warning above is coming from code used in the internals of Koalas. Do not worry about this warning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Run Utility Function In Parrallel\n",
    "Now that the utility function has been tested on smaller dataframes, we are safe to run it on a large data set in parrallel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2019-01-01'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a list of the dates in our dataframe\n",
    "dates = koalas_dataframe[\"date\"].unique().sort_values().to_numpy()\n",
    "dates[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the dataframe which is no longer needed.\n",
    "if 'koalas_dataframe' in locals() or 'koalas_dataframe' in globals():\n",
    "    del koalas_dataframe\n",
    "if 'df_01_02_2019' in locals() or 'df_01_02_2019' in globals():\n",
    "    del df_01_02_2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to build some utilities to facilitate the parralization of these opersations. As we mentioned in the Gotchas section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a wrapper function to call our kmeans function on our data and do some other managerial things \n",
    "import time\n",
    "\n",
    "def thread_func(params, retries=5):\n",
    "\n",
    "    global completed_ops\n",
    "\n",
    "    # Retrieve params\n",
    "    date = params[0]\n",
    "    progress_bar = params[1]\n",
    "    lock = params[2]\n",
    "    input_file_path = params[3]\n",
    "    result_file_path = params[4]\n",
    "    thread_result = None\n",
    "    try:\n",
    "        # Load data\n",
    "        thread_df = koalas.read_csv(input_file_path, converters=converter_mapping)\n",
    "                \n",
    "        while True:\n",
    "            try:\n",
    "                # Train the model\n",
    "                date_df = thread_df.loc[thread_df[\"date\"] == date]    \n",
    "                thread_result = perform_kmeans_on_dataframe(date_df, column_names=[\"open\", \"close\"])\n",
    "\n",
    "                # Force spark to not be lazy and to do the computation\n",
    "                thread_result.shape  \n",
    "\n",
    "                # Record the results to a local data file\n",
    "                lock.acquire()\n",
    "                if completed_ops == 0:                    \n",
    "                    thread_result.to_pandas().to_csv(result_file_path, mode='a', index=False)\n",
    "                else:\n",
    "                    thread_result.to_pandas().to_csv(result_file_path, mode='a', index=False, header=False)\n",
    "                lock.release()\n",
    "                \n",
    "                # Return results and timing info to the ThreadHelper\n",
    "                return thread_result\n",
    "\n",
    "            except Exception as e:\n",
    "                retries -= 1\n",
    "                if retries > 0:\n",
    "                    time.sleep(1)\n",
    "                else:\n",
    "                    raise e\n",
    "    finally:  \n",
    "        # Update the progress bar\n",
    "        lock.acquire()\n",
    "        if thread_result is not None:\n",
    "            completed_ops += 1\n",
    "            progress_bar.update(completed_ops)\n",
    "        lock.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write a utility function to create a progress bar (again, magagerial stuff for parrallelism)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import progressbar\n",
    "\n",
    "def create_progress_bar(num_ops):\n",
    "\n",
    "    progress_bar_widgets = [\n",
    "        progressbar.Bar('=', '[', ']'), \n",
    "        ' ', \n",
    "        progressbar.FormatLabel('Processed: %(value)d / {0} ops'.format(num_ops)),\n",
    "        ' ', \n",
    "        progressbar.ETA()\n",
    "    ]\n",
    "    return  progressbar.ProgressBar(maxval=num_ops, widgets=progress_bar_widgets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function to kick things off in parrallel and returns the results. The trick here is that the results file is going to be stored in our example datasets folder and served to the workers via our web server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a ThreadPool and kick off the parrallel training sessions\n",
    "import os\n",
    "from multiprocessing.pool import ThreadPool\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "import threading\n",
    "\n",
    "def run_multithreaded_kmeans(dates):\n",
    "    try:\n",
    "        print(\"Create vars to help with synchronization\")\n",
    "        mutex = threading.Lock()\n",
    "        num_threads = 10\n",
    "        thread_pool = ThreadPool(num_threads)\n",
    "            \n",
    "        print(\"Create result file to store results from threads\")\n",
    "        project_root_dir  = pyprojroot.here()\n",
    "        result_file_name = \"results.csv\"\n",
    "        result_file_path = os.path.join(project_root_dir, \"Example Data Sets\", result_file_name)\n",
    "        do_work = False\n",
    "        if not os.path.exists(result_file_path):          \n",
    "            with open(result_file_path, 'w') as fp:\n",
    "                pass\n",
    "            do_work = True\n",
    "        else:\n",
    "            print(\"No work to do, results file already exists!\")\n",
    "        \n",
    "        print(\"Setting up local datastore for driver\")\n",
    "        data_dir_name = \"Example Data Sets\"\n",
    "        data_dir_path = os.path.join(project_root_dir, data_dir_name)\n",
    "        spark_helper.link_data_dir_to_root(data_dir_path)\n",
    "\n",
    "        if do_work:\n",
    "            print(\"No result file exists, doing work...\")\n",
    "            start = datetime.now()\n",
    "            print(\"Starting: {0}\".format(start))\n",
    "            \n",
    "            print(\"Create objects to help track multithreading progress\")\n",
    "            num_ops = len(dates)\n",
    "            bar = create_progress_bar(num_ops)\n",
    "            bar.start()\n",
    "            global completed_ops\n",
    "            completed_ops = 0\n",
    "        \n",
    "            print(\"Create an iterator of params for the thread function\")\n",
    "            \n",
    "            input_file_name = \"nasdaq_2019.csv\"\n",
    "            input_file_path = \"file:///{0}\".format(input_file_name)\n",
    "        \n",
    "            iterator = zip(dates, \n",
    "                           itertools.repeat(bar), \n",
    "                           itertools.repeat(mutex),\n",
    "                           itertools.repeat(input_file_path),\n",
    "                           itertools.repeat(result_file_path))\n",
    "\n",
    "            print(\"Run training sessions for each data in parrallel\")\n",
    "            results = thread_pool.map(thread_func, iterator)\n",
    "\n",
    "            # Record the end time\n",
    "            calc_end = datetime.now()\n",
    "            calc_diff = (calc_end - start).total_seconds()\n",
    "            print(\"Ending: {0}\".format(calc_end))\n",
    "            print(\"Total calculation time: {0}s\".format(calc_diff))\n",
    "            date_diff = calc_diff / num_ops\n",
    "            print(\"Time per date: {0}s\".format(date_diff))\n",
    "\n",
    "        print(\"Load results file\")\n",
    "        ip_address = spark_helper.determine_ip_address()\n",
    "        web_server_port = 80\n",
    "        result_file_url = \"http://{0}:{1}/{2}\".format(ip_address, web_server_port, result_file_name)\n",
    "        worker_result_file_path = \"file:///{0}\".format(result_file_name)\n",
    "        try:\n",
    "            mutex.acquire()\n",
    "            sc.addFile(result_file_url)\n",
    "            merged_df = koalas.read_csv(worker_result_file_path, converters=converter_mapping)\n",
    "        finally:\n",
    "            mutex.release()\n",
    "        \n",
    "        return merged_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Cleanup spark\n",
    "        sc.cancelAllJobs()\n",
    "        # Raise error\n",
    "        raise e\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[                                         ] Processed: 0 / 4 ops ETA:  --:--:--\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create vars to help with synchronization\n",
      "Create result file to store results from threads\n",
      "Setting up local datastore for driver\n",
      "Creating Symlink: /root/ml-training-jupyter-notebooks/Example Data Sets/.ipynb_checkpoints -> /.ipynb_checkpoints\n",
      "No result file exists, doing work...\n",
      "Starting: 2021-12-17 20:13:31.214931\n",
      "Create objects to help track multithreading progress\n",
      "Create an iterator of params for the thread function\n",
      "Run training sessions for each data in parrallel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/pyspark/sql/pandas/functions.py:392: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  \"in the future releases. See SPARK-28264 for more details.\", UserWarning)\n",
      "[==========================================] Processed: 4 / 4 ops ETA:  0:00:00\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending: 2021-12-17 20:16:11.933703\n",
      "Total calculation time: 160.718772s\n",
      "Time per date: 40.179693s\n",
      "Load results file\n"
     ]
    }
   ],
   "source": [
    "# Run the calculation\n",
    "merged_df = run_multithreaded_kmeans(dates[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>interval</th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>cluster_indices</th>\n",
       "      <th>cluster_centroids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19217</th>\n",
       "      <td>APPF</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>59.22</td>\n",
       "      <td>59.22</td>\n",
       "      <td>59.22</td>\n",
       "      <td>59.22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[48.53718713 48.53718713]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19218</th>\n",
       "      <td>HMST</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>21.23</td>\n",
       "      <td>21.23</td>\n",
       "      <td>21.23</td>\n",
       "      <td>21.23</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[10.78294524 10.78294524]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19219</th>\n",
       "      <td>HYGS</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[10.78294524 10.78294524]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19220</th>\n",
       "      <td>IRIX</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>4.70</td>\n",
       "      <td>4.70</td>\n",
       "      <td>4.70</td>\n",
       "      <td>4.70</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[10.78294524 10.78294524]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19221</th>\n",
       "      <td>LBTYB</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>21.00</td>\n",
       "      <td>21.00</td>\n",
       "      <td>21.00</td>\n",
       "      <td>21.00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[10.78294524 10.78294524]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ticker interval        date   open   high    low  close  volume  cluster_indices          cluster_centroids\n",
       "19217   APPF        D  2019-01-01  59.22  59.22  59.22  59.22       0                0  [48.53718713 48.53718713]\n",
       "19218   HMST        D  2019-01-01  21.23  21.23  21.23  21.23       0                1  [10.78294524 10.78294524]\n",
       "19219   HYGS        D  2019-01-01   5.00   5.00   5.00   5.00       0                1  [10.78294524 10.78294524]\n",
       "19220   IRIX        D  2019-01-01   4.70   4.70   4.70   4.70       0                1  [10.78294524 10.78294524]\n",
       "19221  LBTYB        D  2019-01-01  21.00  21.00  21.00  21.00       0                1  [10.78294524 10.78294524]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the union df\n",
    "merged_df.loc[merged_df[\"date\"] == dates[0]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If anything went wrong, we should ask the SparkContext to kill any abandoned jobs that may be lingering in ghosted threads from the thread pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.cancelAllJobs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run a large set of dates using our threadpool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[                                       ] Processed: 0 / 158 ops ETA:  --:--:--\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create vars to help with synchronization\n",
      "Create result file to store results from threads\n",
      "Setting up local datastore for driver\n",
      "No result file exists, doing work...\n",
      "Starting: 2021-12-17 20:20:02.085403\n",
      "Create objects to help track multithreading progress\n",
      "Create an iterator of params for the thread function\n",
      "Run training sessions for each data in parrallel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[======================================] Processed: 158 / 158 ops ETA:  0:00:00\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending: 2021-12-17 21:03:34.666871\n",
      "Total calculation time: 2612.581468s\n",
      "Time per date: 16.535325746835444s\n",
      "Load results file\n"
     ]
    }
   ],
   "source": [
    "merged_df = run_multithreaded_kmeans(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>interval</th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>cluster_indices</th>\n",
       "      <th>cluster_centroids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>89522</th>\n",
       "      <td>APEN</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-10</td>\n",
       "      <td>3.79</td>\n",
       "      <td>3.79</td>\n",
       "      <td>3.63</td>\n",
       "      <td>3.630</td>\n",
       "      <td>2700</td>\n",
       "      <td>0</td>\n",
       "      <td>[13.64757638 13.71332506]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89523</th>\n",
       "      <td>ASFI</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-10</td>\n",
       "      <td>4.32</td>\n",
       "      <td>4.32</td>\n",
       "      <td>4.26</td>\n",
       "      <td>4.265</td>\n",
       "      <td>16100</td>\n",
       "      <td>0</td>\n",
       "      <td>[13.64757638 13.71332506]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89524</th>\n",
       "      <td>CPIX</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-10</td>\n",
       "      <td>6.45</td>\n",
       "      <td>6.50</td>\n",
       "      <td>6.11</td>\n",
       "      <td>6.110</td>\n",
       "      <td>3400</td>\n",
       "      <td>0</td>\n",
       "      <td>[13.64757638 13.71332506]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89525</th>\n",
       "      <td>CZWI</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-10</td>\n",
       "      <td>11.08</td>\n",
       "      <td>11.25</td>\n",
       "      <td>11.07</td>\n",
       "      <td>11.170</td>\n",
       "      <td>6200</td>\n",
       "      <td>0</td>\n",
       "      <td>[13.64757638 13.71332506]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89526</th>\n",
       "      <td>FTXH</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-10</td>\n",
       "      <td>20.16</td>\n",
       "      <td>20.16</td>\n",
       "      <td>19.92</td>\n",
       "      <td>20.050</td>\n",
       "      <td>1400</td>\n",
       "      <td>0</td>\n",
       "      <td>[13.64757638 13.71332506]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ticker interval        date   open   high    low   close  volume  cluster_indices          cluster_centroids\n",
       "89522   APEN        D  2019-01-10   3.79   3.79   3.63   3.630    2700                0  [13.64757638 13.71332506]\n",
       "89523   ASFI        D  2019-01-10   4.32   4.32   4.26   4.265   16100                0  [13.64757638 13.71332506]\n",
       "89524   CPIX        D  2019-01-10   6.45   6.50   6.11   6.110    3400                0  [13.64757638 13.71332506]\n",
       "89525   CZWI        D  2019-01-10  11.08  11.25  11.07  11.170    6200                0  [13.64757638 13.71332506]\n",
       "89526   FTXH        D  2019-01-10  20.16  20.16  19.92  20.050    1400                0  [13.64757638 13.71332506]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.loc[merged_df[\"date\"] == dates[7]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, every time we see the csv file get reloaded, we know that a worker crashed. I was watching the linux OS hosing the kubernetes/spark workers. If the CPU/Memory got pegged at 100% utilization the worker would crash. \n",
    "\n",
    "In some cases the driver may run into an issue as follows:\n",
    "\n",
    "```\n",
    "Py4JJavaError: An error occurred while calling o689322.createOrReplaceTempView.\n",
    ": java.lang.OutOfMemoryError: Java heap space\n",
    "```\n",
    "\n",
    "I also saw this. Only solution was adding retries\n",
    "\n",
    "```\n",
    "Py4JJavaError: An error occurred while calling o436701.csv.\n",
    ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 22489.0 failed 4 times, most recent failure: Lost task 3.3 in stage 22489.0 (TID 311712) (10.42.0.1 executor 2): java.io.EOFException: Cannot seek after EOF\n",
    "```\n",
    "\n",
    "For example when merging all the tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark_app_name = \"spark-jupyter-mlib\"\n",
    "docker_image = \"tschneider/pyspark:v5\"\n",
    "spark_master_url = \"15.4.7.11\"\n",
    "\n",
    "sparkConf = spark_helper.create_spark_context(spark_master_url, spark_app_name, docker_image)\n",
    "sparkConf.set(\"spark.files.useFetchCache\", \"false\")\n",
    "for item in sparkConf.getAll():\n",
    "    print(item)\n",
    "    \n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_session = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "sc = spark_session.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. Koalas will set it for you but it does not work if there is a Spark context already launched.\n",
      "INFO:spark:Patching spark automatically. You can disable it by setting SPARK_KOALAS_AUTOPATCH=false in your environment\n",
      "INFO:root:Get /root/ml-training-jupyter-notebooks/Example Data Sets/results.csv\n",
      "INFO:werkzeug:15.4.7.102 - - [17/Dec/2021 20:05:51] \"GET /results.csv HTTP/1.1\" 200 -\n",
      "INFO:root:Get /root/ml-training-jupyter-notebooks/Example Data Sets/results.csv\n",
      "INFO:root:Get /root/ml-training-jupyter-notebooks/Example Data Sets/results.csv\n",
      "INFO:werkzeug:15.4.7.103 - - [17/Dec/2021 20:05:57] \"GET /results.csv HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:15.4.7.101 - - [17/Dec/2021 20:05:57] \"GET /results.csv HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>interval</th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>cluster_indices</th>\n",
       "      <th>cluster_centroids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ATLO</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-29</td>\n",
       "      <td>25.14</td>\n",
       "      <td>25.37</td>\n",
       "      <td>25.04</td>\n",
       "      <td>25.13</td>\n",
       "      <td>5900</td>\n",
       "      <td>0</td>\n",
       "      <td>[14.97376351 14.92929989]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AXSM</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-29</td>\n",
       "      <td>8.24</td>\n",
       "      <td>8.26</td>\n",
       "      <td>7.63</td>\n",
       "      <td>8.15</td>\n",
       "      <td>741000</td>\n",
       "      <td>0</td>\n",
       "      <td>[14.97376351 14.92929989]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BATRK</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-29</td>\n",
       "      <td>26.75</td>\n",
       "      <td>27.14</td>\n",
       "      <td>26.71</td>\n",
       "      <td>26.72</td>\n",
       "      <td>108900</td>\n",
       "      <td>0</td>\n",
       "      <td>[14.97376351 14.92929989]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CALM</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-29</td>\n",
       "      <td>41.88</td>\n",
       "      <td>42.24</td>\n",
       "      <td>41.59</td>\n",
       "      <td>41.99</td>\n",
       "      <td>260700</td>\n",
       "      <td>0</td>\n",
       "      <td>[14.97376351 14.92929989]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DDIV</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-29</td>\n",
       "      <td>22.67</td>\n",
       "      <td>22.73</td>\n",
       "      <td>22.64</td>\n",
       "      <td>22.73</td>\n",
       "      <td>3500</td>\n",
       "      <td>0</td>\n",
       "      <td>[14.97376351 14.92929989]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker interval        date   open   high    low  close  volume  cluster_indices          cluster_centroids\n",
       "0   ATLO        D  2019-01-29  25.14  25.37  25.04  25.13    5900                0  [14.97376351 14.92929989]\n",
       "1   AXSM        D  2019-01-29   8.24   8.26   7.63   8.15  741000                0  [14.97376351 14.92929989]\n",
       "2  BATRK        D  2019-01-29  26.75  27.14  26.71  26.72  108900                0  [14.97376351 14.92929989]\n",
       "3   CALM        D  2019-01-29  41.88  42.24  41.59  41.99  260700                0  [14.97376351 14.92929989]\n",
       "4   DDIV        D  2019-01-29  22.67  22.73  22.64  22.73    3500                0  [14.97376351 14.92929989]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sc.addFile(\"http://15.4.12.12:80/results.csv\")\n",
    "import numpy\n",
    "def convert_date_string_to_date(input_string):\n",
    "\n",
    "    try:\n",
    "        # We then do our manipulation\n",
    "        input_string = input_string.strip()\n",
    "\n",
    "        # Make it a date\n",
    "        result = numpy.datetime64(input_string, 'D')\n",
    "\n",
    "        return result\n",
    "\n",
    "    except:\n",
    "        print(input_string)\n",
    "        raise\n",
    "\n",
    "converter_mapping = {\n",
    "    \"date\": convert_date_string_to_date\n",
    "}\n",
    "\n",
    "\n",
    "from databricks import koalas\n",
    "result_file_url = \"http://15.4.12.12:80/results.csv\"\n",
    "sc.addFile(result_file_url)\n",
    "worker_result_file_path = \"file:///results.csv\"\n",
    "merged_df = koalas.read_csv(worker_result_file_path, converters=converter_mapping)\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.addFile(result_file_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.clearFiles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark.SparkFiles.get(\"results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove(pyspark.SparkFiles.get(\"results.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foobar(idx):\n",
    "    import socket\n",
    "    return socket.gethostname()\n",
    "\n",
    "def delete_files(idx):\n",
    "    import os\n",
    "    if os.path.exists(\"/results.csv\"):\n",
    "        os.remove(\"/results.csv\")\n",
    "\n",
    "sc.parallelize([0,1,2]).map(delete_files).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If anything went wrong, we should ask the SparkContext to kill any abandoned jobs that may be lingering in ghosted threads from the thread pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.cancelAllJobs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that computing every date is almost as fast as computing a single date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Compare Results\n",
    "\n",
    "We will time ourselves when running against a single day vs all days to show the operations are occurring in parrallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 6. Cleanup Spark Cluster On Kubernetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if os.path.exists(csv_link_path) and os.path.islink(csv_link_path):\n",
    "    os.unlink(csv_link_path)\n",
    "    print(\"Deleted symlinked data file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! kubectl -n spark get pod"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
