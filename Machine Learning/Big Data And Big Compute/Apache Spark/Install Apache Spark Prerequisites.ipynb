{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "312f4a72-17a6-4937-b72d-a84fe4c0b2c2",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "In order to be able to submit python code to generate spark workloads we need to setup the following prerequisites:\n",
    "1. Install Java\n",
    "2. Install Apache Spark\n",
    "3. Install Programming Language\n",
    "4. Install Language Bindings\n",
    "\n",
    "If we will be leveraging the spark/kubernetes integration built into the SparkContext we will need to setup:\n",
    "\n",
    "5. Install Kubectl\n",
    "6. Configure Kubernetes To Host Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3f4d5a-d95f-4b95-98b2-e1baeb8b6cc3",
   "metadata": {},
   "source": [
    "# 1. Install Java"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0deeb83-68be-415c-ae36-d5de4d65d7de",
   "metadata": {},
   "source": [
    "According to the documentation Apark 3.1.1 requires Java 8/11. In the case of the openjdk, we will see a version of 1.8.x coresponding to Oracle version 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84e9621e-4a7a-41c8-bf6a-fb0955fa1801",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "java version \"1.8.0_291\"\n",
      "Java(TM) SE Runtime Environment (build 1.8.0_291-b10)\n",
      "Java HotSpot(TM) 64-Bit Server VM (build 25.291-b10, mixed mode)\n"
     ]
    }
   ],
   "source": [
    "# Check the java version\n",
    "! java -version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9780464a-5136-4a9f-ad35-62a3e6480978",
   "metadata": {},
   "source": [
    "# 2. Install Apache spark\n",
    "Apache Spark is supplied as an archive file as opposed to an installation program (like an .exe, .msi, .rpm, etc). The archive needs to be downloaded and extracted to a directory location with no spaces.\n",
    "\n",
    "In my case, the archive has been extracted to the following directory:\n",
    "```\n",
    "c:\\spark\\spark-3.1.1-bin-hadoop2.7\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f95baa-02fd-430c-ad70-f816886dcd46",
   "metadata": {},
   "source": [
    "# 3. Install Programming Language\n",
    "Accodring to the documentation Spark has the following compatabilities for it's landuage bindings:\n",
    "- Scala 2.12\n",
    "- Python 3.6+\n",
    "- R 3.5+\n",
    "\n",
    "Insure a compatable version is installed. In our case we are using python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3c4a11e-c72d-4b6d-9b4a-36a24eb3bdc7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.8 (tags/v3.6.8:3c6b436a57, Dec 24 2018, 00:16:47) [MSC v.1916 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print (sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69469fc-62a6-4f67-974a-1ec55eedf60f",
   "metadata": {},
   "source": [
    "# 4. Install Language Bindings\n",
    "There are a few python libraries we will be using today:\n",
    "- **findspark** - a utility which adds spark to the PATH variable. By doing so, it allows the pyspark library to find and use the spark libraries and binaries.\n",
    "- **pyspark** - the python spark library which gives us access to spark through python.\n",
    "- **py4j** - a library which enables Python programs running in a Python interpreter to dynamically access Java objects in a Java Virtual Machine. This library is consumed by pyspark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df21dc79-6ef0-4df6-8892-34bd05e8d151",
   "metadata": {},
   "source": [
    "We can check the installed version of these libraries with the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37e1b130-73c0-400b-97c6-b254e9a68284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "findspark           1.4.2\n"
     ]
    }
   ],
   "source": [
    "# Check if pyspark is intalled\n",
    "\n",
    "! pip list | findstr \"findspark\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff2b0be8-e3f1-4e67-9836-828bac9205f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyspark             3.1.1\n"
     ]
    }
   ],
   "source": [
    "# Check if pyspark is intalled\n",
    "\n",
    "! pip list | findstr \"pyspark\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67c2ca36-70ec-4ef8-9880-19935bdaf9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "py4j                0.10.9\n"
     ]
    }
   ],
   "source": [
    "# Check if py4j is intalled\n",
    "\n",
    "! pip list | findstr \"py4j\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3629fee1-9df3-4e9e-a4e4-56398d066c51",
   "metadata": {},
   "source": [
    "# 5. Install kubectl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de035fd1-07a2-4d6c-8ab1-6287085abc91",
   "metadata": {},
   "source": [
    "There are a number of ways to install kubectl. The easiest and fully featured way is to use the [chocolatey](https://kubernetes.io/docs/tasks/tools/install-kubectl-windows/#install-on-windows-using-chocolatey-or-scoop) installation process.\n",
    "\n",
    "Once the installation is complete we can run the following command to check the version of our kubectl command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fad4878c-3812-4d76-8f28-f0113dd28d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client Version: version.Info{Major:\"1\", Minor:\"21\", GitVersion:\"v1.21.0\", GitCommit:\"cb303e613a121a29364f75cc67d3d580833a7479\", GitTreeState:\"clean\", BuildDate:\"2021-04-08T16:31:21Z\", GoVersion:\"go1.16.1\", Compiler:\"gc\", Platform:\"windows/amd64\"}\n"
     ]
    }
   ],
   "source": [
    "! kubectl version --client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85815ebf-acb0-4426-ab48-57f00d9fd837",
   "metadata": {},
   "source": [
    "If we are properly configured, we need to configure kubectl so that it can connect to the kubernetes cluster.\n",
    "\n",
    "This is done by creating and editing the \"kubeconfig\" file. This is a file located in your user's \"home directory\". We first create the .kube directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95999434-ac81-4eeb-8b16-91b5f5f9094b",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cd %USERPROFILE% & mkdir .kube 2> NUL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923754c2-0460-4e9f-8c20-d84da489667d",
   "metadata": {},
   "source": [
    "We then create the kubeconfi file. For simple POC installations we can copy it from the master node of our kubernetes cluster. Setting up kubernetes is outside our scope.\n",
    "\n",
    "Once configured We can execute the following commands to get information about our cluster and the nodes in the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1230d3d1-9f23-41f5-a64b-a9b0cd94ff7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kubernetes control plane is running at https://15.4.7.11:6443\n",
      "CoreDNS is running at https://15.4.7.11:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\n",
      "\n",
      "To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
     ]
    }
   ],
   "source": [
    "! kubectl cluster-info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "719ad1e9-3ec7-4b7f-a2a8-610b2d23fd19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                           STATUS   ROLES                  AGE   VERSION\n",
      "os004k8-master001.foobar.com   Ready    control-plane,master   24d   v1.21.1\n",
      "os004k8-worker001.foobar.com   Ready    <none>                 24d   v1.21.1\n",
      "os004k8-worker002.foobar.com   Ready    <none>                 24d   v1.21.1\n",
      "os004k8-worker003.foobar.com   Ready    <none>                 24d   v1.21.1\n"
     ]
    }
   ],
   "source": [
    "! kubectl get node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de73a68e-c3f5-4f4e-aaf0-65fae20a2577",
   "metadata": {},
   "source": [
    "# 6. Configure Kubernetes To Host Apache Spark\n",
    "\n",
    "In order for our kubernetes cluster to successfully run a spark cluster we need to do a few things:\n",
    "1. Configure RBAC - We will need to set permissions so that our jupyter notebook and spark components have the appropriate permissions.\n",
    "2. Build containers - We will need to build the contaienrs which host our spark cluster nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0c0aa0-3d16-4870-9590-3e079596db54",
   "metadata": {},
   "source": [
    "## 6.1. Configure Kubernetes RBAC\n",
    "There are many ways to configure the RBAC of the kubernetes cluster. We are taking the simplest route posisble. We will define the follwing kubernetes objects:\n",
    "- Namspace named \"spark\" which will container our spark related infrastructure\n",
    "- ServiceAccount named spark-sa to serve as an identity to invoke commands as\n",
    "- ClusterRole named spark-role with required permissions\n",
    "- ClusterRoleBinding named spark-role-binding which attached the permissions defined in the role with the service account\n",
    "\n",
    "We will put our configurations into a kubernetes manifest file which was defined as follows:\n",
    "\n",
    "```\n",
    "---\n",
    "kind: Namespace\n",
    "apiVersion: v1\n",
    "metadata:\n",
    "  name: spark\n",
    "---\n",
    "kind: ServiceAccount\n",
    "apiVersion: v1\n",
    "metadata:\n",
    "  name: spark-sa\n",
    "---\n",
    "kind: ClusterRole\n",
    "apiVersion: rbac.authorization.k8s.io/v1\n",
    "metadata:\n",
    "  namespace: default\n",
    "  name: spark-role\n",
    "rules:\n",
    "  - apiGroups: [\"\"]\n",
    "    resources: [\"pods\", \"services\", \"configmaps\" ]\n",
    "    verbs: [\"create\", \"get\", \"watch\", \"list\", \"post\", \"delete\"  ]\n",
    "---\n",
    "kind: ClusterRoleBinding\n",
    "apiVersion: rbac.authorization.k8s.io/v1\n",
    "metadata:\n",
    "  name: spark-role-binding\n",
    "subjects:\n",
    "  - kind: ServiceAccount\n",
    "    name: spark-sa\n",
    "    namespace: spark\n",
    "roleRef:\n",
    "  kind: ClusterRole\n",
    "  name: spark-role\n",
    "  apiGroup: rbac.authorization.k8s.io\n",
    "\n",
    "```\n",
    "\n",
    "We can apply the configuration using our kubectl command\n",
    "\n",
    "```\n",
    "! kubectl apply -f spark_rbac.manifest\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c90a519-9bca-42bc-af30-5beefedf0801",
   "metadata": {},
   "source": [
    "## 6.2. Build Spark Containers For Kubernetes\n",
    "\n",
    "In order to run Apache Spark on kubernetes, Docker container images need to be prepared to serve as the master/slave notes of the Spark cluster. Inside this container image we will install our spark binaries etc. As we will see, when we create our SparkContex we specify this container when it is created.\n",
    "\n",
    "Spark (starting with version 2.3) ships with a Dockerfile that can be used for this purpose. These containers are debian based and the Dockerfile will install the latest version of python. This version of python neets to match the major version being run on the client which is hosting the jupyter noatbook. In other words, if we are running python 3.6 in our jupyter notebook, we need to ensure our docker container has the same version installed. \n",
    "\n",
    "In most cases one will need to modify or extend this image. Not only to modify the version of python being installed but also to modify the python packages which are installed on the spark nodes. Any packages we expect to execute on a cluster node will need to be installed in the docker image. This includes any software we might be using to store data, train machine learning algorithms, or perform optimizations.\n",
    "\n",
    "I have prepared a container which runs python 3.6 and is packaged with the necessary packages to execute these notebooks.\n",
    "\n",
    "For more information see the [Kubernetes 3.1.1 documentation](https://spark.apache.org/docs/3.1.1/running-on-kubernetes.html#docker-images)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
