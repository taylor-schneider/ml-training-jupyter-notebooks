{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "312f4a72-17a6-4937-b72d-a84fe4c0b2c2",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "In order to be able to submit python code to generate spark workloads we need to setup the following prerequisites:\n",
    "1. Install Java\n",
    "2. Install Apache Spark\n",
    "3. Install Programming Language\n",
    "4. Install Language Bindings\n",
    "\n",
    "If we will be leveraging the spark/kubernetes integration built into the SparkContext we will need to take a couple of additional steps. It is strongly reccomended to review the [Overview Notebook](Overview%20Running%20Apache%20Spark%20On%20Kubernetes.ipynb) before continuing.\n",
    "\n",
    "5. Install Kubectl\n",
    "6. Configure Kubernetes To Host Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3f4d5a-d95f-4b95-98b2-e1baeb8b6cc3",
   "metadata": {},
   "source": [
    "# 1. Install Java"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0deeb83-68be-415c-ae36-d5de4d65d7de",
   "metadata": {},
   "source": [
    "According to the documentation Apark 3.1.1 requires Java 8/11. In the case of the openjdk, we will see a version of 1.8.x coresponding to Oracle version 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84e9621e-4a7a-41c8-bf6a-fb0955fa1801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openjdk version \"11.0.14\" 2022-01-18 LTS\n",
      "OpenJDK Runtime Environment 18.9 (build 11.0.14+9-LTS)\n",
      "OpenJDK 64-Bit Server VM 18.9 (build 11.0.14+9-LTS, mixed mode, sharing)\n"
     ]
    }
   ],
   "source": [
    "# Check the java version\n",
    "! java -version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9780464a-5136-4a9f-ad35-62a3e6480978",
   "metadata": {},
   "source": [
    "# 2. Install Apache spark\n",
    "Apache Spark is supplied as an archive file as opposed to an installation program (like an .exe, .msi, .rpm, etc). The archive needs to be downloaded and extracted to a directory location with no spaces.\n",
    "\n",
    "In my case, the archive has been extracted to the following directory (on my windows host):\n",
    "```\n",
    "c:\\spark\\spark-3.1.1-bin-hadoop2.7\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f95baa-02fd-430c-ad70-f816886dcd46",
   "metadata": {},
   "source": [
    "# 3. Install Programming Language\n",
    "Accodring to the documentation Spark has the following compatabilities for it's landuage bindings:\n",
    "- Scala 2.12\n",
    "- Python 3.6+\n",
    "- R 3.5+\n",
    "\n",
    "Insure a compatable version is installed. In our case we are using python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3c4a11e-c72d-4b6d-9b4a-36a24eb3bdc7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.1 (default, Feb 13 2022, 16:10:43) \n",
      "[GCC 4.8.5 20150623 (Red Hat 4.8.5-44)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print (sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69469fc-62a6-4f67-974a-1ec55eedf60f",
   "metadata": {},
   "source": [
    "# 4. Install Language Bindings\n",
    "There are a few python libraries we will be using today:\n",
    "- **findspark** - a utility which adds spark to the PATH variable. By doing so, it allows the pyspark library to find and use the spark libraries and binaries.\n",
    "- **pyspark** - the python spark library which gives us access to spark through python.\n",
    "- **py4j** - a library which enables Python programs running in a Python interpreter to dynamically access Java objects in a Java Virtual Machine. This library is consumed by pyspark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df21dc79-6ef0-4df6-8892-34bd05e8d151",
   "metadata": {},
   "source": [
    "We can check the installed version of these libraries using the command line utilities. For linux we have `grep` and for windows we have `findstr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37e1b130-73c0-400b-97c6-b254e9a68284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "findspark            1.4.2\n",
      "\u001b[33mWARNING: You are using pip version 22.0.3; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Check if pyspark is intalled\n",
    "\n",
    "! pip list | grep \"findspark\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff2b0be8-e3f1-4e67-9836-828bac9205f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyspark              3.1.1\n",
      "\u001b[33mWARNING: You are using pip version 22.0.3; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Check if pyspark is intalled\n",
    "\n",
    "! pip list | grep \"pyspark\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67c2ca36-70ec-4ef8-9880-19935bdaf9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "py4j                 0.10.9\n",
      "\u001b[33mWARNING: You are using pip version 22.0.3; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Check if py4j is intalled\n",
    "\n",
    "! pip list | grep \"py4j\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3629fee1-9df3-4e9e-a4e4-56398d066c51",
   "metadata": {},
   "source": [
    "# 5. Install And Configure Kubectl\n",
    "Kubectl is a command line utility used for interacting with a kubernetes cluster. This utility is used by the spark libraries to submit work to the kubernetes cluster. We need to install it and then configure it so it can speak to our spark cluster.\n",
    "\n",
    "**Note**: The spark software will dictate which user is used to submit work to kubernets. This user may be different than the one the kubectl cli is configured to use. In my case I built a separate RBAC definition for a new spark user rather than using the cluster admin used by the kubectl cli. More on this later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4324a30-954a-4f07-bc0c-69948b9c355e",
   "metadata": {},
   "source": [
    "## 5.1. Install Software"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de035fd1-07a2-4d6c-8ab1-6287085abc91",
   "metadata": {},
   "source": [
    "There are a number of ways to install kubectl. For windows useres, the easiest and fully featured way is to use the [chocolatey](https://kubernetes.io/docs/tasks/tools/install-kubectl-windows/#install-on-windows-using-chocolatey-or-scoop) package installation manager. For linux users, you must use the distro specific package manager (YUM, apt, etc.)\n",
    "\n",
    "Once the installation is complete we can run the following command to check the version of our kubectl command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97218d16-3a00-4fce-8f37-d8bdb225c558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client Version: version.Info{Major:\"1\", Minor:\"21\", GitVersion:\"v1.21.9\", GitCommit:\"b631974d68ac5045e076c86a5c66fba6f128dc72\", GitTreeState:\"clean\", BuildDate:\"2022-01-19T17:51:12Z\", GoVersion:\"go1.16.12\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
     ]
    }
   ],
   "source": [
    "! kubectl version --client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cbf19c-08a1-4508-a96a-13364c40b4d2",
   "metadata": {},
   "source": [
    "## 5.2. Configure Client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85815ebf-acb0-4426-ab48-57f00d9fd837",
   "metadata": {},
   "source": [
    "Once installed, we need to configure kubectl so that it can connect to the kubernetes cluster.\n",
    "\n",
    "This is done by creating and editing the \"kubeconfig\" file. This is a file located in your user's \"home directory\".\n",
    "\n",
    "We first create the .kube directory in our user directory. For example, on a windows pc we can run the following command in the terminal to create our '.kube' directory in our user's home directory:\n",
    "\n",
    "`cd %USERPROFILE% & mkdir .kube 2> NUL`\n",
    "\n",
    "On a linux pc we can run the following:\n",
    "\n",
    "`cd ~/ && mkdir .kube`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85edd4a9-d9e6-4001-b40c-aa302aed1215",
   "metadata": {},
   "source": [
    "We then create the kubeconfig file. For simple POC installations we can copy it from the master node of our kubernetes cluster. Setting up kubernetes is outside our scope.\n",
    "\n",
    "In my case, the file resembled the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1a98050-6c5f-499f-b36e-352517b27b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apiVersion: v1\n",
      "clusters:\n",
      "- cluster:\n",
      "    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUM1ekNDQWMrZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRJeU1ESXhOREl4TXpBeE9Wb1hEVE15TURJeE1qSXhNekF4T1Zvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTGNtCjVhRnBQV1EvWnA2UEVacDFRSmMxbVdHMGNYTGJ6UjZibCtubkNwRGszd3gyOXhKbitqQ0VWS2wzVTN4UlJFV28Kang4Z25LWmZNME1sLzBTRnUwOHdXUXFUV0V4cjZpYi9pZ0NCd1FWTUpPYkhzMjZhSVZrSWRpMlB2eVk1VTVDWgpQcWNjbnNUZnZweXRBR0c3cHo1WFlMdGsvVFZHN09qWUdrdWlGck5TbHN2M2w4YksyMlpDaVBrakg0akx6OUg3ClFVeldvM3czNlFBWmp6M3VkaHlYUjhvWWVGTS9OUnFYVzdxYUlmQ3FvQjJNVlR6eTVQTVdDQzZXaWtPZnVTY2UKRjB5N3JnaUM1Zk9kVmZTcFJ1NUc1NXhmQW5WWWF5TWt0RU5HYW04d3JESEhockNsWHIxcjhpNXZNSWRDZktLcwpMWnpmSnB1Y29kYUhLUGVVTlpFQ0F3RUFBYU5DTUVBd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0hRWURWUjBPQkJZRUZFdWgyVVhvOENuQXNsbjc4N014VHA2UHFYejNNQTBHQ1NxR1NJYjMKRFFFQkN3VUFBNElCQVFBTG93SktLQU5kVENRcUNGa3VsYTF2RnN4QnBCZTJudXJndWgwRlpMN09US1hFVG5sSQpJOUcrV3Vyc0E1ZjE0K1RzdktMTkoxVWs4Sm1tNVUrempOTXAydHVFWXptS2N2VDJkWFlkRVFvbEZtaUdselUrClBkaVBmNS9TaVkzWXZIVFZ2WTVsNzRzS1ZXYnRYRFdCd1pmVGwwWFNwK20vSHBDcjFRSms4bC9OY01yZU5LRGcKUU85WWx3R0RsWmMwbW8zcjh0WHBNdGpFRU55ajk2bU52cEU0enYzZVJ1Z29wZVRZbE5LSVNDdjVFWlA2WTEweQplTEQwNFduUmJCQXNjRE9OZFFyWkwwZTRMY0xGZXBxc2R5Wk8wVTgxZ0kyeFlqdFhBMWpPVE83RjBVS3h0d0dPCnlRUDA1UUc3MVduQ215bUVnTytaZXVrVGsxQzN1NC9ackxNKwotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==\n",
      "    server: https://15.4.7.11:6443\n",
      "  name: kubernetes\n",
      "contexts:\n",
      "- context:\n",
      "    cluster: kubernetes\n",
      "    user: kubernetes-admin\n",
      "  name: kubernetes-admin@kubernetes\n",
      "current-context: kubernetes-admin@kubernetes\n",
      "kind: Config\n",
      "preferences: {}\n",
      "users:\n",
      "- name: kubernetes-admin\n",
      "  user:\n",
      "    client-certificate-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURJVENDQWdtZ0F3SUJBZ0lJTmtOYWVrVE9jYjh3RFFZSktvWklodmNOQVFFTEJRQXdGVEVUTUJFR0ExVUUKQXhNS2EzVmlaWEp1WlhSbGN6QWVGdzB5TWpBeU1UUXlNVE13TVRsYUZ3MHlNekF5TVRReU1UTXdNak5hTURReApGekFWQmdOVkJBb1REbk41YzNSbGJUcHRZWE4wWlhKek1Sa3dGd1lEVlFRREV4QnJkV0psY201bGRHVnpMV0ZrCmJXbHVNSUlCSWpBTkJna3Foa2lHOXcwQkFRRUZBQU9DQVE4QU1JSUJDZ0tDQVFFQXdZL1VmZ3BCVElUaml0V3kKalNsd04xNmFwYlEzMEFvQ1lraVliOUtXOUxPajBnamFoVW55YjVNUUdHMVlEOG9QbUpnbDJpTEs1aDQ3MUljSApZenFjZG54bVMrR3o1RkpLRnhwRUt1VEV2aXBlSHBaa0k5cFpmZ2FiTmxYNkU2OE9kWlFNYnI3MUpyZmt0SlR6CjlmT2FEK2F1eWNLNU5GdWNqOHp2TFg1Y0ltdEtWOGp0YVdEZnhlU00vZ3BIU002UnRLamtWalYwL3hSUUs2WW4KTkdrNTh1Q1laakJFanJzUHdONERhaVNoZVc1TkNNK0hjYkVEcUx3V2VncmpIZkR4YlYybjYyVXFQZldUeFN6KwozSGVGcytJNm1iZEVUOUVLUzN5c1QwYVFQMXJUcmpEeFNla29XUlVnTHN5cXFBbjZPU01TdEJFUFBzam1HMEJzCko0S3A1UUlEQVFBQm8xWXdWREFPQmdOVkhROEJBZjhFQkFNQ0JhQXdFd1lEVlIwbEJBd3dDZ1lJS3dZQkJRVUgKQXdJd0RBWURWUjBUQVFIL0JBSXdBREFmQmdOVkhTTUVHREFXZ0JSTG9kbEY2UEFwd0xKWisvT3pNVTZlajZsOAo5ekFOQmdrcWhraUc5dzBCQVFzRkFBT0NBUUVBb3BLa1p3UjFFdEw0M1dUVHRqTU1LWkExWWhMbGJqTUZRTllVCnJlcDNOV0pjQndUQ1ZQelZNUHdzMW5kcW5RTDNoOGxYbTYrK25pMUxIN0ZCVVBQYm1oS1owZU9WWEFxMGtGVm0KSGVOb0pjemZFeGV1aVgvRTdoZkZyR3diQjlZZ1hHRzNuaUtrTVJ2L1JjTUkyZUIwaUthZXlDNXMwdkJHaU53Sgp6c2E2dlg5Q3pTb2RIQ0tIVk1BdElEZjUyVmZTOHpSQ1Uxck10c0FmVkxMc2ZEL1ZxMjdhck0zYUIwL2F4YUZECi9GY09TL3gvZS9MbFlOdG02bUdmSWNlM2Fvd0FyU3ZUREpOSjQydjB4empSUlpWalY4dC9yaGR4Vm9PUVRYd2cKbW5EYTF6SzR0b3VJNTc3eHBwNDhycDRRbm45NEZ5OUZuV0xpRmxZeFRkN3dKU01HNlE9PQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==\n",
      "    client-key-data: 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\n"
     ]
    }
   ],
   "source": [
    "! cat ~/.kube/config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923754c2-0460-4e9f-8c20-d84da489667d",
   "metadata": {},
   "source": [
    "Once the kubeconfig file is created we can check that the client version matches the server version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62b01858-a85b-4e2f-9e45-1d21fcbd0812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client Version: version.Info{Major:\"1\", Minor:\"21\", GitVersion:\"v1.21.9\", GitCommit:\"b631974d68ac5045e076c86a5c66fba6f128dc72\", GitTreeState:\"clean\", BuildDate:\"2022-01-19T17:51:12Z\", GoVersion:\"go1.16.12\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n",
      "Server Version: version.Info{Major:\"1\", Minor:\"21\", GitVersion:\"v1.21.9\", GitCommit:\"b631974d68ac5045e076c86a5c66fba6f128dc72\", GitTreeState:\"clean\", BuildDate:\"2022-01-19T17:45:53Z\", GoVersion:\"go1.16.12\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
     ]
    }
   ],
   "source": [
    "! kubectl version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f80cfc-4a88-4f0b-92d1-4d3bf693bef8",
   "metadata": {},
   "source": [
    "We can also execute the following commands to get information about our cluster and the nodes in the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1230d3d1-9f23-41f5-a64b-a9b0cd94ff7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;32mKubernetes control plane\u001b[0m is running at \u001b[0;33mhttps://15.4.7.11:6443\u001b[0m\n",
      "\u001b[0;32mCoreDNS\u001b[0m is running at \u001b[0;33mhttps://15.4.7.11:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\u001b[0m\n",
      "\n",
      "To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
     ]
    }
   ],
   "source": [
    "! kubectl cluster-info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "719ad1e9-3ec7-4b7f-a2a8-610b2d23fd19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                           STATUS   ROLES                  AGE   VERSION\n",
      "os004k8-master001.foobar.com   Ready    control-plane,master   20d   v1.21.9\n",
      "os004k8-worker001.foobar.com   Ready    <none>                 20d   v1.21.9\n",
      "os004k8-worker002.foobar.com   Ready    <none>                 20d   v1.21.9\n",
      "os004k8-worker003.foobar.com   Ready    <none>                 20d   v1.21.9\n"
     ]
    }
   ],
   "source": [
    "! kubectl get node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de73a68e-c3f5-4f4e-aaf0-65fae20a2577",
   "metadata": {},
   "source": [
    "# 6. Configure Kubernetes To Host Apache Spark\n",
    "\n",
    "In order for our kubernetes cluster to successfully run a spark cluster we need to configure the kubernetes Role Based Access Control permissions so that our jupyter notebook and spark components have the appropriate permissions in order to utilize resources from the kubernetes cluster (create driver pods and executor pods)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0c0aa0-3d16-4870-9590-3e079596db54",
   "metadata": {},
   "source": [
    "There are many ways to configure the RBAC of the kubernetes cluster. We are taking the simplest and most repeatable route posisble. \n",
    "\n",
    "We will define the follwing kubernetes objects:\n",
    "- Namspace named \"spark\" which will contain our spark related infrastructure\n",
    "- ServiceAccount named spark-sa to serve as an identity to invoke commands as\n",
    "- ClusterRole named spark-role with required permissions\n",
    "- ClusterRoleBinding named spark-role-binding which attached the permissions defined in the role with the service account\n",
    "\n",
    "We will put our configurations into a kubernetes manifest file which was defined as follows:\n",
    "\n",
    "```\n",
    "---\n",
    "kind: Namespace\n",
    "apiVersion: v1\n",
    "metadata:\n",
    "  name: spark\n",
    "---\n",
    "kind: ServiceAccount\n",
    "apiVersion: v1\n",
    "metadata:\n",
    "  name: spark-sa\n",
    "  namespace: spark\n",
    "---\n",
    "kind: ClusterRole\n",
    "apiVersion: rbac.authorization.k8s.io/v1\n",
    "metadata:\n",
    "  namespace: default\n",
    "  name: spark-role\n",
    "rules:\n",
    "  - apiGroups: [\"\"]\n",
    "    resources: [\"pods\", \"services\", \"configmaps\" ]\n",
    "    verbs: [\"create\", \"get\", \"watch\", \"list\", \"post\", \"delete\"  ]\n",
    "---\n",
    "kind: ClusterRoleBinding\n",
    "apiVersion: rbac.authorization.k8s.io/v1\n",
    "metadata:\n",
    "  name: spark-role-binding\n",
    "subjects:\n",
    "  - kind: ServiceAccount\n",
    "    name: spark-sa\n",
    "    namespace: spark\n",
    "roleRef:\n",
    "  kind: ClusterRole\n",
    "  name: spark-role\n",
    "  apiGroup: rbac.authorization.k8s.io\n",
    "```\n",
    "\n",
    "We can apply the configuration using our kubectl command\n",
    "\n",
    "```\n",
    "[root@os004k8-master001 ~]# kubectl apply -f spark.manifest\n",
    "namespace/spark created\n",
    "serviceaccount/spark-sa created\n",
    "clusterrole.rbac.authorization.k8s.io/spark-role created\n",
    "clusterrolebinding.rbac.authorization.k8s.io/spark-role-binding created\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fb7937-d727-407f-b9de-5145efdc91f5",
   "metadata": {},
   "source": [
    "We can then comfirm that the objects were creates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a09865b9-d63e-4f78-a6cb-a8f5b6249ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                   STATUS   AGE\n",
      "spark                  Active   19d\n"
     ]
    }
   ],
   "source": [
    "! kubectl get namespace | grep -E \"NAME|spark\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4eef2736-d99a-4a4a-8bf5-a922eaf23514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME       SECRETS   AGE\n",
      "spark-sa   1         19d\n"
     ]
    }
   ],
   "source": [
    "! kubectl -n spark get serviceAccounts | grep -E \"NAME|spark\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "efc94703-1ea8-460e-b933-ac97e251ea46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                                                   CREATED AT\n",
      "spark-role                                                             2022-02-16T02:13:48Z\n"
     ]
    }
   ],
   "source": [
    "! kubectl -n spark get clusterRoles | grep -E \"NAME|spark\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b511baff-29b0-4d7b-9f58-979674606a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                                   ROLE                                                                               AGE\n",
      "spark-role-binding                                     ClusterRole/spark-role                                                             19d\n"
     ]
    }
   ],
   "source": [
    "! kubectl -n spark get clusterRoleBindings | grep -E \"NAME|spark\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08ef08a-9091-4a0a-8c71-aacb4ba5ca2c",
   "metadata": {},
   "source": [
    "In a later notebook we will learn how to [create a SparkContext which utilizes kubernetes](Create%20A%20SparkContext%20For%20Kubernetes%20Hosted%20Cluster.ipynb) on the back-end. There, we will make a configuration to inform the SparkContext to use the spark-sa service account when communicating with the spark cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e023b1a5-4f87-44e2-bfb8-3ef115ff89d9",
   "metadata": {},
   "source": [
    "# 7. Build Container Images For Kubernetes\n",
    "\n",
    "When we talk about running Apache Spark on kubernetes... what are we talking about? Kubernetes is a container orchestration tool. So what we are talking about is using Kubernetes to orchestrate the deployment our Apache Spark clusters. In other words, when we need a new cluster, we will have kubernetes build it.\n",
    "\n",
    "Again, kubernetes is a **container** orchestrator. In order to run Apache Spark on kubernetes it needs to be in a container. To make this work we will build a dockerfile which contains the software for the driver/executor (master/slave) nodes of the Apache Spark cluster as well as a few customizations to allow them to speak to kubernets. But dont worry, this isn't as hard as you may think. Spark (starting with version 2.3) ships with a Dockerfile that can be used for this purpose. These containers are debian based and the Dockerfile will install the latest version of python available at the time. IMHU this is better for testing rather than a production workflow (I want to be in control of what version etc). \n",
    "\n",
    "For more information see the [Kubernetes 3.1.1 documentation](https://spark.apache.org/docs/3.1.1/running-on-kubernetes.html#docker-images).\n",
    "\n",
    "**Note**: The version of python being used by your jupyter notebook must match the version of python running in the spark cluster. If we are running python 3.6 in our jupyter notebook, we need to ensure our docker container has the same version installed. If you do not do this, and spark doesnt yell at you, you could face some very nasy errors. This requirement is due to the fact that python objects are being serialized and sent to executor nodes and thus versions must match for the serialization and deserialization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b16fd1a-21e5-478d-8168-f649dea1c789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.1\n"
     ]
    }
   ],
   "source": [
    "! python3 --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c90a519-9bca-42bc-af30-5beefedf0801",
   "metadata": {},
   "source": [
    "**Note**: All the libraries being used by the jupyter notebook will need to be installed in the spark cluster image.\n",
    "\n",
    "Because of these two requirements, it is highly likely that we will need to modify or extend this base image provided by Spark. Not only to modify the version of python being installed but also to modify the python packages which are installed on the spark nodes. Any packages we expect to execute on a cluster node will need to be installed in the docker image. This includes any software we might be using to store data, train machine learning algorithms, or perform optimizations.\n",
    "\n",
    "I have prepared a container which is based on CentOS, runs python 3.9.1, and has all the necessary packages to execute these notebooks installed.\n",
    "\n",
    "I am working to make the dockerfile etc available. For now the build process is a bit of a black box. Images can be found on [DockerHub](https://hub.docker.com/r/tschneider/apache-spark-k8)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
