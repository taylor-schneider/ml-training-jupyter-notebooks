{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "312f4a72-17a6-4937-b72d-a84fe4c0b2c2",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "In order to be able to submit python code to generate spark workloads we need to setup the following prerequisites:\n",
    "1. Install Java\n",
    "2. Install Apache Spark\n",
    "3. Install Programming Language\n",
    "4. Install Language Bindings\n",
    "\n",
    "If we will be leveraging the spark/kubernetes integration built into the SparkContext we will need to setup:\n",
    "\n",
    "5. Install Kubectl\n",
    "6. Configure Kubernetes To Host Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3f4d5a-d95f-4b95-98b2-e1baeb8b6cc3",
   "metadata": {},
   "source": [
    "# 1. Install Java"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0deeb83-68be-415c-ae36-d5de4d65d7de",
   "metadata": {},
   "source": [
    "According to the documentation Apark 3.1.1 requires Java 8/11. In the case of the openjdk, we will see a version of 1.8.x coresponding to Oracle version 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84e9621e-4a7a-41c8-bf6a-fb0955fa1801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openjdk version \"11.0.13\" 2021-10-19 LTS\n",
      "OpenJDK Runtime Environment 18.9 (build 11.0.13+8-LTS)\n",
      "OpenJDK 64-Bit Server VM 18.9 (build 11.0.13+8-LTS, mixed mode, sharing)\n"
     ]
    }
   ],
   "source": [
    "# Check the java version\n",
    "! java -version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9780464a-5136-4a9f-ad35-62a3e6480978",
   "metadata": {},
   "source": [
    "# 2. Install Apache spark\n",
    "Apache Spark is supplied as an archive file as opposed to an installation program (like an .exe, .msi, .rpm, etc). The archive needs to be downloaded and extracted to a directory location with no spaces.\n",
    "\n",
    "In my case, the archive has been extracted to the following directory (on my windows host):\n",
    "```\n",
    "c:\\spark\\spark-3.1.1-bin-hadoop2.7\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f95baa-02fd-430c-ad70-f816886dcd46",
   "metadata": {},
   "source": [
    "# 3. Install Programming Language\n",
    "Accodring to the documentation Spark has the following compatabilities for it's landuage bindings:\n",
    "- Scala 2.12\n",
    "- Python 3.6+\n",
    "- R 3.5+\n",
    "\n",
    "Insure a compatable version is installed. In our case we are using python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3c4a11e-c72d-4b6d-9b4a-36a24eb3bdc7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.1 (default, Jan 15 2022, 03:37:13) \n",
      "[GCC 4.8.5 20150623 (Red Hat 4.8.5-44)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print (sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69469fc-62a6-4f67-974a-1ec55eedf60f",
   "metadata": {},
   "source": [
    "# 4. Install Language Bindings\n",
    "There are a few python libraries we will be using today:\n",
    "- **findspark** - a utility which adds spark to the PATH variable. By doing so, it allows the pyspark library to find and use the spark libraries and binaries.\n",
    "- **pyspark** - the python spark library which gives us access to spark through python.\n",
    "- **py4j** - a library which enables Python programs running in a Python interpreter to dynamically access Java objects in a Java Virtual Machine. This library is consumed by pyspark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df21dc79-6ef0-4df6-8892-34bd05e8d151",
   "metadata": {},
   "source": [
    "We can check the installed version of these libraries with the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37e1b130-73c0-400b-97c6-b254e9a68284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "findspark            1.4.2\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.0.3 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Check if pyspark is intalled\n",
    "\n",
    "! pip list | grep \"findspark\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff2b0be8-e3f1-4e67-9836-828bac9205f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyspark              3.1.1\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.0.3 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Check if pyspark is intalled\n",
    "\n",
    "! pip list | grep \"pyspark\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67c2ca36-70ec-4ef8-9880-19935bdaf9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "py4j                 0.10.9\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.0.3 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Check if py4j is intalled\n",
    "\n",
    "! pip list | grep \"py4j\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3629fee1-9df3-4e9e-a4e4-56398d066c51",
   "metadata": {},
   "source": [
    "# 5. Install And Configure Kubectl\n",
    "Kubectl is a command line utility used for interacting with a kubernetes cluster. This utility is used by the spark libraries to submit work to the kubernetes cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4324a30-954a-4f07-bc0c-69948b9c355e",
   "metadata": {},
   "source": [
    "## 5.1. Install Software"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de035fd1-07a2-4d6c-8ab1-6287085abc91",
   "metadata": {},
   "source": [
    "There are a number of ways to install kubectl. For windows useres, the easiest and fully featured way is to use the [chocolatey](https://kubernetes.io/docs/tasks/tools/install-kubectl-windows/#install-on-windows-using-chocolatey-or-scoop) package installation manager. For linux users, you must use the distro specific package manager (YUM, apt, etc.)\n",
    "\n",
    "Once the installation is complete we can run the following command to check the version of our kubectl command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97218d16-3a00-4fce-8f37-d8bdb225c558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client Version: version.Info{Major:\"1\", Minor:\"5\", GitVersion:\"v1.5.2\", GitCommit:\"269f928217957e7126dc87e6adfa82242bfe5b1e\", GitTreeState:\"clean\", BuildDate:\"2017-07-03T15:31:10Z\", GoVersion:\"go1.7.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n",
      "Server Version: version.Info{Major:\"1\", Minor:\"21\", GitVersion:\"v1.21.9\", GitCommit:\"b631974d68ac5045e076c86a5c66fba6f128dc72\", GitTreeState:\"clean\", BuildDate:\"2022-01-19T17:45:53Z\", GoVersion:\"go1.16.12\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
     ]
    }
   ],
   "source": [
    "! kubectl version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fad4878c-3812-4d76-8f28-f0113dd28d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client Version: version.Info{Major:\"1\", Minor:\"5\", GitVersion:\"v1.5.2\", GitCommit:\"269f928217957e7126dc87e6adfa82242bfe5b1e\", GitTreeState:\"clean\", BuildDate:\"2017-07-03T15:31:10Z\", GoVersion:\"go1.7.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
     ]
    }
   ],
   "source": [
    "! kubectl version --client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cbf19c-08a1-4508-a96a-13364c40b4d2",
   "metadata": {},
   "source": [
    "## 5.2. Configure Client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85815ebf-acb0-4426-ab48-57f00d9fd837",
   "metadata": {},
   "source": [
    "Once installed, we need to configure kubectl so that it can connect to the kubernetes cluster.\n",
    "\n",
    "This is done by creating and editing the \"kubeconfig\" file. This is a file located in your user's \"home directory\".\n",
    "\n",
    "We first create the .kube directory in our user directory. For example, on a windows pc we can run the following command in the terminal to create our '.kube' directory in our user's home directory:\n",
    "\n",
    "`cd %USERPROFILE% & mkdir .kube 2> NUL`\n",
    "\n",
    "On a linux pc we can run the following:\n",
    "\n",
    "`cd ~/ && mkdir .kube`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85edd4a9-d9e6-4001-b40c-aa302aed1215",
   "metadata": {},
   "source": [
    "We then create the kubeconfig file. For simple POC installations we can copy it from the master node of our kubernetes cluster. Setting up kubernetes is outside our scope.\n",
    "\n",
    "In my case, the file resembled the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9413155d-ad85-479e-9eab-9f042bb315aa",
   "metadata": {},
   "source": [
    "```\n",
    "[root@os004k8-master001 ~]# cat ~/.kube/config\n",
    "apiVersion: v1\n",
    "clusters:\n",
    "- cluster:\n",
    "    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUM1ekNDQWMrZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRJeU1ESXhNVEExTVRVME5Wb1hEVE15TURJd09UQTFNVFUwTlZvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTXFFClpiN2dNVm44dk96cGJGNkpiWUw4Y2NhY0QxajFkRk5pV1VVOVNrbzRYQW8wVG83d0tBQ0VsUWF3ZFJMaXVObnQKUEJHSlQ0d3NTd2tZYnk3eTdac3hkTVZXNnlXa1B0Umh6TVhGVTJ5QWhnaDRVYlVKNXNMUUdNV292UzZXZ2p3MgphNC9WUnRxWFFLQmh4Wnc2dHRNTkI1WldTb3NaUXZkMXQwV1pzSXM3bysxclJlN1BFcXBDTFhwNXV6OGlIMDlDClNBK0lacitFa1Z2b2J5Qzk3bFJyN0toMFV4aGh3VUQ5WjdmLzdlSURORnhwVXdtUURQK2RtTVdpUWRUbVh0ZW4KSlc2WTZsZXpUeHJUQXRLektqakExb0FiY1ZsMldXM2hMWnVmK3BQOHRkS3AvOFY3cHM3MWxoSEJLaEl2S2IvUApkY3c2RGUwd0pOZVQ2SnptbXBrQ0F3RUFBYU5DTUVBd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0hRWURWUjBPQkJZRUZDNkloeEMwMGg4emtJNmpQalg4ekhNUXZNNU9NQTBHQ1NxR1NJYjMKRFFFQkN3VUFBNElCQVFDY1M2N0Z1WUNGeWlhM1drQlRFMjU5QUd1TThYc2hpOGFBMTJlaHd6WldtSHVPRWwzdgpNQnlpZ29ZR0JMdlBPTWQxM0tneSt0UWRPYjhtM21qZi9YOTBXV0NMWGtKTHdOZWdUTTRibjNWdmxJVlBmZ2JXCjM4cXlXeVA2ZFI4aUVUK1ZjZWdGdFlzYkROTllad2RwemViZUhmcFc0VFFDM1pDZ2ROM0M5SDFQeC9Ob0YrWEEKV2FyUjNsNWp2WmwwRElTZjE4M0V5MEU5MWZxS1hwR1FlWFVtcFVVbml5RWpxYy9jckREM01LbGdNTnltWTVYYQpTZjhSL2FCREZPMnJENUlwc0RQZWU3bklQSm5kUitGb3RWeDVnUzBNTmFUbmNSQVY4UnNIM0RrMjAxQzdBSWduClhSb0ZBWFlTMGFIUnFjZzJwK0FkdFZ2bnlOeWtubW5aS1NGVgotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==\n",
    "    server: https://15.4.7.11:6443\n",
    "  name: kubernetes\n",
    "contexts:\n",
    "- context:\n",
    "    cluster: kubernetes\n",
    "    user: kubernetes-admin\n",
    "  name: kubernetes-admin@kubernetes\n",
    "current-context: kubernetes-admin@kubernetes\n",
    "kind: Config\n",
    "preferences: {}\n",
    "users:\n",
    "- name: kubernetes-admin\n",
    "  user:\n",
    "    client-certificate-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURJVENDQWdtZ0F3SUJBZ0lJY1BqMUlCODFBeDR3RFFZSktvWklodmNOQVFFTEJRQXdGVEVUTUJFR0ExVUUKQXhNS2EzVmlaWEp1WlhSbGN6QWVGdzB5TWpBeU1URXdOVEUxTkRWYUZ3MHlNekF5TVRFd05URTFORGhhTURReApGekFWQmdOVkJBb1REbk41YzNSbGJUcHRZWE4wWlhKek1Sa3dGd1lEVlFRREV4QnJkV0psY201bGRHVnpMV0ZrCmJXbHVNSUlCSWpBTkJna3Foa2lHOXcwQkFRRUZBQU9DQVE4QU1JSUJDZ0tDQVFFQXllSjNHQmgxdFU4N204MlAKekVEdVBPUlA2ZUdZTG5rRExrRkpEKzhBcUxBSDU3OUVDMGlCTEE5OUxCSGdPZDZ2UGZJNzFxSmQwdHRlQXpueQpXTXd2b2F1RFJRSGNrZTl1eCtiSnNwcHNXbDA4b284N25MdTdOQ2QxM2xQZVhDYndWR3VEVXJWcktNZ1JMdXBYCnhRc2RTaUNuUysvNGpORmVVSjFVZ2cwd2Z5c3M3Rjg3WnRIREFRQ3dQb2JUdFJqdi92dEduS09iL3dhMUdLTk0Kb1oycUhIRGVteGI5Vm0zR2VlTk9rLzR5TFF3YmNvd0dNVGUzRjdqWkJaRzYrNWh1T0lSOTRMOCtZbGEzZC8yWQpnSlNGRWxHNGJqTDlLQlFVT0VtZ1d2WDBhbWYyekxPbDBMMlloaHdtUFFRV251ME5BOHRXK1hveXVWOGk3dXNCCjhvQ3kvUUlEQVFBQm8xWXdWREFPQmdOVkhROEJBZjhFQkFNQ0JhQXdFd1lEVlIwbEJBd3dDZ1lJS3dZQkJRVUgKQXdJd0RBWURWUjBUQVFIL0JBSXdBREFmQmdOVkhTTUVHREFXZ0JRdWlJY1F0TklmTTVDT296NDEvTXh6RUx6TwpUakFOQmdrcWhraUc5dzBCQVFzRkFBT0NBUUVBWkZCdWxoL2QvZnNKUnhHcHBCdy9JOW1qUlJINDdCSEprVjJJCm9uOWZsSi92NnU2dWxZeFlSaXpMMTd5ZE0zaEN4Sjl3WlhWM3lOWXpRMnhZTEJlS05MdEpmNnFiL2hFWGVwbHoKd0xyMEJtYlNKVGpwRWVBc1RwRTdpa0xPd1NhTlduYituSllCVDg5MUJNVXB6cEYvTzBiaWRrWU1KSHRybVYyTQppYWtnbGlETnJIWDVBUXZHL2VJOVhtQnhVeVhIejhrcWtkOTUrQ2FIRXZwK2FEQkQ0TlpJUTNMQ2xrejMrL2Z1CnlXS3kvZ3NPTHI3KzVEVWMyWXFBNU51eUpHTlhseEFlUm1wQjFHYXcxZ1UvMUJ3eklpRHRaRVpEU0FpelhwS00KZnZsVnR4RHk4WGkzaFIveFU5OUg0L28wTGcyMlU2TEw3elN1RFlDMVk4ek1vVGhSU0E9PQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==\n",
    "    client-key-data: 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\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923754c2-0460-4e9f-8c20-d84da489667d",
   "metadata": {},
   "source": [
    "Once the kubeconfig file is created, our client is configured. To see it is properly configured, we can execute the following commands to get information about our cluster and the nodes in the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1230d3d1-9f23-41f5-a64b-a9b0cd94ff7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;32mKubernetes master\u001b[0m is running at \u001b[0;33mhttps://15.4.7.11:6443\u001b[0m\n",
      "\u001b[0;32mCoreDNS\u001b[0m is running at \u001b[0;33mhttps://15.4.7.11:6443/api/v1/proxy/namespaces/kube-system/services/kube-dns\u001b[0m\n",
      "\n",
      "To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
     ]
    }
   ],
   "source": [
    "! kubectl cluster-info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "719ad1e9-3ec7-4b7f-a2a8-610b2d23fd19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                           STATUS    AGE\n",
      "os004k8-master001.foobar.com   Ready     1d\n",
      "os004k8-worker001.foobar.com   Ready     1d\n",
      "os004k8-worker002.foobar.com   Ready     1d\n",
      "os004k8-worker003.foobar.com   Ready     1d\n"
     ]
    }
   ],
   "source": [
    "! kubectl get node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75d2559d-a404-4709-bc81-96e998ac8006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAMESPACE              NAME                                                   READY     STATUS    RESTARTS   AGE\n",
      "kube-system            coredns-558bd4d5db-29j5m                               1/1       Running   1          1d\n",
      "kube-system            coredns-558bd4d5db-957j8                               1/1       Running   1          1d\n",
      "kube-system            etcd-os004k8-master001.foobar.com                      1/1       Running   40         1d\n",
      "kube-system            kube-apiserver-os004k8-master001.foobar.com            1/1       Running   64         1d\n",
      "kube-system            kube-controller-manager-os004k8-master001.foobar.com   1/1       Running   179        1d\n",
      "kube-system            kube-proxy-b8fgw                                       1/1       Running   1          1d\n",
      "kube-system            kube-proxy-c7bbb                                       1/1       Running   0          1d\n",
      "kube-system            kube-proxy-hr6wk                                       1/1       Running   0          1d\n",
      "kube-system            kube-proxy-zt78c                                       1/1       Running   1          1d\n",
      "kube-system            kube-scheduler-os004k8-master001.foobar.com            1/1       Running   169        1d\n",
      "kube-system            metrics-server-6b6478446b-nhd4z                        1/1       Running   86         1d\n",
      "kube-system            weave-net-bm4dd                                        2/2       Running   0          1d\n",
      "kube-system            weave-net-jtxhq                                        2/2       Running   2          1d\n",
      "kube-system            weave-net-w5d9k                                        2/2       Running   0          1d\n",
      "kube-system            weave-net-xlm2w                                        2/2       Running   8          1d\n",
      "kubernetes-dashboard   dashboard-metrics-scraper-5594697f48-6wckg             1/1       Running   0          1d\n",
      "kubernetes-dashboard   kubernetes-dashboard-57c9bfc8c8-6769h                  1/1       Running   73         1d\n"
     ]
    }
   ],
   "source": [
    "! kubectl get pod --all-namespaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de73a68e-c3f5-4f4e-aaf0-65fae20a2577",
   "metadata": {},
   "source": [
    "# 6. Configure Kubernetes To Host Apache Spark\n",
    "\n",
    "In order for our kubernetes cluster to successfully run a spark cluster we need to do a few things:\n",
    "1. Configure RBAC - We will need to set permissions so that our jupyter notebook and spark components have the appropriate permissions.\n",
    "2. Build containers - We will need to build the contaienrs which host our spark cluster nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0c0aa0-3d16-4870-9590-3e079596db54",
   "metadata": {},
   "source": [
    "## 6.1. Configure Kubernetes RBAC\n",
    "There are many ways to configure the RBAC of the kubernetes cluster. We are taking the simplest route posisble. We will define the follwing kubernetes objects:\n",
    "- Namspace named \"spark\" which will container our spark related infrastructure\n",
    "- ServiceAccount named spark-sa to serve as an identity to invoke commands as\n",
    "- ClusterRole named spark-role with required permissions\n",
    "- ClusterRoleBinding named spark-role-binding which attached the permissions defined in the role with the service account\n",
    "\n",
    "We will put our configurations into a kubernetes manifest file which was defined as follows:\n",
    "\n",
    "```\n",
    "---\n",
    "kind: Namespace\n",
    "apiVersion: v1\n",
    "metadata:\n",
    "  name: spark\n",
    "---\n",
    "kind: ServiceAccount\n",
    "apiVersion: v1\n",
    "metadata:\n",
    "  name: spark-sa\n",
    "---\n",
    "kind: ClusterRole\n",
    "apiVersion: rbac.authorization.k8s.io/v1\n",
    "metadata:\n",
    "  namespace: default\n",
    "  name: spark-role\n",
    "rules:\n",
    "  - apiGroups: [\"\"]\n",
    "    resources: [\"pods\", \"services\", \"configmaps\" ]\n",
    "    verbs: [\"create\", \"get\", \"watch\", \"list\", \"post\", \"delete\"  ]\n",
    "---\n",
    "kind: ClusterRoleBinding\n",
    "apiVersion: rbac.authorization.k8s.io/v1\n",
    "metadata:\n",
    "  name: spark-role-binding\n",
    "subjects:\n",
    "  - kind: ServiceAccount\n",
    "    name: spark-sa\n",
    "    namespace: spark\n",
    "roleRef:\n",
    "  kind: ClusterRole\n",
    "  name: spark-role\n",
    "  apiGroup: rbac.authorization.k8s.io\n",
    "\n",
    "```\n",
    "\n",
    "We can apply the configuration using our kubectl command\n",
    "\n",
    "```\n",
    "[root@os004k8-master001 ~]# kubectl apply -f spark.manifest\n",
    "namespace/spark created\n",
    "serviceaccount/spark-sa created\n",
    "clusterrole.rbac.authorization.k8s.io/spark-role created\n",
    "clusterrolebinding.rbac.authorization.k8s.io/spark-role-binding created\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fb7937-d727-407f-b9de-5145efdc91f5",
   "metadata": {},
   "source": [
    "We can then comfirm that the objects were creates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a09865b9-d63e-4f78-a6cb-a8f5b6249ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                   STATUS    AGE\n",
      "default                Active    1d\n",
      "kube-node-lease        Active    1d\n",
      "kube-public            Active    1d\n",
      "kube-system            Active    1d\n",
      "kubernetes-dashboard   Active    1d\n",
      "spark                  Active    55m\n"
     ]
    }
   ],
   "source": [
    "! kubectl get namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4eef2736-d99a-4a4a-8bf5-a922eaf23514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME       SECRETS   AGE\n",
      "default    1         1d\n",
      "spark-sa   1         55m\n"
     ]
    }
   ],
   "source": [
    "! kubectl get serviceAccounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "efc94703-1ea8-460e-b933-ac97e251ea46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                                                   KIND\n",
      "admin                                                                  ClusterRole.v1.rbac.authorization.k8s.io\n",
      "cluster-admin                                                          ClusterRole.v1.rbac.authorization.k8s.io\n",
      "edit                                                                   ClusterRole.v1.rbac.authorization.k8s.io\n",
      "kubeadm:get-nodes                                                      ClusterRole.v1.rbac.authorization.k8s.io\n",
      "kubernetes-dashboard                                                   ClusterRole.v1.rbac.authorization.k8s.io\n",
      "spark-role                                                             ClusterRole.v1.rbac.authorization.k8s.io\n",
      "system:aggregate-to-admin                                              ClusterRole.v1.rbac.authorization.k8s.io\n"
     ]
    }
   ],
   "source": [
    "! kubectl -n spark get clusterRoles | head -n 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b511baff-29b0-4d7b-9f58-979674606a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                                   KIND\n",
      "cluster-admin                                          ClusterRoleBinding.v1.rbac.authorization.k8s.io\n",
      "kubeadm:get-nodes                                      ClusterRoleBinding.v1.rbac.authorization.k8s.io\n",
      "kubeadm:kubelet-bootstrap                              ClusterRoleBinding.v1.rbac.authorization.k8s.io\n",
      "kubeadm:node-autoapprove-bootstrap                     ClusterRoleBinding.v1.rbac.authorization.k8s.io\n",
      "kubeadm:node-autoapprove-certificate-rotation          ClusterRoleBinding.v1.rbac.authorization.k8s.io\n",
      "kubeadm:node-proxier                                   ClusterRoleBinding.v1.rbac.authorization.k8s.io\n",
      "kubernetes-dashboard                                   ClusterRoleBinding.v1.rbac.authorization.k8s.io\n",
      "metrics-server:system:auth-delegator                   ClusterRoleBinding.v1.rbac.authorization.k8s.io\n",
      "spark-role-binding                                     ClusterRoleBinding.v1.rbac.authorization.k8s.io\n",
      "system:basic-user                                      ClusterRoleBinding.v1.rbac.authorization.k8s.io\n",
      "system:controller:attachdetach-controller              ClusterRoleBinding.v1.rbac.authorization.k8s.io\n",
      "system:controller:certificate-controller               ClusterRoleBinding.v1.rbac.authorization.k8s.io\n",
      "system:controller:clusterrole-aggregation-controller   ClusterRoleBinding.v1.rbac.authorization.k8s.io\n",
      "system:controller:cronjob-controller                   ClusterRoleBinding.v1.rbac.authorization.k8s.io\n",
      "system:controller:daemon-set-controller                ClusterRoleBinding.v1.rbac.authorization.k8s.io\n",
      "system:controller:deployment-controller                ClusterRoleBinding.v1.rbac.authorization.k8s.io\n",
      "system:controller:disruption-controller                ClusterRoleBinding.v1.rbac.authorization.k8s.io\n",
      "system:controller:endpoint-controller                  ClusterRoleBinding.v1.rbac.authorization.k8s.io\n",
      "system:controller:endpointslice-controller             ClusterRoleBinding.v1.rbac.authorization.k8s.io\n",
      "system:controller:endpointslicemirroring-controller    ClusterRoleBinding.v1.rbac.authorization.k8s.io\n",
      "system:controller:ephemeral-volume-controller          ClusterRoleBinding.v1.rbac.authorization.k8s.io\n",
      "system:controller:expand-controller                    ClusterRoleBinding.v1.rbac.authorization.k8s.io\n",
      "system:controller:generic-garbage-collector            ClusterRoleBinding.v1.rbac.authorization.k8s.io\n",
      "system:controller:horizontal-pod-autoscaler            ClusterRoleBinding.v1.rbac.authorization.k8s.io\n",
      "system:controller:job-controller                       ClusterRoleBinding.v1.rbac.authorization.k8s.io\n",
      "system:controller:namespace-controller                 ClusterRoleBinding.v1.rbac.authorization.k8s.io\n",
      "system:controller:node-controller                      ClusterRoleBinding.v1.rbac.authorization.k8s.io\n",
      "system:controller:persistent-volume-binder             ClusterRoleBinding.v1.rbac.authorization.k8s.io\n",
      "system:controller:pod-garbage-collector                ClusterRoleBinding.v1.rbac.authorization.k8s.io\n",
      "system:controller:pv-protection-controller             ClusterRoleBinding.v1.rbac.authorization.k8s.io\n",
      "system:controller:pvc-protection-controller            ClusterRoleBinding.v1.rbac.authorization.k8s.io\n",
      "system:controller:replicaset-controller                ClusterRoleBinding.v1.rbac.authorization.k8s.io\n",
      "system:controller:replication-controller               ClusterRoleBinding.v1.rbac.authorization.k8s.io\n",
      "system:controller:resourcequota-controller             ClusterRoleBinding.v1.rbac.authorization.k8s.io\n",
      "system:controller:root-ca-cert-publisher               ClusterRoleBinding.v1.rbac.authorization.k8s.io\n",
      "system:controller:route-controller                     ClusterRoleBinding.v1.rbac.authorization.k8s.io\n",
      "system:controller:service-account-controller           ClusterRoleBinding.v1.rbac.authorization.k8s.io\n",
      "system:controller:service-controller                   ClusterRoleBinding.v1.rbac.authorization.k8s.io\n",
      "system:controller:statefulset-controller               ClusterRoleBinding.v1.rbac.authorization.k8s.io\n",
      "system:controller:ttl-after-finished-controller        ClusterRoleBinding.v1.rbac.authorization.k8s.io\n",
      "system:controller:ttl-controller                       ClusterRoleBinding.v1.rbac.authorization.k8s.io\n",
      "system:coredns                                         ClusterRoleBinding.v1.rbac.authorization.k8s.io\n",
      "system:discovery                                       ClusterRoleBinding.v1.rbac.authorization.k8s.io\n",
      "system:kube-controller-manager                         ClusterRoleBinding.v1.rbac.authorization.k8s.io\n",
      "system:kube-dns                                        ClusterRoleBinding.v1.rbac.authorization.k8s.io\n",
      "system:kube-scheduler                                  ClusterRoleBinding.v1.rbac.authorization.k8s.io\n",
      "system:metrics-server                                  ClusterRoleBinding.v1.rbac.authorization.k8s.io\n",
      "system:monitoring                                      ClusterRoleBinding.v1.rbac.authorization.k8s.io\n",
      "system:node                                            ClusterRoleBinding.v1.rbac.authorization.k8s.io\n",
      "system:node-proxier                                    ClusterRoleBinding.v1.rbac.authorization.k8s.io\n",
      "system:public-info-viewer                              ClusterRoleBinding.v1.rbac.authorization.k8s.io\n",
      "system:service-account-issuer-discovery                ClusterRoleBinding.v1.rbac.authorization.k8s.io\n",
      "system:volume-scheduler                                ClusterRoleBinding.v1.rbac.authorization.k8s.io\n",
      "weave-net                                              ClusterRoleBinding.v1.rbac.authorization.k8s.io\n"
     ]
    }
   ],
   "source": [
    "! kubectl -n spark get clusterRoleBindings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c90a519-9bca-42bc-af30-5beefedf0801",
   "metadata": {},
   "source": [
    "## 6.2. Build Spark Containers For Kubernetes\n",
    "\n",
    "In order to run Apache Spark on kubernetes, Docker container images need to be prepared to serve as the master/slave notes of the Spark cluster. Inside this container image we will install our spark binaries etc. As we will see, when we create our SparkContex we specify this container when it is created.\n",
    "\n",
    "Spark (starting with version 2.3) ships with a Dockerfile that can be used for this purpose. These containers are debian based and the Dockerfile will install the latest version of python. This version of python neets to match the major version being run on the client which is hosting the jupyter noatbook. In other words, if we are running python 3.6 in our jupyter notebook, we need to ensure our docker container has the same version installed. \n",
    "\n",
    "In most cases one will need to modify or extend this image. Not only to modify the version of python being installed but also to modify the python packages which are installed on the spark nodes. Any packages we expect to execute on a cluster node will need to be installed in the docker image. This includes any software we might be using to store data, train machine learning algorithms, or perform optimizations.\n",
    "\n",
    "I have prepared a container which runs python 3.6 and is packaged with the necessary packages to execute these notebooks.\n",
    "\n",
    "For more information see the [Kubernetes 3.1.1 documentation](https://spark.apache.org/docs/3.1.1/running-on-kubernetes.html#docker-images)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
