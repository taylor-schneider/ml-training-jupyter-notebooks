{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "Previously we have seen examples of running the k-means algorithm provided by both scikit-learn and MLlib. In the previous notebook regarding [Running MLlib Algorothms](Running%20MLlib%20Algorithms%20%28k-means%29.ipynb) we learned that the MLlib algorithms are parallelized for us. In other words, behind the scenes the library automagically leverages the spark framework to distribute data to the cluster and perform work in parallel. This is a very useful feature as it allows us to shorten the amount of time to train/test a model on a large data set while also reducing the complexity of our ML code.\n",
    "\n",
    "The problem exists however with the Spark implimentation; it does not support nesting. If we want to run multiple MLlib algorithms we will not be able to do it in parallel (long discussion below) out of the box.\n",
    "\n",
    "Ok, so what are we building in this notebook? We will be building a workaround so we can run MLlib algorithms in parallel from a single notebook.\n",
    "\n",
    "Below is a detailed explanation of the problem and some warnings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Doesnt Support Nesting\n",
    "\n",
    "What we mean to say is that Apache Spark doesn't allow the workers to act as drivers; They cannot submit work to cluster which they themselves are a part of. I had a look at the spark project's source code and saw that Spark libraries will infact check if it is running on a worker and will raise an exception if that is the case. (more on this in a minute).\n",
    "\n",
    "Because our workers cannot submit work to the cluster they cannot run MLlib functions. This means that if we want to run an MLlib algorithm, it needs to be run from the driver. In doing so, the MLlib library will in turn use the driver's SparkContext to submit work to the spark cluster.\n",
    "\n",
    "For curiosity's sake, if you tried to have a Spark Executor (worker) create and train an MLlib algorithm you would see the following error pop up:\n",
    "\n",
    "```\n",
    "AttributeError: Cannot load _jvm from SparkContext. Is SparkContext initialized?\n",
    "```\n",
    "\n",
    "This took me a while to figure out the root issue, but it is pointed out in this [discussion](https://coderedirect.com/questions/310003/run-ml-algorithm-inside-map-function-in-spark)\n",
    "\n",
    "## Why did we want nesting?\n",
    "Originally, I wanted nesting so that I could acheive parallelism. I wanted to be able to run multiple MLlib processes in parallel. Out-of-the-box, Spark provides a number of useful mechanism for kicking off parallel processes based on our dataset. For example, in the previous notebook using scikit-learn models, we use the `.groupby(date).apply(training_function)` functionality to kick off a training job for each group of data in parallel. \n",
    "\n",
    "Due to the limitations noted above, we are not able to use this api with MLlib. as such we will have to come up with our own implimentation to achieve this \"nested parallelism\"; we parallelize and MLlib parallelizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Options For Home Grown Nested Parllalelism\n",
    "\n",
    "Ok, so we cant use Spark to parrallelize spark calls, what are our options?\n",
    "\n",
    "1. Have the driver kick off and manage parralel operations using a single SparkContext\n",
    "2. Have the driver kick off and manage parralel operations using multiple SparkContexts pointing to multiple clusters\n",
    "3. Hack the worker to create a SparkContext / Submit a PR to Spark's official Repo\n",
    "4. Use a higher level orchestration tool to manage one layer of the parallelization.\n",
    "\n",
    "**Note**: If you have other suggestions, please open a github issue so we can chat!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1: Driver Manages Parallel Calls To SparkContext\n",
    "I'm sure my curiosity will lead me to explore all these vairous options (as of writing this I have seen a few alternate solutions which I haven't had time to document).\n",
    "\n",
    "For this notebook we will look at Option 1: Managing parallel operations with a single spark context.\n",
    "\n",
    "We can do this using either multithreading or multiprocessing. If you are not familiar with these topics, please review the [prepared materials](../../../Programming/Parallelization/README.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## There Be Dragons Ahead\n",
    "**Note**: This is a very complex subject and requires engineering skills as well as knowledge of troubleshooting spark. See gotchas section.\n",
    "\n",
    "One could make the case that we are abusing Spark (ie. using it in a way it was not designed). As a systems engineer that's basically my job so Nuts to that. With most distributed systems we will run into problems that we cannot explain or cannot reproduce. While writing this notebook I documented some issues I ran into and their solutions.\n",
    "\n",
    "You might run out of memory when you have multiple jobs competing against eachother. \n",
    "\n",
    "```\n",
    "Py4JJavaError: An error occurred while calling o689322.createOrReplaceTempView.\n",
    ": java.lang.OutOfMemoryError: Java heap space\n",
    "```\n",
    "\n",
    "As such you may need to adjust your sparkConf to allocate more memory to the driver/worker. After reading through various blog posts here are the relevant settings and example values for each setting that worked for me.\n",
    "\n",
    "```python\n",
    "    sparkConf.set(\"spark.executor.instances\", \"3\")\n",
    "    sparkConf.set(\"spark.executor.cores\", \"2\")\n",
    "    sparkConf.set(\"spark.executor.memory\", \"4096m\")\n",
    "    sparkConf.set(\"spark.executor.memoryOverhead\", \"1024m\")\n",
    "    sparkConf.set(\"spark.driver.memory\", \"1024m\")\n",
    "```\n",
    "\n",
    "Thing blowing up may cause other things to blow up unexpectedly. I coded a bug into my solution where i was killing a job before it finished writing data which corrupted the data file it was writing... Don't do this.\n",
    "\n",
    "```\n",
    "Py4JJavaError: An error occurred while calling o436701.csv.\n",
    ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 22489.0 failed 4 times, most recent failure: Lost task 3.3 in stage 22489.0 (TID 311712) (10.42.0.1 executor 2): java.io.EOFException: Cannot seek after EOF\n",
    "```\n",
    "\n",
    "I encountered some other issues and wrapped my function in retry logic just incase (this may not have been needed as the issue might have been avoidable and also resolved in a later version of code)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "It assumes you have already read the following notebooks:\n",
    "- [Install Apache Spark Prerequisites](Install%20Apache%20Spark%20Prerequisites.ipynb)\n",
    "- [Spark Pi - The Hello World Example For Apache spark](Spark%20Pi%20-%20The%20Hello%20World%20Example%20For%20Apache%20spark.ipynb)\n",
    "- [Intro To Koalas](Intro%20To%20Koalas.ipynb)\n",
    "- [K-Means](../../Algorithms/Unsupervised%20Learning/Cluster%20Analysis/K-Means.ipynb)\n",
    "- [Load CSV Into Apache Spark On Kubernetes](Load%20CSV%20Into%20Apache%20Spark%20On%20Kubernetes.ipynb)\n",
    "- [Parallelization](../../../Programming/Parallelization/README.md)\n",
    "\n",
    "The instructions are basically the same as [Running MLlib Algorothms](Running%20MLlib%20Algorithms%20%28k-means%29.ipynb) with the added framework I mentioned.\n",
    "\n",
    "## Adjenda\n",
    "1. Create SparkContext\n",
    "2. Create Web Server To Host Data\n",
    "3. Load The Data\n",
    "4. Develop Parallelization Framework\n",
    "5. Test Framework Performance\n",
    "8. Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Create SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting SPARK_HOME\n",
      "/usr/lib/spark-3.1.1-bin-hadoop2.7\n",
      "\n",
      "Running findspark.init() function\n",
      "['/usr/lib/spark-3.1.1-bin-hadoop2.7/python', '/usr/lib/spark-3.1.1-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip', '/ml-training-jupyter-notebooks/Machine Learning/Big Data And Big Compute/Apache Spark', '/usr/local/lib/python39.zip', '/usr/local/lib/python3.9', '/usr/local/lib/python3.9/lib-dynload', '', '/usr/local/lib/python3.9/site-packages', '/ml-training-jupyter-notebooks/Utilities']\n",
      "\n",
      "Setting PYSPARK_PYTHON\n",
      "/usr/local/bin/python3\n",
      "\n",
      "Configuring URL for kubernetes master\n",
      "k8s://https://15.4.7.11:6443\n",
      "\n",
      "Determining IP Of Server\n",
      "The ip was detected as: 15.4.12.12\n",
      "\n",
      "Creating SparkConf Object\n",
      "('spark.master', 'k8s://https://15.4.7.11:6443')\n",
      "('spark.app.name', 'spark-jupyter-mlib')\n",
      "('spark.submit.deploy.mode', 'cluster')\n",
      "('spark.kubernetes.container.image', 'tschneider/apache-spark-k8:v7')\n",
      "('spark.kubernetes.namespace', 'spark')\n",
      "('spark.kubernetes.pyspark.pythonVersion', '3')\n",
      "('spark.kubernetes.authenticate.driver.serviceAccountName', 'spark-sa')\n",
      "('spark.kubernetes.authenticate.serviceAccountName', 'spark-sa')\n",
      "('spark.executor.instances', '3')\n",
      "('spark.executor.cores', '2')\n",
      "('spark.executor.memory', '4096m')\n",
      "('spark.executor.memoryOverhead', '1024m')\n",
      "('spark.driver.memory', '1024m')\n",
      "('spark.driver.host', '15.4.12.12')\n",
      "('spark.files.overwrite', 'true')\n",
      "('spark.files.useFetchCache', 'false')\n",
      "\n",
      "Creating SparkSession Object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/20 22:45:33 WARN Utils: Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 15.4.12.12 instead (on interface eth0)\n",
      "22/01/20 22:45:33 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/lib/spark-3.1.1-bin-hadoop2.7/jars/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/01/20 22:45:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "spark_app_name = \"spark-jupyter-mlib\"\n",
    "docker_image = \"tschneider/apache-spark-k8:v7\"\n",
    "k8_master_ip = \"15.4.7.11\"\n",
    "\n",
    "import spark_helper\n",
    "spark_session = spark_helper.create_spark_session(spark_app_name, docker_image, k8_master_ip)\n",
    "sc = spark_session.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                         READY     STATUS    RESTARTS   AGE\n",
      "spark-jupyter-mlib-1f85777e79aab58c-exec-1   1/1       Running   0          23s\n",
      "spark-jupyter-mlib-1f85777e79aab58c-exec-2   1/1       Running   0          23s\n",
      "spark-jupyter-mlib-1f85777e79aab58c-exec-3   1/1       Running   0          23s\n"
     ]
    }
   ],
   "source": [
    "! kubectl -n spark get pods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Setup Datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ml-training-jupyter-notebooks\n"
     ]
    }
   ],
   "source": [
    "import pyprojroot\n",
    "project_root_dir  = pyprojroot.here()\n",
    "print(project_root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data_dir_name = \"Example Data Sets\"\n",
    "data_dir_path = os.path.join(project_root_dir, data_dir_name)\n",
    "spark_helper.symlink_dir_to_root(data_dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our custom module for the web server we wrote\n",
    "import PythonHttpFileServer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Starting server on port 80\n",
      "INFO:root:Web root specified as: /ml-training-jupyter-notebooks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app 'PythonHttpFileServer' (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Import the library\n",
    "import threading\n",
    "\n",
    "# Configure the logger and log level (incase we need/want to debug)\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Create and start the thread if it doesnt exist\n",
    "web_root = project_root_dir\n",
    "var_exists = 'web_server_thread' in locals() or 'web_server_thread' in globals()\n",
    "if not var_exists:\n",
    "    web_server_port = 80\n",
    "    web_server_args = (web_server_port, web_root)\n",
    "    web_server_thread = threading.Thread(target=PythonHttpFileServer.run_server, args=web_server_args)\n",
    "    web_server_thread.start()\n",
    "else:\n",
    "    print(\"Web Server thread already exists\")\n",
    "    print(\"To kill it you need to restart the kernel.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Load The Data \n",
    "To help debug/test our framework we will load the data and build a few utilities targeting pices of the data. Once everything works we will run everything in parallel. Again, this step is just for Testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Add File To Spark Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:werkzeug: * Running on all addresses.\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "INFO:werkzeug: * Running on http://15.4.12.12:80/ (Press CTRL+C to quit)\n",
      "INFO:root:Get /ml-training-jupyter-notebooks/Example Data Sets/nasdaq_2019.csv\n",
      "INFO:werkzeug:15.4.12.12 - - [20/Jan/2022 22:46:05] \"GET /Example%2520Data%2520Sets/nasdaq_2019.csv HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading file 'http://15.4.12.12:80/Example%20Data%20Sets/nasdaq_2019.csv' to Spark cluster.\n"
     ]
    }
   ],
   "source": [
    "import urllib.parse\n",
    "import spark_helper\n",
    "\n",
    "ip_address = spark_helper.determine_ip_address()\n",
    "csv_file_name = \"nasdaq_2019.csv\"\n",
    "csv_file_path = os.path.join(web_root, data_dir_name, csv_file_name)\n",
    "csv_file_url = \"http://{0}:{1}/{2}/{3}\".format(\n",
    "    ip_address, \n",
    "    web_server_port, \n",
    "    urllib.parse.quote(data_dir_name), \n",
    "    urllib.parse.quote(csv_file_name))\n",
    "\n",
    "print(\"Uploading file '{0}' to Spark cluster.\".format(csv_file_url))\n",
    "sc.addFile(csv_file_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Use Koalas To Load Data File Into DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the utility function to convert a date string to a datetime object from our utilities module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the utilities module we wrote\n",
    "import utilities\n",
    "\n",
    "# Define a mapping to convert our data field to the correct type\n",
    "converter_mapping = {\n",
    "    \"date\": utilities.convert_date_string_to_date\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Load our OHCLV data Into a koalas dataframe and pull out a single day in the say way we would in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Get /ml-training-jupyter-notebooks/Example Data Sets/nasdaq_2019.csv\n",
      "INFO:werkzeug:15.4.7.101 - - [20/Jan/2022 22:46:14] \"GET /Example%20Data%20Sets/nasdaq_2019.csv HTTP/1.1\" 200 -\n",
      "INFO:root:Get /ml-training-jupyter-notebooks/Example Data Sets/nasdaq_2019.csv  \n",
      "INFO:root:Get /ml-training-jupyter-notebooks/Example Data Sets/nasdaq_2019.csv\n",
      "INFO:werkzeug:15.4.7.103 - - [20/Jan/2022 22:46:18] \"GET /Example%20Data%20Sets/nasdaq_2019.csv HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:15.4.7.102 - - [20/Jan/2022 22:46:18] \"GET /Example%20Data%20Sets/nasdaq_2019.csv HTTP/1.1\" 200 -\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Avoid a warning\n",
    "import os\n",
    "os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\n",
    "os.environ[\"SPARK_KOALAS_AUTOPATCH\"] = \"0\"\n",
    "\n",
    "from databricks import koalas\n",
    "koalas_dataframe = koalas.read_csv(u\"file:////{0}\".format(csv_file_name), converters=converter_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/20 22:46:32 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/01/20 22:46:40 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>interval</th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AABA</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>70.90</td>\n",
       "      <td>71.5200</td>\n",
       "      <td>70.3250</td>\n",
       "      <td>70.57</td>\n",
       "      <td>10234800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAL</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>33.14</td>\n",
       "      <td>33.6632</td>\n",
       "      <td>32.5301</td>\n",
       "      <td>32.88</td>\n",
       "      <td>8995100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAME</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>2.43</td>\n",
       "      <td>2.4300</td>\n",
       "      <td>2.4000</td>\n",
       "      <td>2.40</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAOI</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>10.70</td>\n",
       "      <td>10.8900</td>\n",
       "      <td>10.0100</td>\n",
       "      <td>10.18</td>\n",
       "      <td>883100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAON</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>50.57</td>\n",
       "      <td>50.9850</td>\n",
       "      <td>48.5600</td>\n",
       "      <td>49.73</td>\n",
       "      <td>180200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker interval        date   open     high      low  close    volume\n",
       "0   AABA        D  2019-07-01  70.90  71.5200  70.3250  70.57  10234800\n",
       "1    AAL        D  2019-07-01  33.14  33.6632  32.5301  32.88   8995100\n",
       "2   AAME        D  2019-07-01   2.43   2.4300   2.4000   2.40       500\n",
       "3   AAOI        D  2019-07-01  10.70  10.8900  10.0100  10.18    883100\n",
       "4   AAON        D  2019-07-01  50.57  50.9850  48.5600  49.73    180200"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "koalas_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Extract the dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case seach date's worth od data is independent. This means we can run kmeans in parallel on the independent chunks of data. As such, we have have the independent processes load the coresponding chunk of data. We simply need a list of dates to feed into our parallelization framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/20 22:46:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2019-01-01'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a list of the dates in our dataframe\n",
    "dates = koalas_dataframe[\"date\"].unique().sort_values().to_numpy()\n",
    "dates[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Develop Parralelization Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this next section we will develop the utilities that allow us to apply the kmeans algorithm from Spark MLlib to each date in our koalas_dataframe object in parallel. \n",
    "\n",
    "Note: The dates contain mutually exclusive data so we can parallelize accross this variable.\n",
    "\n",
    "We will need several utility functions to help us out:\n",
    "1. The function that runs on the workers\n",
    "\n",
    "   This function will load the data, filter it to a specific date, apply the algorithm,\n",
    "   and return the results.\n",
    "\n",
    "2. The function that runs on the driver\n",
    "\n",
    "   This is that master function. It will need to instruct the workers to do their work\n",
    "   and coordinate the aggregation of the results. The results will be written to a local\n",
    "   data file.\n",
    "   \n",
    "   We will break this giant function into a few nested function.\n",
    "\n",
    "3. The function to handle retries\n",
    "\n",
    "   We have experienced transient errors with this solution so we want a function to\n",
    "   automatgically retry on failure rather than killing the workflow or having a hole\n",
    "   in the data set.\n",
    "\n",
    "We will leverage a parallelization utility we wrote previously. We can do this using either multithreading or multiprocessing. If you are not familiar with these topics, please review the [prepared materials](../../../Programming/Parallelization/README.md). Is section #5 of this notebook we see the rubber hit the road.\n",
    "\n",
    "We will talk more about this in a second. For the moment, we need to write the utilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Create Parallelizable Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to write a function that can run in parallel on the worker nodes of our spark cluster. This function will need to load the data, perform kmeans, and return the resuling classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas\n",
    "#import pyspark\n",
    "#from pyspark.sql.functions import pandas_udf, udf\n",
    "#from pyspark.sql.types import *\n",
    "from databricks import koalas\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "# Allow data to transfer between dataframes\n",
    "# Without this we get an error\n",
    "koalas.set_option(\"compute.ops_on_diff_frames\", True)\n",
    "\n",
    "def perform_kmeans_on_dataframe(k, date, column_names, seed=None):\n",
    "    \n",
    "    # Load the data coresponding to the date\n",
    "    tmp = koalas.read_csv(u\"file:////nasdaq_2019.csv\", converters=converter_mapping)\n",
    "    tmp = tmp.loc[tmp[\"date\"] == date]\n",
    "    \n",
    "    # Create our model\n",
    "    if seed:\n",
    "        model = KMeans().setK(k).setSeed(seed)\n",
    "    else:\n",
    "        model = KMeans().setK(k)\n",
    "        \n",
    "    # Do some magic to get the data in the right format for the spark model\n",
    "    # (The model needs vectorized parameters)\n",
    "    from pyspark.ml.feature import VectorAssembler\n",
    "    assembler = VectorAssembler(inputCols=column_names, outputCol=\"features\")\n",
    "    if type(tmp) == koalas.frame.DataFrame:\n",
    "        model_parameters = assembler.transform(tmp[[*column_names]].to_spark())\n",
    "    elif type(tmp) == pandas.DataFrame:\n",
    "        tmp = koalas.DataFrame(tmp)\n",
    "        model_parameters = assembler.transform(tmp[[*column_names]].to_spark())\n",
    "\n",
    "    # Train the model\n",
    "    trained_model = model.fit(model_parameters)\n",
    "    \n",
    "    # Gather information about the clustering of our data\n",
    "    predictions = trained_model.transform(model_parameters)\n",
    "    cluster_indices = predictions.select(\"prediction\")\n",
    "    cluster_indices = koalas.DataFrame(cluster_indices).to_numpy().reshape(-1)\n",
    "    cluster_indices = koalas.Series(cluster_indices, index=tmp.index.to_numpy())\n",
    "    cluster_centroids = trained_model.clusterCenters()\n",
    "    \n",
    "    # Update our dataframe with this clustering information\n",
    "    tmp[\"cluster_indices\"] = cluster_indices    \n",
    "    tmp[\"cluster_centroids\"] = tmp[\"cluster_indices\"].apply(lambda i: str(cluster_centroids[i]))\n",
    "    \n",
    "    # Return the results\n",
    "    return tmp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/20 22:47:08 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "22/01/20 22:47:08 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "22/01/20 22:47:16 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/01/20 22:47:18 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/01/20 22:47:22 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "/usr/local/lib/python3.9/site-packages/pyspark/sql/pandas/functions.py:389: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n",
      "22/01/20 22:47:30 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/01/20 22:47:34 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>interval</th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>cluster_indices</th>\n",
       "      <th>cluster_centroids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>96829</th>\n",
       "      <td>ACLS</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>17.48</td>\n",
       "      <td>18.14</td>\n",
       "      <td>16.920</td>\n",
       "      <td>17.79</td>\n",
       "      <td>284200</td>\n",
       "      <td>0</td>\n",
       "      <td>[18.28656245 18.6213758 ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96923</th>\n",
       "      <td>ALLK</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>51.26</td>\n",
       "      <td>53.94</td>\n",
       "      <td>50.115</td>\n",
       "      <td>51.99</td>\n",
       "      <td>151900</td>\n",
       "      <td>0</td>\n",
       "      <td>[18.28656245 18.6213758 ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97216</th>\n",
       "      <td>BRPAU</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>10.55</td>\n",
       "      <td>10.55</td>\n",
       "      <td>10.550</td>\n",
       "      <td>10.55</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[18.28656245 18.6213758 ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97658</th>\n",
       "      <td>DWFI</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>22.21</td>\n",
       "      <td>22.24</td>\n",
       "      <td>22.200</td>\n",
       "      <td>22.20</td>\n",
       "      <td>41300</td>\n",
       "      <td>0</td>\n",
       "      <td>[18.28656245 18.6213758 ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97699</th>\n",
       "      <td>EFAS</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>15.08</td>\n",
       "      <td>15.08</td>\n",
       "      <td>15.080</td>\n",
       "      <td>15.08</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[18.28656245 18.6213758 ]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ticker interval        date   open   high     low  close  volume  cluster_indices          cluster_centroids\n",
       "96829   ACLS        D  2019-01-02  17.48  18.14  16.920  17.79  284200                0  [18.28656245 18.6213758 ]\n",
       "96923   ALLK        D  2019-01-02  51.26  53.94  50.115  51.99  151900                0  [18.28656245 18.6213758 ]\n",
       "97216  BRPAU        D  2019-01-02  10.55  10.55  10.550  10.55       0                0  [18.28656245 18.6213758 ]\n",
       "97658   DWFI        D  2019-01-02  22.21  22.24  22.200  22.20   41300                0  [18.28656245 18.6213758 ]\n",
       "97699   EFAS        D  2019-01-02  15.08  15.08  15.080  15.08       0                0  [18.28656245 18.6213758 ]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perform_kmeans_on_dataframe(5, '2019-01-02', column_names=[\"open\", \"close\"]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: The warning above is coming from code used in the internals of Koalas. Do not worry about this warning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Create Utilities For Parallelization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We previously wrote a function which will perform kmeans for a specific date. Now we need to go a few steps further:\n",
    "\n",
    "Ultimately we would like to aggregate the results of each date into a consolidated dataframe. I have found however that the easiest way to do this is the most inefficient. Appending data to an existing spark dataframe was very very slow and resource intensive. Results from workers are returned as pandas dataframes (they need to be serielized and sent over the wire) as such, appending them means the data needs to be kept in memory and then sent back over the wire and then sent back to us one more time in order to us to get it into a dataframe.\n",
    "\n",
    "To solve this problem, we will have the driver collect the results from the workers and write them into a data file. The trick is that we are parallelizing calls to MLlib so results will be returned simultaneously. We will use a mutex to synchronize our asynchronous threads. If you are not familiar with these topics, please review the [prepared materials](../../../Programming/Parallelization/README.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Define a function to perform kmeans and store the results in a local data file\n",
    "\n",
    "def perform_kmeans_on_dataframe_and_store_results(params):\n",
    "    \n",
    "    # Retrieve params\n",
    "    k = params[0]\n",
    "    date = params[1]\n",
    "    column_names = params[2]\n",
    "    lock = params[3]\n",
    "    result_file_path = params[4]\n",
    "    \n",
    "    # Do the computation\n",
    "    result_df = perform_kmeans_on_dataframe(k, date, column_names)\n",
    "    \n",
    "    # Force spark to not be lazy and to do the computation\n",
    "    result_df.shape  \n",
    "    \n",
    "    # Record the results to a local data file\n",
    "    with lock:\n",
    "        if os.path.exists(result_file_path):                  \n",
    "            result_df.to_pandas().to_csv(result_file_path, mode='a', index=False)\n",
    "        else:\n",
    "            result_df.to_pandas().to_csv(result_file_path, mode='w', index=False, header=True)\n",
    "\n",
    "    # Return results and timing info to the ThreadHelper\n",
    "    return result_df\n",
    "\n",
    "# Write a function to retry the code if an error is encountered.\n",
    "# We may enconuter some weird transient errors when runnong on spark\n",
    "# As I mentioned earlier \"there be dragons\"\n",
    "# So we have a simply retry wrapper to handle any weird execptions that \n",
    "# can be resolved by rerunning the same code\n",
    "\n",
    "def retry_wrapper(func, params, max_retries=5):\n",
    "    retries_remaining = max_retries\n",
    "    while True:\n",
    "        try:\n",
    "            return func(params)\n",
    "        except Exception as e:\n",
    "            retries_remaining -= 1\n",
    "            if retries_remaining > 0:\n",
    "                print(\"Error occurred with parameter set: {0}. {1} retries remaining\".format(params, max_retires - retries_remaining))\n",
    "                time.sleep(1)\n",
    "            else:\n",
    "                raise e\n",
    "                \n",
    "def perform_kmeans_on_dataframe__store_results__and_retry(params):\n",
    "    \n",
    "    retry_wrapper(perform_kmeans_on_dataframe_and_store_results, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test that this works with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Get /ml-training-jupyter-notebooks/Example Data Sets/results.csv      \n",
      "INFO:werkzeug:15.4.12.12 - - [20/Jan/2022 22:49:37] \"GET /Example%20Data%20Sets/results.csv HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating file on driver.\n",
      "Updating file on workers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Get /ml-training-jupyter-notebooks/Example Data Sets/results.csv\n",
      "INFO:werkzeug:15.4.7.103 - - [20/Jan/2022 22:49:37] \"GET /Example%20Data%20Sets/results.csv HTTP/1.1\" 200 -\n",
      "INFO:root:Get /ml-training-jupyter-notebooks/Example Data Sets/results.csv\n",
      "INFO:root:Get /ml-training-jupyter-notebooks/Example Data Sets/results.csv\n",
      "INFO:werkzeug:15.4.7.102 - - [20/Jan/2022 22:49:37] \"GET /Example%20Data%20Sets/results.csv HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:15.4.7.101 - - [20/Jan/2022 22:49:37] \"GET /Example%20Data%20Sets/results.csv HTTP/1.1\" 200 -\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark-jupyter-mlib-1f85777e79aab58c-exec-1 -> Deleted. Downloaded.\n",
      "spark-jupyter-mlib-1f85777e79aab58c-exec-2 -> Deleted. Downloaded.\n",
      "spark-jupyter-mlib-1f85777e79aab58c-exec-3 -> Deleted. Downloaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 37.857057\n",
      "Time per date: 37.857057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>interval</th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>cluster_indices</th>\n",
       "      <th>cluster_centroids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ASUR</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-02-14</td>\n",
       "      <td>5.89</td>\n",
       "      <td>6.35</td>\n",
       "      <td>5.85</td>\n",
       "      <td>6.02</td>\n",
       "      <td>109600</td>\n",
       "      <td>0</td>\n",
       "      <td>[15.85340587 15.90234198]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CLVS</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-02-14</td>\n",
       "      <td>24.98</td>\n",
       "      <td>25.65</td>\n",
       "      <td>24.66</td>\n",
       "      <td>25.47</td>\n",
       "      <td>1207600</td>\n",
       "      <td>0</td>\n",
       "      <td>[15.85340587 15.90234198]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FNSR</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-02-14</td>\n",
       "      <td>22.98</td>\n",
       "      <td>23.11</td>\n",
       "      <td>22.69</td>\n",
       "      <td>23.00</td>\n",
       "      <td>769500</td>\n",
       "      <td>0</td>\n",
       "      <td>[15.85340587 15.90234198]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LIND</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-02-14</td>\n",
       "      <td>12.74</td>\n",
       "      <td>12.74</td>\n",
       "      <td>12.53</td>\n",
       "      <td>12.58</td>\n",
       "      <td>36900</td>\n",
       "      <td>0</td>\n",
       "      <td>[15.85340587 15.90234198]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MFIN</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-02-14</td>\n",
       "      <td>5.65</td>\n",
       "      <td>5.67</td>\n",
       "      <td>5.56</td>\n",
       "      <td>5.56</td>\n",
       "      <td>13100</td>\n",
       "      <td>0</td>\n",
       "      <td>[15.85340587 15.90234198]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker interval        date   open   high    low  close   volume  cluster_indices          cluster_centroids\n",
       "0   ASUR        D  2019-02-14   5.89   6.35   5.85   6.02   109600                0  [15.85340587 15.90234198]\n",
       "1   CLVS        D  2019-02-14  24.98  25.65  24.66  25.47  1207600                0  [15.85340587 15.90234198]\n",
       "2   FNSR        D  2019-02-14  22.98  23.11  22.69  23.00   769500                0  [15.85340587 15.90234198]\n",
       "3   LIND        D  2019-02-14  12.74  12.74  12.53  12.58    36900                0  [15.85340587 15.90234198]\n",
       "4   MFIN        D  2019-02-14   5.65   5.67   5.56   5.56    13100                0  [15.85340587 15.90234198]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine path to resutls file\n",
    "def get_driver_result_file_path(results_file_name):\n",
    "    project_root_dir  = pyprojroot.here()\n",
    "    result_file_path = os.path.join(project_root_dir, \"Example Data Sets\", results_file_name)\n",
    "    return result_file_path\n",
    "\n",
    "def get_worker_result_file_path(results_file_name):\n",
    "    return \"/{0}\".format(results_file_name)\n",
    "\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "import datetime\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "# Assemble parameters for function\n",
    "import threading\n",
    "k = 5\n",
    "date = '2019-01-01'\n",
    "column_names = [\"open\", \"close\"]\n",
    "lock = threading.Lock()\n",
    "results_file_name = \"results.csv\"\n",
    "results_file_path = get_driver_result_file_path(results_file_name)\n",
    "params = [k, date, column_names, lock, results_file_path]\n",
    "\n",
    "# run function which creates the results file\n",
    "perform_kmeans_on_dataframe__store_results__and_retry(params)\n",
    "\n",
    "# Copy python modules to workers (for updating our data file)\n",
    "spark_helper.symlink_dir_to_root(os.path.join(project_root_dir, \"Utilities\"))\n",
    "spark_helper_url = \"http://{0}:{1}/{2}/{3}\".format(ip_address, web_server_port, \"Utilities\", \"spark_helper.py\")\n",
    "sc.addFile(spark_helper_url) \n",
    "\n",
    "# Symlink the results file for the http file server\n",
    "spark_helper.symlink_dir_to_root(os.path.join(project_root_dir, data_dir_name))\n",
    "\n",
    "# load the results file\n",
    "result_file_url = \"http://{0}:{1}/{2}/{3}\".format(\n",
    "    ip_address, \n",
    "    web_server_port, \n",
    "    urllib.parse.quote(data_dir_name),\n",
    "    urllib.parse.quote(results_file_name))\n",
    "spark_helper.add_file_to_cluster(spark_session, result_file_url)\n",
    "result_df = koalas.read_csv(u\"file://{0}\".format(get_worker_result_file_path(results_file_name)), converters=converter_mapping)\n",
    "result_df.shape\n",
    "\n",
    "# Cleanup the result file\n",
    "os.remove(results_file_path)\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "wall_time = (end_time - start_time).total_seconds()\n",
    "print(\"Wall time: {0}\".format(wall_time))\n",
    "time_per_date = wall_time\n",
    "print(\"Time per date: {0}\".format(time_per_date))\n",
    "\n",
    "# Show results\n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test out this function with our parallelization framework bu running against 5 dates in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run training sessions for each data in parrallel\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bdd343398d248c2b6cb7c29728c34d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/158 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Get /ml-training-jupyter-notebooks/Example Data Sets/results.csv      ]/ 200]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy python modules to workers (for updating our data file)\n",
      "Symlink the results file for the http file server\n",
      "Load the results file\n",
      "Updating file on driver.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:15.4.12.12 - - [21/Jan/2022 00:32:41] \"GET /Example%20Data%20Sets/results.csv HTTP/1.1\" 200 -\n",
      "INFO:root:Get /ml-training-jupyter-notebooks/Example Data Sets/results.csv\n",
      "INFO:root:Get /ml-training-jupyter-notebooks/Example Data Sets/results.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating file on workers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Get /ml-training-jupyter-notebooks/Example Data Sets/results.csv\n",
      "INFO:werkzeug:15.4.7.102 - - [21/Jan/2022 00:32:42] \"GET /Example%20Data%20Sets/results.csv HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:15.4.7.103 - - [21/Jan/2022 00:32:43] \"GET /Example%20Data%20Sets/results.csv HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:15.4.7.101 - - [21/Jan/2022 00:32:43] \"GET /Example%20Data%20Sets/results.csv HTTP/1.1\" 200 -\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark-jupyter-mlib-1f85777e79aab58c-exec-3 -> Deleted. Downloaded.\n",
      "spark-jupyter-mlib-1f85777e79aab58c-exec-1 -> Deleted. Downloaded.\n",
      "spark-jupyter-mlib-1f85777e79aab58c-exec-2 -> Deleted. Downloaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2690.129143\n",
      "Time per date: 17.026133816455697\n",
      "Cleanup the result file\n"
     ]
    }
   ],
   "source": [
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Assemble parameters for function\n",
    "import threading\n",
    "k = 5\n",
    "column_names = [\"open\", \"close\"]\n",
    "lock = threading.Lock()\n",
    "results_file_name = \"results.csv\"\n",
    "results_file_path = get_driver_result_file_path(results_file_name)\n",
    "arg_set = [[[k, date, column_names, lock, results_file_path]] for date in dates]\n",
    "kwarg_set = [{} for date in dates]\n",
    "\n",
    "import datetime\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "print(\"Run training sessions for each data in parrallel\")\n",
    "import parallelization\n",
    "thread_pool = parallelization.EnhancedThreadPool(num_workers=4)\n",
    "results = thread_pool.parallelize(perform_kmeans_on_dataframe__store_results__and_retry, arg_set, kwarg_set)\n",
    "\n",
    "print(\"Copy python modules to workers (for updating our data file)\")\n",
    "spark_helper.symlink_dir_to_root(os.path.join(project_root_dir, \"Utilities\"))\n",
    "spark_helper_url = \"http://{0}:{1}/{2}/{3}\".format(ip_address, web_server_port, \"Utilities\", \"spark_helper.py\")\n",
    "sc.addFile(spark_helper_url) \n",
    "\n",
    "print(\"Symlink the results file for the http file server\")\n",
    "spark_helper.symlink_dir_to_root(os.path.join(project_root_dir, data_dir_name))\n",
    "\n",
    "print(\"Load the results file\")\n",
    "result_file_url = \"http://{0}:{1}/{2}/{3}\".format(\n",
    "    ip_address, \n",
    "    web_server_port, \n",
    "    urllib.parse.quote(data_dir_name),\n",
    "    urllib.parse.quote(results_file_name))\n",
    "spark_helper.add_file_to_cluster(spark_session, result_file_url)\n",
    "result_df = koalas.read_csv(u\"file://{0}\".format(get_worker_result_file_path(results_file_name)), converters=converter_mapping)\n",
    "result_df.shape\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "wall_time = (end_time - start_time).total_seconds()\n",
    "print(\"Wall time: {0}\".format(wall_time))\n",
    "time_per_date = wall_time / len(dates)\n",
    "print(\"Time per date: {0}\".format(time_per_date))\n",
    "\n",
    "print(\"Cleanup the result file\")\n",
    "os.remove(results_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>interval</th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>cluster_indices</th>\n",
       "      <th>cluster_centroids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHKP</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-02-12</td>\n",
       "      <td>116.80</td>\n",
       "      <td>119.26</td>\n",
       "      <td>116.66</td>\n",
       "      <td>119.09</td>\n",
       "      <td>1112900</td>\n",
       "      <td>3</td>\n",
       "      <td>[92.13633654 92.72692308]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EFII</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-02-12</td>\n",
       "      <td>26.42</td>\n",
       "      <td>27.59</td>\n",
       "      <td>25.81</td>\n",
       "      <td>27.52</td>\n",
       "      <td>465100</td>\n",
       "      <td>0</td>\n",
       "      <td>[16.34191932 16.42099128]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FGM</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-02-12</td>\n",
       "      <td>40.62</td>\n",
       "      <td>40.74</td>\n",
       "      <td>40.48</td>\n",
       "      <td>40.69</td>\n",
       "      <td>2500</td>\n",
       "      <td>0</td>\n",
       "      <td>[16.34191932 16.42099128]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FTAG</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-02-12</td>\n",
       "      <td>23.34</td>\n",
       "      <td>23.34</td>\n",
       "      <td>23.34</td>\n",
       "      <td>23.34</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>[16.34191932 16.42099128]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IMGN</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-02-12</td>\n",
       "      <td>5.85</td>\n",
       "      <td>5.86</td>\n",
       "      <td>5.27</td>\n",
       "      <td>5.36</td>\n",
       "      <td>3807500</td>\n",
       "      <td>0</td>\n",
       "      <td>[16.34191932 16.42099128]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker interval        date    open    high     low   close   volume  cluster_indices          cluster_centroids\n",
       "0   CHKP        D  2019-02-12  116.80  119.26  116.66  119.09  1112900                3  [92.13633654 92.72692308]\n",
       "1   EFII        D  2019-02-12   26.42   27.59   25.81   27.52   465100                0  [16.34191932 16.42099128]\n",
       "2    FGM        D  2019-02-12   40.62   40.74   40.48   40.69     2500                0  [16.34191932 16.42099128]\n",
       "3   FTAG        D  2019-02-12   23.34   23.34   23.34   23.34      500                0  [16.34191932 16.42099128]\n",
       "4   IMGN        D  2019-02-12    5.85    5.86    5.27    5.36  3807500                0  [16.34191932 16.42099128]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If anything went wrong, we should ask the SparkContext to kill any abandoned jobs that may be lingering in ghosted threads from the thread pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.cancelAllJobs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Obeserve Results\n",
    "\n",
    "Running things in parallel is much faster! 17 seconds per date rather than 37 seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 6. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing Symlink: /ml-training-jupyter-notebooks/Example Data Sets/Test Scores.csv -> /Test Scores.csv\n",
      "Removing Symlink: /ml-training-jupyter-notebooks/Example Data Sets/nasdaq_2019.csv -> /nasdaq_2019.csv\n",
      "Removing Symlink: /ml-training-jupyter-notebooks/Example Data Sets/.gitignore -> /.gitignore\n",
      "Removing Symlink: /ml-training-jupyter-notebooks/Example Data Sets/.ipynb_checkpoints -> /.ipynb_checkpoints\n",
      "Removing Symlink: /ml-training-jupyter-notebooks/Example Data Sets/demo_data.csv -> /demo_data.csv\n"
     ]
    }
   ],
   "source": [
    "spark_helper.unlink_dir_from_root(data_dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing Symlink: /ml-training-jupyter-notebooks/Utilities/PythonHttpFileServer.py -> /PythonHttpFileServer.py\n",
      "Removing Symlink: /ml-training-jupyter-notebooks/Utilities/utilities.py -> /utilities.py\n",
      "Removing Symlink: /ml-training-jupyter-notebooks/Utilities/Using Progressbars.ipynb -> /Using Progressbars.ipynb\n",
      "Removing Symlink: /ml-training-jupyter-notebooks/Utilities/spark_helper.py -> /spark_helper.py\n",
      "Removing Symlink: /ml-training-jupyter-notebooks/Utilities/__pycache__ -> /__pycache__\n",
      "Removing Symlink: /ml-training-jupyter-notebooks/Utilities/parallelization.py -> /parallelization.py\n",
      "Removing Symlink: /ml-training-jupyter-notebooks/Utilities/Utilities.egg-info -> /Utilities.egg-info\n"
     ]
    }
   ],
   "source": [
    "spark_helper.unlink_dir_from_root(os.path.join(project_root_dir, \"Utilities\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_session.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "! kubectl -n spark get pod"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
