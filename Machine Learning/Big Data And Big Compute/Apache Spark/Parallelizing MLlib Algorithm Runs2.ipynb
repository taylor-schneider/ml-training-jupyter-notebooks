{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "Previously we have seen examples of running the k-means algorithm provided by both scikit-learn and MLlib. In the previous notebook regarding [Running MLlib Algorothms](Running%20MLlib%20Algorithms%20%28k-means%29.ipynb) we learned that the MLlib algorithms are parallelized. This is a very useful feature as it allows us to shorten the amount of time to train/test a model on a large data set.\n",
    "\n",
    "The problem exists however with the Spark implimentation; it does not support nested parallelism.\n",
    "\n",
    "## Spark Doesnt Support Nested Parallelism\n",
    "\n",
    "What we mean to say is that Apache Spark doesn't support any form of nesting in terms of spark managed parallelism. Distributed operations can be initialized only by the driver (ie. not in a worker process created by the driver). Having looked at the source code there are a few reasons for this. The main reason is that Sparks code checks and prevents a SparkContext from being created on a worker. Without access to the SparkContext we do not have access to distributed data structures, like Spark DataFrame, and execution of parallel processing functions, like training an MLlib algorithm.\n",
    "\n",
    "If you tried to have a Spark Executor (worker) create and train an MLlib algorithm you would see the following error pop up:\n",
    "\n",
    "```\n",
    "AttributeError: Cannot load _jvm from SparkContext. Is SparkContext initialized?\n",
    "```\n",
    "\n",
    "This took me a while to figure out, but it is pointed out in this [discussion](https://coderedirect.com/questions/310003/run-ml-algorithm-inside-map-function-in-spark)\n",
    "\n",
    "This is inconvenient as spark provides a number of useful mechanism for kicking off parallel processes based on our dataset. For example, in the previous notebook using scikit-learn models, we use the `.groupby(date).apply(training_function)` functionality to kick off a training job for each group of data in parallel. But unfortunayely, doe to the limitations noted above, we are not able to use this api with mlib. Again, this is because the training  function is executed on the worker where a spark context is not found and cannot be created.\n",
    "\n",
    "### Options For Home Grown Nested Parllalelism\n",
    "\n",
    "Ok, so we cant use a single SparkContext to provide nested parallelism, what are our options? Based on my understanding, I suspect the following two alternatives are possible (we prove one out in this notebook, the other is for a rainy day):\n",
    "1. Have the driver kick off and manage parralel operations using a single SparkContext\n",
    "2. Have the driver kick off and manage parralel operations using multiple SparkContexts pointing to multiple clusters\n",
    "3. Hack the worker to create a SparkContext / Submit a PR to Spark's official Repo\n",
    "\n",
    "**Note**: If you have other suggestions, please open a github issue so we can chat!\n",
    "\n",
    "#### Option 1: Driver Manages Parallel Calls To SparkContext\n",
    "We can do this using either multithreading or multiprocessing. If you are not familiar with these topics, please review the [prepared materials](../../../Programming/Parallelization/README.md)\n",
    "\n",
    "Ok, so what are we building? We will be creating a function which will launch multiple threads, tell them to do work in parallel, collect the results, and assemble the result rest into one big result. Sometimes this is called \"map reduce\" sometimes its called \"divide and conquer\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## There Be Dragons Ahead\n",
    "**Note**: This is a very complex subject and requires engineering skills as well as knowledge of troubleshooting spark. See gotchas section.\n",
    "\n",
    "One could make the case that we are abusing Spark (ie. using it in a way it was not designed). As a systems engineer that's basically my job so Nuts to that. With most distributed systems we will run into problems that we cannot explain or cannot reproduce. While writing this notebook I documented some issues I ran into and their solutions.\n",
    "\n",
    "You might run out of memory when you have multiple jobs competing against eachother. \n",
    "\n",
    "```\n",
    "Py4JJavaError: An error occurred while calling o689322.createOrReplaceTempView.\n",
    ": java.lang.OutOfMemoryError: Java heap space\n",
    "```\n",
    "\n",
    "As such you may need to adjust your sparkConf to allocate more memory to the driver/worker.\n",
    "```python\n",
    "    sparkConf.set(\"spark.executor.instances\", \"3\")\n",
    "    sparkConf.set(\"spark.executor.cores\", \"2\")\n",
    "    sparkConf.set(\"spark.executor.memory\", \"4096m\")\n",
    "    sparkConf.set(\"spark.executor.memoryOverhead\", \"1024m\")\n",
    "    sparkConf.set(\"spark.driver.memory\", \"1024m\")\n",
    "```\n",
    "\n",
    "Thing blowing up may cause other things to blow up unexpectedly. I coded a bug into my solution where i was killing a job before it finished writing data which corrupted the data file it was writing... Don't do this.\n",
    "\n",
    "```\n",
    "Py4JJavaError: An error occurred while calling o436701.csv.\n",
    ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 22489.0 failed 4 times, most recent failure: Lost task 3.3 in stage 22489.0 (TID 311712) (10.42.0.1 executor 2): java.io.EOFException: Cannot seek after EOF\n",
    "```\n",
    "\n",
    "I encountered some other issues and wrapped my function in retry logic just incase (this may not have been needed as the issue might have been unavoidable and also resolved in a later version of code)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "It assumes you have already read the following notebooks:\n",
    "- [Install Apache Spark Prerequisites](Install%20Apache%20Spark%20Prerequisites.ipynb)\n",
    "- [Spark Pi - The Hello World Example For Apache spark](Spark%20Pi%20-%20The%20Hello%20World%20Example%20For%20Apache%20spark.ipynb)\n",
    "- [Intro To Koalas](Intro%20To%20Koalas.ipynb)\n",
    "- [K-Means](../../Algorithms/Unsupervised%20Learning/Cluster%20Analysis/K-Means.ipynb)\n",
    "- [Load CSV Into Apache Spark On Kubernetes](Load%20CSV%20Into%20Apache%20Spark%20On%20Kubernetes.ipynb)\n",
    "\n",
    "The instructions are basically the same as [Running MLlib Algorothms](Running%20MLlib%20Algorithms%20%28k-means%29.ipynb) with the added framework I mentioned.\n",
    "\n",
    "## Adjenda\n",
    "1. Create SparkContext\n",
    "2. Create Web Server To Host Data\n",
    "3. Load The Data\n",
    "4. Develop Parallelization Framework\n",
    "5. Test Framework Performance\n",
    "8. Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Create SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ml-training-jupyter-notebooks\n"
     ]
    }
   ],
   "source": [
    "import pyprojroot\n",
    "project_root_dir  = pyprojroot.here()\n",
    "print(project_root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting SPARK_HOME\n",
      "/usr/lib/spark-3.1.1-bin-hadoop2.7\n",
      "\n",
      "Running findspark.init() function\n",
      "['/usr/lib/spark-3.1.1-bin-hadoop2.7/python', '/usr/lib/spark-3.1.1-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip', '/ml-training-jupyter-notebooks/Machine Learning/Big Data And Big Compute/Apache Spark', '/usr/local/lib/python39.zip', '/usr/local/lib/python3.9', '/usr/local/lib/python3.9/lib-dynload', '', '/usr/local/lib/python3.9/site-packages', '/ml-training-jupyter-notebooks/Utilities']\n",
      "\n",
      "Setting PYSPARK_PYTHON\n",
      "/usr/local/bin/python3\n",
      "\n",
      "Configuring URL for kubernetes master\n",
      "k8s://https://15.4.7.11:6443\n",
      "\n",
      "Determining IP Of Server\n",
      "The ip was detected as: 15.4.12.12\n",
      "\n",
      "Creating SparkConf Object\n",
      "('spark.master', 'k8s://https://15.4.7.11:6443')\n",
      "('spark.app.name', 'spark-jupyter-mlib')\n",
      "('spark.submit.deploy.mode', 'cluster')\n",
      "('spark.kubernetes.container.image', 'tschneider/apache-spark-k8:v7')\n",
      "('spark.kubernetes.namespace', 'spark')\n",
      "('spark.kubernetes.pyspark.pythonVersion', '3')\n",
      "('spark.kubernetes.authenticate.driver.serviceAccountName', 'spark-sa')\n",
      "('spark.kubernetes.authenticate.serviceAccountName', 'spark-sa')\n",
      "('spark.executor.instances', '3')\n",
      "('spark.executor.cores', '2')\n",
      "('spark.executor.memory', '4096m')\n",
      "('spark.executor.memoryOverhead', '1024m')\n",
      "('spark.driver.memory', '1024m')\n",
      "('spark.driver.host', '15.4.12.12')\n",
      "('spark.files.overwrite', 'true')\n",
      "('spark.files.useFetchCache', 'false')\n",
      "\n",
      "Creating SparkSession Object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/17 21:50:43 WARN Utils: Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 15.4.12.12 instead (on interface eth0)\n",
      "22/01/17 21:50:43 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/lib/spark-3.1.1-bin-hadoop2.7/jars/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/01/17 21:50:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "spark_app_name = \"spark-jupyter-mlib\"\n",
    "docker_image = \"tschneider/apache-spark-k8:v7\"\n",
    "k8_master_ip = \"15.4.7.11\"\n",
    "\n",
    "import spark_helper\n",
    "spark_session = spark_helper.create_spark_session(spark_app_name, docker_image, k8_master_ip)\n",
    "sc = spark_session.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                         READY     STATUS    RESTARTS   AGE\n",
      "spark-jupyter-mlib-169d3f7e6a05714e-exec-1   1/1       Running   0          23s\n",
      "spark-jupyter-mlib-169d3f7e6a05714e-exec-2   1/1       Running   0          22s\n",
      "spark-jupyter-mlib-169d3f7e6a05714e-exec-3   1/1       Running   0          21s\n"
     ]
    }
   ],
   "source": [
    "! kubectl -n spark get pods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Setup Datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data_dir_name = \"Example Data Sets\"\n",
    "data_dir_path = os.path.join(project_root_dir, data_dir_name)\n",
    "spark_helper.link_data_dir_to_root(data_dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our custom module for the web server we wrote\n",
    "import PythonHttpFileServer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Web root and data file exist!\n",
      "web root: /ml-training-jupyter-notebooks/Example Data Sets\n",
      "data file: /ml-training-jupyter-notebooks/Example Data Sets/nasdaq_2019.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "data_dir_name = \"Example Data Sets\"\n",
    "web_root = os.path.join(project_root_dir, data_dir_name)\n",
    "\n",
    "if not os.path.exists(web_root):\n",
    "    raise Exception(\"The web root for the server does not exist.\")\n",
    "\n",
    "csv_file_name = \"nasdaq_2019.csv\"\n",
    "csv_file_path = os.path.join(web_root, csv_file_name)\n",
    "\n",
    "if not os.path.exists(csv_file_path):\n",
    "    raise Exception(\"The data file does not exist.\")\n",
    "    \n",
    "print(\"Web root and data file exist!\")\n",
    "print(\"web root: {0}\".format(web_root))\n",
    "print(\"data file: {0}\".format(csv_file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Starting server on port 80\n",
      "INFO:root:Web root specified as: /ml-training-jupyter-notebooks/Example Data Sets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app 'PythonHttpFileServer' (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:werkzeug: * Running on all addresses.\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "INFO:werkzeug: * Running on http://15.4.12.12:80/ (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "# Import the library\n",
    "import threading\n",
    "\n",
    "# Configure the logger and log level (incase we need/want to debug)\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Create and start the thread if it doesnt exist\n",
    "var_exists = 'web_server_thread' in locals() or 'web_server_thread' in globals()\n",
    "if not var_exists:\n",
    "    web_server_port = 80\n",
    "    web_server_args = (web_server_port, web_root)\n",
    "    web_server_thread = threading.Thread(target=PythonHttpFileServer.run_server, args=web_server_args)\n",
    "    web_server_thread.start()\n",
    "else:\n",
    "    print(\"Web Server thread already exists\")\n",
    "    print(\"To kill it you need to restart the kernel.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Load The Data \n",
    "To help debug/test our framework we will load the data and build a few utilities targeting pices of the data. Once everything works we will run everything in parallel. Again, this step is just for Testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Add File To Spark Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Get /ml-training-jupyter-notebooks/Example Data Sets/nasdaq_2019.csv\n",
      "INFO:werkzeug:15.4.12.12 - - [17/Jan/2022 21:51:35] \"GET /nasdaq_2019.csv HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading file 'http://15.4.12.12:80/nasdaq_2019.csv' to Spark cluster.\n"
     ]
    }
   ],
   "source": [
    "ip_address = spark_helper.determine_ip_address()\n",
    "csv_file_name = \"nasdaq_2019.csv\"\n",
    "csv_file_url = \"http://{0}:{1}/{2}\".format(ip_address, web_server_port, csv_file_name)\n",
    "print(\"Uploading file '{0}' to Spark cluster.\".format(csv_file_url))\n",
    "sc.addFile(csv_file_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Use Koalas To Load Data File Into DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the utility function to convert a date string to a datetime object from our utilities module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the utilities module we wrote\n",
    "import utilities\n",
    "\n",
    "# Define a mapping to convert our data field to the correct type\n",
    "converter_mapping = {\n",
    "    \"date\": utilities.convert_date_string_to_date\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Load our OHCLV data Into a koalas dataframe and pull out a single day in the say way we would in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Get /ml-training-jupyter-notebooks/Example Data Sets/nasdaq_2019.csv\n",
      "INFO:werkzeug:15.4.7.102 - - [17/Jan/2022 21:51:47] \"GET /nasdaq_2019.csv HTTP/1.1\" 200 -\n",
      "INFO:root:Get /ml-training-jupyter-notebooks/Example Data Sets/nasdaq_2019.csv  \n",
      "INFO:werkzeug:15.4.7.103 - - [17/Jan/2022 21:51:51] \"GET /nasdaq_2019.csv HTTP/1.1\" 200 -\n",
      "INFO:root:Get /ml-training-jupyter-notebooks/Example Data Sets/nasdaq_2019.csv6]\n",
      "INFO:werkzeug:15.4.7.101 - - [17/Jan/2022 21:51:51] \"GET /nasdaq_2019.csv HTTP/1.1\" 200 -\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Avoid a warning\n",
    "import os\n",
    "os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\n",
    "os.environ[\"SPARK_KOALAS_AUTOPATCH\"] = \"0\"\n",
    "\n",
    "from databricks import koalas\n",
    "koalas_dataframe = koalas.read_csv(u\"file:////nasdaq_2019.csv\", converters=converter_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/17 21:52:05 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/01/17 21:52:13 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>interval</th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AABA</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>70.90</td>\n",
       "      <td>71.5200</td>\n",
       "      <td>70.3250</td>\n",
       "      <td>70.57</td>\n",
       "      <td>10234800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAL</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>33.14</td>\n",
       "      <td>33.6632</td>\n",
       "      <td>32.5301</td>\n",
       "      <td>32.88</td>\n",
       "      <td>8995100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAME</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>2.43</td>\n",
       "      <td>2.4300</td>\n",
       "      <td>2.4000</td>\n",
       "      <td>2.40</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAOI</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>10.70</td>\n",
       "      <td>10.8900</td>\n",
       "      <td>10.0100</td>\n",
       "      <td>10.18</td>\n",
       "      <td>883100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAON</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>50.57</td>\n",
       "      <td>50.9850</td>\n",
       "      <td>48.5600</td>\n",
       "      <td>49.73</td>\n",
       "      <td>180200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker interval        date   open     high      low  close    volume\n",
       "0   AABA        D  2019-07-01  70.90  71.5200  70.3250  70.57  10234800\n",
       "1    AAL        D  2019-07-01  33.14  33.6632  32.5301  32.88   8995100\n",
       "2   AAME        D  2019-07-01   2.43   2.4300   2.4000   2.40       500\n",
       "3   AAOI        D  2019-07-01  10.70  10.8900  10.0100  10.18    883100\n",
       "4   AAON        D  2019-07-01  50.57  50.9850  48.5600  49.73    180200"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "koalas_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Develop Parralelization Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our objective is to apply the kmeans algorithm from Spark MLlib to each date in our koalas_dataframe object. Because the data contain mutually exclusive data, we can parallelize accross this variable.\n",
    "\n",
    "To do this, we leverage a parallelization utility we wrote previously. We can do this using either multithreading or multiprocessing. If you are not familiar with these topics, please review the [prepared materials](../../../Programming/Parallelization/README.md).\n",
    "\n",
    "We will talk more about this in a second. For the moment, we need to write the function which we will parallelize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Create Utility Function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then write and test our function which performs kmeans on a specific date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import pyspark\n",
    "from pyspark.sql.functions import pandas_udf, udf\n",
    "from pyspark.sql.types import *\n",
    "from databricks import koalas\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "koalas.set_option(\"compute.ops_on_diff_frames\", True)\n",
    "\n",
    "def perform_kmeans_on_dataframe(date, column_names):\n",
    "    \n",
    "    # Load the data and filter for the data of interest\n",
    "    tmp = koalas.read_csv(u\"file:////nasdaq_2019.csv\", converters=converter_mapping)\n",
    "    tmp = tmp.loc[tmp[\"date\"] == date]\n",
    "    \n",
    "    # Create our model\n",
    "    model = KMeans().setK(5).setSeed(42)\n",
    "\n",
    "    # Do some magic to get the data in the right format for the spark model\n",
    "    from pyspark.ml.feature import VectorAssembler\n",
    "    assembler = VectorAssembler(inputCols=column_names, outputCol=\"features\")\n",
    "    if type(tmp) == koalas.frame.DataFrame:\n",
    "        model_parameters = assembler.transform(tmp[[*column_names]].to_spark())\n",
    "    elif type(tmp) == pandas.DataFrame:\n",
    "        tmp = koalas.DataFrame(tmp)\n",
    "        model_parameters = assembler.transform(tmp[[*column_names]].to_spark())\n",
    "\n",
    "    # Train the model\n",
    "    trained_model = model.fit(model_parameters)\n",
    "    \n",
    "    # Extract the cluster information for the training data\n",
    "    predictions = trained_model.transform(model_parameters)\n",
    "    cluster_indices = predictions.select(\"prediction\")\n",
    "    cluster_indices = koalas.DataFrame(cluster_indices).to_numpy().reshape(-1)\n",
    "    cluster_indices = koalas.Series(cluster_indices, index=tmp.index.to_numpy())\n",
    "    tmp[\"cluster_indices\"] = cluster_indices\n",
    "    cluster_centroids = trained_model.clusterCenters()\n",
    "    tmp[\"cluster_centroids\"] = tmp[\"cluster_indices\"].apply(lambda i: str(cluster_centroids[i]))\n",
    "\n",
    "    return tmp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/17 21:53:06 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "22/01/17 21:53:06 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "22/01/17 21:53:19 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/01/17 21:53:21 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/01/17 21:53:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "/usr/local/lib/python3.9/site-packages/pyspark/sql/pandas/functions.py:389: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n",
      "22/01/17 21:53:34 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/01/17 21:53:40 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>interval</th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>cluster_indices</th>\n",
       "      <th>cluster_centroids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>96829</th>\n",
       "      <td>ACLS</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>17.48</td>\n",
       "      <td>18.14</td>\n",
       "      <td>16.920</td>\n",
       "      <td>17.79</td>\n",
       "      <td>284200</td>\n",
       "      <td>0</td>\n",
       "      <td>[12.97707127 13.2648608 ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96923</th>\n",
       "      <td>ALLK</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>51.26</td>\n",
       "      <td>53.94</td>\n",
       "      <td>50.115</td>\n",
       "      <td>51.99</td>\n",
       "      <td>151900</td>\n",
       "      <td>3</td>\n",
       "      <td>[66.74787778 67.59067778]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97216</th>\n",
       "      <td>BRPAU</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>10.55</td>\n",
       "      <td>10.55</td>\n",
       "      <td>10.550</td>\n",
       "      <td>10.55</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[12.97707127 13.2648608 ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97658</th>\n",
       "      <td>DWFI</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>22.21</td>\n",
       "      <td>22.24</td>\n",
       "      <td>22.200</td>\n",
       "      <td>22.20</td>\n",
       "      <td>41300</td>\n",
       "      <td>0</td>\n",
       "      <td>[12.97707127 13.2648608 ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97699</th>\n",
       "      <td>EFAS</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>15.08</td>\n",
       "      <td>15.08</td>\n",
       "      <td>15.080</td>\n",
       "      <td>15.08</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[12.97707127 13.2648608 ]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ticker interval        date   open   high     low  close  volume  cluster_indices          cluster_centroids\n",
       "96829   ACLS        D  2019-01-02  17.48  18.14  16.920  17.79  284200                0  [12.97707127 13.2648608 ]\n",
       "96923   ALLK        D  2019-01-02  51.26  53.94  50.115  51.99  151900                3  [66.74787778 67.59067778]\n",
       "97216  BRPAU        D  2019-01-02  10.55  10.55  10.550  10.55       0                0  [12.97707127 13.2648608 ]\n",
       "97658   DWFI        D  2019-01-02  22.21  22.24  22.200  22.20   41300                0  [12.97707127 13.2648608 ]\n",
       "97699   EFAS        D  2019-01-02  15.08  15.08  15.080  15.08       0                0  [12.97707127 13.2648608 ]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perform_kmeans_on_dataframe('2019-01-02', column_names=[\"open\", \"close\"]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: The warning above is coming from code used in the internals of Koalas. Do not worry about this warning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Prepare Parallelization Inputs\n",
    "Now that the utility function has been tested on smaller dataframes, we are safe to run it on a large data set in parrallel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/17 21:35:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2019-01-01'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a list of the dates in our dataframe\n",
    "dates = koalas_dataframe[\"date\"].unique().sort_values().to_numpy()\n",
    "dates[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the dataframe which is no longer needed and free up spark resources\n",
    "if 'koalas_dataframe' in locals() or 'koalas_dataframe' in globals():\n",
    "    del koalas_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to build some utilities to facilitate the parralization of these opersations. As we mentioned in the Gotchas section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Create Parallelization Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We previousle wrote a function which will perform kmeans on a data. Now we need to go a step further and record the results to an aggregate output file. This output file will later be loaded to show the results for all dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Define a function to perform kmeans and store the results in a local data file\n",
    "\n",
    "def parallel_function(params):\n",
    "    \n",
    "    # Retrieve params\n",
    "    date = params[0]\n",
    "    lock = params[1]\n",
    "    result_file_path = params[2]\n",
    "    \n",
    "    # Do the computation\n",
    "    result_df = perform_kmeans_on_dataframe(\n",
    "        date, \n",
    "        column_names=[\"open\", \"close\"])\n",
    "    \n",
    "    # Force spark to not be lazy and to do the computation\n",
    "    result_df.shape  \n",
    "    \n",
    "    # Record the results to a local data file\n",
    "    with lock:\n",
    "        if os.path.exists(result_file_path):                  \n",
    "            result_df.to_pandas().to_csv(result_file_path, mode='a', index=False)\n",
    "        else:\n",
    "            result_df.to_pandas().to_csv(result_file_path, mode='a', index=False, header=False)\n",
    "\n",
    "    # Return results and timing info to the ThreadHelper\n",
    "    return result_df\n",
    "\n",
    "# Write a function to retry the code if an error is encountered.\n",
    "# We may enconuter some weird transient errors when runnong on spark\n",
    "# As I mentioned earlier \"there be dragons\"\n",
    "# So we have a simply retry wrapper to handle any weird execptions that \n",
    "# can be resolved by rerunning the same code\n",
    "\n",
    "def retry_wrapper(params, max_retries=5):\n",
    "    retries_remaining = max_retries\n",
    "    while True:\n",
    "        try:\n",
    "            return parallel_function(params)\n",
    "        except Exception as e:\n",
    "            retries_remaining -= 1\n",
    "            if retries_remaining > 0:\n",
    "                print(\"Error occurred with parameter set: {0}. {1} retries remaining\".format(params, max_retires - retries_remaining))\n",
    "                time.sleep(1)\n",
    "            else:\n",
    "                raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test that this works with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/17 22:23:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/01/17 22:23:54 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/01/17 22:23:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/01/17 22:23:59 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/01/17 22:24:05 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                \r"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'os' has no attribute 'delete'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [29]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m result_df\u001b[38;5;241m.\u001b[39mhead()\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Cleanup\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdelete\u001b[49m(result_file_path)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'os' has no attribute 'delete'"
     ]
    }
   ],
   "source": [
    "# Determine path to resutls file\n",
    "def get_result_file_path():\n",
    "    project_root_dir  = pyprojroot.here()\n",
    "    result_file_name = \"results.csv\"\n",
    "    result_file_path = os.path.join(project_root_dir, \"Example Data Sets\", result_file_name)\n",
    "    return result_file_path\n",
    "\n",
    "# Assemble parameters for function\n",
    "import threading\n",
    "date = '2019-01-01'\n",
    "lock = threading.Lock()\n",
    "result_file_path = get_result_file_path()\n",
    "params = [date, lock, result_file_path]\n",
    "\n",
    "# run function\n",
    "result_df = retry_wrapper(params)\n",
    "result_df.head()\n",
    "\n",
    "# Cleanup\n",
    "os.r(result_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function to kick things off in parrallel and returns the results. The trick here is that the results file is going to be stored in our example datasets folder and served to the workers via our web server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a ThreadPool and kick off the parrallel training sessions\n",
    "import os\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "import threading\n",
    "import parallelization\n",
    "\n",
    "def run_multithreaded_kmeans(dates):\n",
    "    try:\n",
    "        print(\"Create vars to help with synchronization and parallelization\")\n",
    "        mutex = threading.Lock()\n",
    "        num_threads = 10\n",
    "        thread_pool = parallelization.EnhancedThreadPool(num_threads)\n",
    "            \n",
    "        print(\"Create result file to store results from threads\")\n",
    "        project_root_dir  = pyprojroot.here()\n",
    "        result_file_name = \"results.csv\"\n",
    "        result_file_path = os.path.join(project_root_dir, \"Example Data Sets\", result_file_name)\n",
    "        do_work = False\n",
    "        if not os.path.exists(result_file_path):          \n",
    "            with open(result_file_path, 'w') as fp:\n",
    "                pass\n",
    "            do_work = True\n",
    "        else:\n",
    "            print(\"No work to do, results file already exists!\")\n",
    "        \n",
    "        print(\"Setting up local datastore for driver\")\n",
    "        data_dir_name = \"Example Data Sets\"\n",
    "        data_dir_path = os.path.join(project_root_dir, data_dir_name)\n",
    "        spark_helper.link_data_dir_to_root(data_dir_path)\n",
    "\n",
    "        if do_work:\n",
    "            print(\"No result file exists, doing work...\")\n",
    "            start = datetime.now()\n",
    "            print(\"Starting: {0}\".format(start))\n",
    "\n",
    "            print(\"Create an iterator of params for the thread function\")\n",
    "            num_ops = len(dates)\n",
    "            input_file_name = \"nasdaq_2019.csv\"\n",
    "            input_file_path = \"file:///{0}\".format(input_file_name)\n",
    "        \n",
    "            iterator = zip(dates, \n",
    "                           itertools.repeat(bar), \n",
    "                           itertools.repeat(mutex),\n",
    "                           itertools.repeat(input_file_path),\n",
    "                           itertools.repeat(result_file_path))\n",
    "            \n",
    "            print(\"Adding data file to cluster\")\n",
    "            sc.addFile(csv_file_url)\n",
    "            \n",
    "            print(\"Run training sessions for each data in parrallel\")\n",
    "            results = thread_pool.parallelize(thread_wrapper__func, iterator)\n",
    "\n",
    "            # Record the end time\n",
    "            calc_end = datetime.now()\n",
    "            calc_diff = (calc_end - start).total_seconds()\n",
    "            print(\"Ending: {0}\".format(calc_end))\n",
    "            print(\"Total calculation time: {0}s\".format(calc_diff))\n",
    "            date_diff = calc_diff / num_ops\n",
    "            print(\"Time per date: {0}s\".format(date_diff))\n",
    "\n",
    "        print(\"Load results file\")\n",
    "        ip_address = spark_helper.determine_ip_address()\n",
    "        web_server_port = 80\n",
    "        result_file_url = \"http://{0}:{1}/{2}\".format(ip_address, web_server_port, result_file_name)\n",
    "        worker_result_file_path = \"file:///{0}\".format(result_file_name)\n",
    "        spark_helper.link_data_dir_to_root(data_dir_path)\n",
    "        try:\n",
    "            mutex.acquire()\n",
    "            sc.addFile(result_file_url)\n",
    "            merged_df = koalas.read_csv(worker_result_file_path, converters=converter_mapping)\n",
    "        finally:\n",
    "            mutex.release()\n",
    "        \n",
    "        return merged_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Cleanup spark\n",
    "        sc.cancelAllJobs()\n",
    "        # Raise error\n",
    "        raise e\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create vars to help with synchronization and parallelization\n",
      "Create result file to store results from threads\n",
      "Setting up local datastore for driver\n",
      "Creating Symlink: /ml-training-jupyter-notebooks/Example Data Sets/results.csv -> /results.csv\n",
      "No result file exists, doing work...\n",
      "Starting: 2022-01-17 21:38:05.092046\n",
      "Create objects to help track multithreading progress\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'create_progress_bar' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [28]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Run the calculation\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m merged_df \u001b[38;5;241m=\u001b[39m \u001b[43mrun_multithreaded_kmeans\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdates\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [27]\u001b[0m, in \u001b[0;36mrun_multithreaded_kmeans\u001b[0;34m(dates)\u001b[0m\n\u001b[1;32m     86\u001b[0m sc\u001b[38;5;241m.\u001b[39mcancelAllJobs()\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# Raise error\u001b[39;00m\n\u001b[0;32m---> 88\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "Input \u001b[0;32mIn [27]\u001b[0m, in \u001b[0;36mrun_multithreaded_kmeans\u001b[0;34m(dates)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreate objects to help track multithreading progress\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     38\u001b[0m num_ops \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dates)\n\u001b[0;32m---> 39\u001b[0m bar \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_progress_bar\u001b[49m(num_ops)\n\u001b[1;32m     40\u001b[0m bar\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m completed_ops\n",
      "\u001b[0;31mNameError\u001b[0m: name 'create_progress_bar' is not defined"
     ]
    }
   ],
   "source": [
    "# Run the calculation\n",
    "merged_df = run_multithreaded_kmeans(dates[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the union df\n",
    "merged_df.loc[merged_df[\"date\"] == dates[0]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If anything went wrong, we should ask the SparkContext to kill any abandoned jobs that may be lingering in ghosted threads from the thread pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.cancelAllJobs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Test The Framework\n",
    "Now we can run a large set of dates using our threadpool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_session.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_session = spark_helper.create_spark_session(spark_app_name, docker_image, k8_master_ip)\n",
    "sc = spark_session.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_file_path = os.path.join(data_dir_path, \"results.csv\")\n",
    "os.remove(results_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "koalas.set_option(\"compute.ops_on_diff_frames\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_df = run_multithreaded_kmeans(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.loc[merged_df[\"date\"] == dates[7]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If anything went wrong, we should ask the SparkContext to kill any abandoned jobs that may be lingering in ghosted threads from the thread pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.cancelAllJobs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that computing every date is almost as fast as computing a single date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Compare Results\n",
    "\n",
    "We timed ourselves when running against a single day, a few days, and all days to show the operations are occurring in parrallel.\n",
    "\n",
    "At first glance, it looks like the actual per date computation got faster. This is a bit strange. My only guess is that loading the data file was now spread accross multiple dates instead of one in the calculation.\n",
    "\n",
    "97 seconds / 4 dates = 24.25 secondsper date\n",
    "2605 seconds / 158 dates = 16.5 seconds per date\n",
    "\n",
    "The important thing here is that we could scale our cluster out more and more nodes and the comutation should decrease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 6. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_helper.unlink_data_dir_from_root(data_dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_session.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! kubectl -n spark get pod"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
