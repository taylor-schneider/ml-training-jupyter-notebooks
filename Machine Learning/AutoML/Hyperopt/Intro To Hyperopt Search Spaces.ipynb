{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ec1342b-86a1-44b2-a1e8-877328153d06",
   "metadata": {},
   "source": [
    "# Overview\n",
    "In this notebook we examine search spaces as defined in the hyperopt library. At this point I am using the latest package versioned 0.2.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa631d93-1437-484d-accf-7d1279b59088",
   "metadata": {},
   "outputs": [],
   "source": [
    "## conditional things? c1?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50f7651-3a6e-42ec-91bc-d72c0f00f856",
   "metadata": {},
   "source": [
    "# 1. Search Spaces\n",
    "As the name suggests, a [Search Space](https://hyperopt.github.io/hyperopt/getting-started/search_spaces/) is a multi-dimensional space through which we will search. The objective of the search is to find an n-dimensional point which yields the optimal result. The optimal result is objectively quantified by an objective function (loss function). In other words, the objective functrion will accept an n-dimensional point and return a numeric representation of the \"goodness\" of the point. \n",
    "\n",
    "The hyperopt framework provides the mechanism to define the search space and the algorithms to conduct the search. The user must define the objective function and the coresponding search space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73641078-04db-45df-b8ff-3d8564575229",
   "metadata": {},
   "source": [
    "## 1.1 The basic problem\n",
    "I struggled with the notion of a search space at first. It is an abstract concept and the official documentation is not as robust as I would have liked being new to the project/terminology. Additionally there are many examples out there tailored to many use cases which add to the difficulty of making inferences about the library's functionality.\n",
    "\n",
    "Again, the search space is generic and polymorphic. Depending on the problem we are trying to solve the physcial characteristics of the search space (schema/topography) will change. To simplify the presentation of the material we will focus on a single problem: Choosing the best machine learning algorithm given a set of input data. As we progress through this notebook we will examine the different representations of the search space that are possible.\n",
    "\n",
    "Before putting our hands on the code, lets take a moment to think about the the problem philosophically. The first, and most obvious, question to answer (variable in our search space) is \"what model will we use\". Imagine we have a choice between \"model a\" and \"model b\". It is very likely that these models are significatnly different and require different hyperparameters. This would be our second question to answer: \"what hyperparameters will we select for the selected model\". We might be interested in exploring different search methods. We might be interested in comparing different performance metrics.\n",
    "\n",
    "With that being said, we now have a number of variables to think about... but how do we describe this space so hyperopt can search over it? And how do we conduct a search through the defined space?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9878818c-d4a0-4eef-86bf-d5aa4a92ad71",
   "metadata": {},
   "source": [
    "## 1.2. The basic syntax to define a search space\n",
    "As mentioned earlier, there are a number of ways to skin a cat in terms of defining a search space. As we will see everything is going to boil down to an expression of some sort which defines the domain of a variable. \n",
    "\n",
    "```\n",
    "space = expression(label, parameters)\n",
    "```\n",
    "\n",
    "Hyperopt has a number of built in expression types. It also has the ability extend the builtin expressions with custom pyll functions which wee will see later.\n",
    "\n",
    "A point of note here: The label must be globally unique or you may see a *DuplicateLabel* Exception get raised. The label helps the hyperopt framework define a graph like structure through which the search is conducted. If the labels were not uniquely identified, the framework woudl not be able to understand what parameters it is looking for. This will become more clear as we look at the complex nexted examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff693102-a1b9-49cb-93dd-6cb17a9ecbee",
   "metadata": {},
   "source": [
    "## 1.3. The objective function\n",
    "As stated earlier, the objective function is responsible for evaluating the \"goodness\" or \"performance\" of a model or function when it is provided with a particular parameter set. The evaluation is provided as a globally comparable metric or measurement (some type of number).\n",
    "\n",
    "```\n",
    "metric = objective(parameter_set)\n",
    "```\n",
    "\n",
    "Defining this measure is outside the scope of this notebook. At a high level, the function might be defined as:\n",
    "\n",
    "```\n",
    "def objective(parameters_set):\n",
    "\n",
    "    // Create a machine learning model\n",
    "    // Train the model\n",
    "    // Test the model\n",
    "    // return test results metric\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "For educational purposed we will instead default to a trivial measure. Our objective function will return a number representing the sum of the sample parameters it was provided from the search space. For example, consider the following:\n",
    "\n",
    "```\n",
    "metric = objective(0)\n",
    "# metric == 0\n",
    "\n",
    "metric = objective(1,2,3)\n",
    "# metric == 6\n",
    "\n",
    "```\n",
    "\n",
    "As stated earlier, defining the objective function depends on the search space. If the schema of the search space changes, so must the objective function. As we go through our examples, we will see the objective function's definition change to fit our search space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccb3196-23e6-483e-89c0-991ab20609c3",
   "metadata": {},
   "source": [
    "## 1.4. How to perform a search?\n",
    "We will cover this topic in greater detail in another notebook. The short answer is that hyperopt provides a function *fmin()*. This function accepts a search space and search algorithm as a parameter and orchestrates the search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86df9a4f-f40d-447d-b94e-f6a9bcb70c3a",
   "metadata": {},
   "source": [
    "# 2. Parameter Expressions\n",
    "Parameter expressions are the backbone of defining search spaces. There are two types of parameter expressions which we can leverage:\n",
    "- Stochastic Expressions - Define the domain of a stoachastic random variable\n",
    "- Pyll Expressions - Define a deterministic (non-stochastic) variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a57c96f-d502-453f-b3e6-397db8800d4b",
   "metadata": {},
   "source": [
    "## 2.1. Stochastic Parameter Expressions\n",
    "Stochastic expressions allow us to define random variables according to a commpon parameterized probability distribution.\n",
    "\n",
    "Currently the following are supported:\n",
    "- choice\n",
    "- pchoice\n",
    "- randint\n",
    "- uniform\n",
    "- quniform\n",
    "- quniformint\n",
    "- loguniform\n",
    "- qloguniform\n",
    "- normal\n",
    "- qnormal\n",
    "- lognormal\n",
    "- qlognormal\n",
    "\n",
    "All of these parameter expressions are found in the hyperopt.hp module. After looking through the code and making some inferences, I believe that hp stands for hyper paremter. Thus these expressions are hyperparameter expressions.\n",
    "\n",
    "We will take a closer look at these individually in the next few sections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642885e2-e98b-4bb1-897b-14c2f7558125",
   "metadata": {},
   "source": [
    "### 2.1.1. The Choice Expression\n",
    "With the choice expression, we can define a choice as a set of posisble outcomes with no specific distribution attached to it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11c89d7-60ab-41d0-8995-53dbc5a0e0c3",
   "metadata": {},
   "source": [
    "#### 2.1.1.1 A Univariate Choice\n",
    "Below we can define a seach space as a binary choice. In this simple case we are choosing between two models (generically named \"model a\" and \"model b\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef9c60f1-49c2-42c5-b214-04e892db8c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hyperopt\n",
    "\n",
    "space = hyperopt.hp.choice('my_choice', [\n",
    "    {'name': 'model a'},\n",
    "    {'name': 'model b'}\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b494ddcf-1be5-4f32-84b0-c631c162c27b",
   "metadata": {},
   "source": [
    "We can use the hyperopt framework to sable from this search space and examine the arguments that would be provided to our objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da1978dc-4bce-4dca-8828-f527a9e69cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'model a'}\n",
      "{'name': 'model b'}\n",
      "{'name': 'model a'}\n"
     ]
    }
   ],
   "source": [
    "print(hyperopt.pyll.stochastic.sample(space))\n",
    "print(hyperopt.pyll.stochastic.sample(space))\n",
    "print(hyperopt.pyll.stochastic.sample(space))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335db2e1-0ebb-4bea-be73-db94312f35b9",
   "metadata": {},
   "source": [
    "We can define a loss function and search through our parameter space for the optimal value using the hyperopt framework:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6941b1ec-4b0b-483f-ade2-8066cc1da49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'model b', 'x': 2, 'y': 3}                                            \n",
      "{'name': 'model a', 'x': 6}                                                    \n",
      "{'name': 'model a', 'x': 1}                                                    \n",
      "{'name': 'model a', 'x': 4}                                                    \n",
      "{'name': 'model b', 'x': 3, 'y': 3}                                            \n",
      "{'name': 'model b', 'x': 1, 'y': 5}                                            \n",
      "{'name': 'model a', 'x': 8}                                                    \n",
      "{'name': 'model a', 'x': 6}                                                    \n",
      "{'name': 'model a', 'x': 8}                                                    \n",
      "{'name': 'model a', 'x': 2}                                                    \n",
      "100%|███████████████████████| 10/10 [00:00<00:00, 79.36trial/s, best loss: 0.0]\n",
      "=========================\n",
      "Optimal args index:\n",
      "{'model_a_x': 3, 'my_choice': 0}\n",
      "Best hyperparameters:\n",
      "{'name': 'model a', 'x': 6}\n"
     ]
    }
   ],
   "source": [
    "import hyperopt\n",
    "import numpy\n",
    "\n",
    "# Define the objective function\n",
    "def objective(args):\n",
    "    print(args)\n",
    "    if args[\"name\"] == \"model a\":\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "# Define an object to keep track of the \"trials\" in the search path\n",
    "trials = hyperopt.Trials()\n",
    "    \n",
    "# Optimize the search space and retrieve the index which points to the best points in the search space\n",
    "optimal_args_index = hyperopt.fmin(objective, space, algo=hyperopt.tpe.suggest, max_evals=10, trials=trials, rstate= numpy.random.RandomState(42))\n",
    "    \n",
    "# Retrieve the resulting hyperparameter set from the search space using the index\n",
    "optimal_hyperparams = hyperopt.space_eval(space, optimal_args_index)\n",
    "\n",
    "# Print the results\n",
    "print(\"=========================\")\n",
    "print(\"Optimal args index:\")\n",
    "print(optimal_args_index)\n",
    "print(\"Best hyperparameters:\")\n",
    "print(optimal_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7406ea6e-7161-4148-a5d9-206ae8e44b73",
   "metadata": {},
   "source": [
    "We can see that \"model a\" was selected as it yields the minimal results from the objective function.\n",
    "\n",
    "**Note:** We see that the search algorithm has a lot of repetitions... This is because it the only boundary on the search is the max_evals parameter. We will look at optimizing the search algorithm later on. For example when we utilize the loss_threshold to allow for early termination."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b12d795-dd3f-48ba-8369-8f590a04f55f",
   "metadata": {},
   "source": [
    "Having a look at the trials opject we inspect it's type and the useful properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05dbbebf-0999-468a-8b58-cc99f1ae33bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<hyperopt.base.Trials at 0x32c89438>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72f96ade-cf7d-42fb-9541-0b9f0ed2cd81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_dynamic_trials',\n",
       " '_exp_key',\n",
       " '_ids',\n",
       " '_insert_trial_docs',\n",
       " '_trials',\n",
       " 'aname',\n",
       " 'argmin',\n",
       " 'assert_valid_trial',\n",
       " 'asynchronous',\n",
       " 'attachments',\n",
       " 'average_best_error',\n",
       " 'best_trial',\n",
       " 'count_by_state_synced',\n",
       " 'count_by_state_unsynced',\n",
       " 'delete_all',\n",
       " 'fmin',\n",
       " 'idxs',\n",
       " 'idxs_vals',\n",
       " 'insert_trial_doc',\n",
       " 'insert_trial_docs',\n",
       " 'losses',\n",
       " 'miscs',\n",
       " 'new_trial_docs',\n",
       " 'new_trial_ids',\n",
       " 'refresh',\n",
       " 'results',\n",
       " 'source_trial_docs',\n",
       " 'specs',\n",
       " 'statuses',\n",
       " 'tids',\n",
       " 'trial_attachments',\n",
       " 'trials',\n",
       " 'vals',\n",
       " 'view']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9bbe1529-a6dc-4237-8801-a6b0a0ffd197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'state': 2,\n",
       "  'tid': 0,\n",
       "  'spec': None,\n",
       "  'result': {'loss': 1.0, 'status': 'ok'},\n",
       "  'misc': {'tid': 0,\n",
       "   'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'workdir': None,\n",
       "   'idxs': {'model_a_x': [],\n",
       "    'model_b_x': [0],\n",
       "    'model_b_y': [0],\n",
       "    'my_choice': [0]},\n",
       "   'vals': {'model_a_x': [],\n",
       "    'model_b_x': [2],\n",
       "    'model_b_y': [1],\n",
       "    'my_choice': [1]}},\n",
       "  'exp_key': None,\n",
       "  'owner': None,\n",
       "  'version': 0,\n",
       "  'book_time': datetime.datetime(2021, 9, 5, 18, 27, 23, 935000),\n",
       "  'refresh_time': datetime.datetime(2021, 9, 5, 18, 27, 23, 941000)},\n",
       " {'state': 2,\n",
       "  'tid': 1,\n",
       "  'spec': None,\n",
       "  'result': {'loss': 0.0, 'status': 'ok'},\n",
       "  'misc': {'tid': 1,\n",
       "   'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'workdir': None,\n",
       "   'idxs': {'model_a_x': [1],\n",
       "    'model_b_x': [],\n",
       "    'model_b_y': [],\n",
       "    'my_choice': [1]},\n",
       "   'vals': {'model_a_x': [3],\n",
       "    'model_b_x': [],\n",
       "    'model_b_y': [],\n",
       "    'my_choice': [0]}},\n",
       "  'exp_key': None,\n",
       "  'owner': None,\n",
       "  'version': 0,\n",
       "  'book_time': datetime.datetime(2021, 9, 5, 18, 27, 23, 947000),\n",
       "  'refresh_time': datetime.datetime(2021, 9, 5, 18, 27, 23, 952000)}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trials.trials[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622f4063-b48d-44ad-8d44-17de0d2f55f2",
   "metadata": {},
   "source": [
    "#### 2.1.1.2. A Nested Univariate Choice\n",
    "In the example below, we define the search space as a choice between two models. Each model accepts a single hyperparameter which ranges in values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5dc48f8e-ac60-46dd-b8c7-990356156644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████| 10/10 [00:00<00:00, 222.21trial/s, best loss: 0.0]\n",
      "=========================\n",
      "Optimal args index:\n",
      "{'model_a_x': 0, 'my_choice': 0}\n",
      "Best hyperparameters:\n",
      "{'name': 'model a', 'x': 0}\n"
     ]
    }
   ],
   "source": [
    "import hyperopt\n",
    "import numpy\n",
    "\n",
    "# Define the search space\n",
    "space = hyperopt.hp.choice('my_choice', [\n",
    "    {\n",
    "        'name': 'model a',\n",
    "        'x': hyperopt.hp.choice('model_a_x', [0,2,4,6,8])\n",
    "    },\n",
    "    {\n",
    "        'name': 'model b',\n",
    "        'x': hyperopt.hp.choice('model_b_x', [1,3,5,7,9])\n",
    "    }\n",
    "])\n",
    "\n",
    "# Define the objective function\n",
    "def objective(args):\n",
    "    x = args['x']\n",
    "    return x\n",
    "\n",
    "# Define an object to keep track of the \"trials\" in the search path\n",
    "trials = hyperopt.Trials()\n",
    "    \n",
    "# Optimize the search space and retrieve the index which points to the best points in the search space\n",
    "optimal_args_index = hyperopt.fmin(objective, space, algo=hyperopt.tpe.suggest, max_evals=10, trials=trials, rstate= numpy.random.RandomState(42))\n",
    "    \n",
    "# Retrieve the resulting hyperparameter set from the search space using the index\n",
    "optimal_hyperparams = hyperopt.space_eval(space, optimal_args_index)\n",
    "\n",
    "# Print the results\n",
    "print(\"=========================\")\n",
    "print(\"Optimal args index:\")\n",
    "print(optimal_args_index)\n",
    "print(\"Best hyperparameters:\")\n",
    "print(optimal_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c9fdee-674d-42e3-8b01-91829b96ad58",
   "metadata": {},
   "source": [
    "#### 2.1.1.3. A multivariate nested choice\n",
    "In the next example we will make things a bit more interesting. We will use models which accept a different set of hyper parameters (one of them being multivariate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f819c920-10ed-4713-b085-856240fb46ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 62%|███████████▋       | 618/1000 [00:05<00:03, 104.51trial/s, best loss: 0.0]\n",
      "=========================\n",
      "Optimal args index:\n",
      "{'model_b_x': 0, 'model_b_y': 0, 'my_choice': 1}\n",
      "Best hyperparameters:\n",
      "{'name': 'model b', 'x': 0, 'y': 0}\n"
     ]
    }
   ],
   "source": [
    "# Define the search space\n",
    "space = hyperopt.hp.choice('my_choice', [\n",
    "    {\n",
    "        'name': 'model a',\n",
    "        'x': hyperopt.hp.choice('model_a_x', [1,2,4,6,8])\n",
    "    },\n",
    "    {\n",
    "        'name': 'model b',\n",
    "        'x': hyperopt.hp.choice('model_b_x', [0,1,2,3,4]),\n",
    "        'y': hyperopt.hp.choice('model_b_y', [0,3,5,7,9])        \n",
    "    }\n",
    "])\n",
    "\n",
    "# Define the objective function\n",
    "def objective(args):\n",
    "    x = args['x']\n",
    "    y = args['y'] if 'y' in args.keys() else 0\n",
    "    return x + y\n",
    "\n",
    "# Define an object to keep track of the \"trials\" in the search path\n",
    "trials = hyperopt.Trials()\n",
    "    \n",
    "# Optimize the search space and retrieve the index which points to the best points in the search space\n",
    "optimal_args_index = hyperopt.fmin(objective, space, algo=hyperopt.tpe.suggest, max_evals=1000, trials=trials, rstate= numpy.random.RandomState(42), loss_threshold=0.1)\n",
    "    \n",
    "# Retrieve the resulting hyperparameter set from the search space using the index\n",
    "optimal_hyperparams = hyperopt.space_eval(space, optimal_args_index)\n",
    "\n",
    "# Print the results\n",
    "print(\"=========================\")\n",
    "print(\"Optimal args index:\")\n",
    "print(optimal_args_index)\n",
    "print(\"Best hyperparameters:\")\n",
    "print(optimal_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6a3cee-b3cb-488b-8c0a-961aeaae3754",
   "metadata": {},
   "source": [
    "**Note:** We had to significantly increase the mas_evals and add the loss threshold param to allow us to terminate early if the loss is less than the threshold supplied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538dad2c-2826-4673-9615-1900bc3d1273",
   "metadata": {},
   "source": [
    "#### 2.1.1.4. A Conditional Parameters\n",
    "In the documentation we see an example of a conditional parameter. The idea is that a varibale is used if come condition is met. In the example below 'c1' and 'c2' are conditional parameters. Each of 'c1' and 'c2' only figures in the returned sample for a particular value of 'a'. If 'a' is 0, then 'c1' is used but not 'c2'. If 'a' is 1, then 'c2' is used but not 'c1'. Whenever it makes sense to do so, you should encode parameters as conditional ones this way, rather than simply ignoring parameters in the objective function. If you expose the fact that 'c1' sometimes has no effect on the objective function (because it has no effect on the argument to the objective function) then search can be more efficient about credit assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c5519087-20f2-4dfd-a7ce-aa67b8cb336d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|      | 1/1000 [00:00<00:04, 199.99trial/s, best loss: -7.032761300708053]\n",
      "=========================\n",
      "Optimal args index:\n",
      "{'c2': -7.032761300708053, 'x': 1}\n",
      "Best hyperparameters:\n",
      "('case 2', -7.032761300708053)\n"
     ]
    }
   ],
   "source": [
    "# Define the search space\n",
    "space = hyperopt.hp.choice('x',\n",
    "    [\n",
    "        ('case 1', 1 + hyperopt.hp.lognormal('c1', 0, 1)),\n",
    "        ('case 2', hyperopt.hp.uniform('c2', -10, 10))\n",
    "    ])\n",
    "\n",
    "# Define the objective function\n",
    "def objective(args):\n",
    "    case, value = args\n",
    "    return value\n",
    "\n",
    "# Define an object to keep track of the \"trials\" in the search path\n",
    "trials = hyperopt.Trials()\n",
    "    \n",
    "# Optimize the search space and retrieve the index which points to the best points in the search space\n",
    "optimal_args_index = hyperopt.fmin(objective, space, algo=hyperopt.tpe.suggest, max_evals=1000, trials=trials, rstate= numpy.random.RandomState(42), loss_threshold=0.1)\n",
    "    \n",
    "# Retrieve the resulting hyperparameter set from the search space using the index\n",
    "optimal_hyperparams = hyperopt.space_eval(space, optimal_args_index)\n",
    "\n",
    "# Print the results\n",
    "print(\"=========================\")\n",
    "print(\"Optimal args index:\")\n",
    "print(optimal_args_index)\n",
    "print(\"Best hyperparameters:\")\n",
    "print(optimal_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27ed31f-5cb7-46e5-bb8c-2a38f12609d3",
   "metadata": {},
   "source": [
    "### 2.1.1 The Randint Expression\n",
    "The Randint Expression allows us to define a hyperparameter as a random integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a1b8192a-e21f-4872-aa1b-4a8f14f5bec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hyperopt\n",
    "\n",
    "# define the upper limit of the domain for the random integer\n",
    "space = hyperopt.hp.randint('my_choice', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eccbfd0f-002e-491c-af96-821caf7be6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "4\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "print(hyperopt.pyll.stochastic.sample(space))\n",
    "print(hyperopt.pyll.stochastic.sample(space))\n",
    "print(hyperopt.pyll.stochastic.sample(space))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d0edd9-9858-4033-9e53-042958416060",
   "metadata": {},
   "source": [
    "Incorporating this into our example decision we have: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6eb3a17b-e7b8-40b4-9e8d-8199e64bd2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████| 10/10 [00:00<00:00, 243.89trial/s, best loss: 0.0]\n",
      "=========================\n",
      "Optimal args index:\n",
      "{'model_a_x': 0, 'my_choice': 0}\n",
      "Best hyperparameters:\n",
      "{'name': 'model a', 'x': 0}\n"
     ]
    }
   ],
   "source": [
    "import hyperopt\n",
    "import numpy\n",
    "\n",
    "# Define the search space\n",
    "space = hyperopt.hp.choice('my_choice', [\n",
    "    {\n",
    "        'name': 'model a',\n",
    "        'x': hyperopt.hp.randint('model_a_x', 10)\n",
    "    },\n",
    "    {\n",
    "        'name': 'model b',\n",
    "        'x': hyperopt.hp.randint('model_b_x', 15)\n",
    "    }\n",
    "])\n",
    "\n",
    "# Define the objective function\n",
    "def objective(args):\n",
    "    x = args['x']\n",
    "    return x\n",
    "\n",
    "# Define an object to keep track of the \"trials\" in the search path\n",
    "trials = hyperopt.Trials()\n",
    "    \n",
    "# Optimize the search space and retrieve the index which points to the best points in the search space\n",
    "optimal_args_index = hyperopt.fmin(objective, space, algo=hyperopt.tpe.suggest, max_evals=10, trials=trials, rstate= numpy.random.RandomState(42))\n",
    "    \n",
    "# Retrieve the resulting hyperparameter set from the search space using the index\n",
    "optimal_hyperparams = hyperopt.space_eval(space, optimal_args_index)\n",
    "\n",
    "# Print the results\n",
    "print(\"=========================\")\n",
    "print(\"Optimal args index:\")\n",
    "print(optimal_args_index)\n",
    "print(\"Best hyperparameters:\")\n",
    "print(optimal_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce41d26d-22ea-47d0-bc4f-9778e09b823c",
   "metadata": {},
   "source": [
    "## 2.2. Non-Stochastic (Deterministic) Expresisons with Pyll\n",
    "In some cases the built-in distributions may not provide the descriptive capabilities we want to model our hyperparameter. for example if we wanted to express our random variable as:\n",
    "\n",
    "$$ f = X + Y; \\ \\ where \\ X \\sim \\mathcal{N}, Y \\sim \\mathcal{U}  $$\n",
    "\n",
    "\n",
    "To accomodate this need, Hyperopt gives us the ability to write complex expressions in two ways:\n",
    "- Embedding an expression in a search space\n",
    "- Using a Pyll function\n",
    "\n",
    "We will see examples of these below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d72fce7-e2a7-4d21-bd73-a45190720c61",
   "metadata": {},
   "source": [
    "### 2.2.1. Embedding an expression in a search space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dfd8e245-1959-4419-82ec-2e22f37816af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|    | 1/10000 [00:00<01:39, 100.00trial/s, best loss: -0.7696510873187563]\n",
      "=========================\n",
      "Optimal args index:\n",
      "{'model_b_x': 2.74672956303527, 'model_b_y': -3.5163806503540265, 'my_choice': 1}\n",
      "Best hyperparameters:\n",
      "{'name': 'model b', 'x': -0.7696510873187563}\n"
     ]
    }
   ],
   "source": [
    "import hyperopt\n",
    "import numpy\n",
    "\n",
    "# ???\n",
    "def foobar(x, y):\n",
    "    return x + y\n",
    "\n",
    "# Define the search space\n",
    "space = hyperopt.hp.choice('my_choice', [\n",
    "    {\n",
    "        'name': 'model a',\n",
    "        'x': hyperopt.hp.choice('model_a_x', [1,2,3,4,5])\n",
    "    },\n",
    "    {\n",
    "        'name': 'model b',\n",
    "        'x': hyperopt.hp.uniform('model_b_x', -10, 10) + hyperopt.hp.uniform('model_b_y', -5, 5)\n",
    "    }\n",
    "])\n",
    "\n",
    "# Define the objective function\n",
    "def objective(args):\n",
    "    x = args['x']\n",
    "    return x\n",
    "\n",
    "# Define an object to keep track of the \"trials\" in the search path\n",
    "trials = hyperopt.Trials()\n",
    "    \n",
    "# Optimize the search space and retrieve the index which points to the best points in the search space\n",
    "optimal_args_index = hyperopt.fmin(objective, space, algo=hyperopt.tpe.suggest, max_evals=10000, trials=trials, rstate= numpy.random.RandomState(42), loss_threshold=0.1)\n",
    "    \n",
    "# Retrieve the resulting hyperparameter set from the search space using the index\n",
    "optimal_hyperparams = hyperopt.space_eval(space, optimal_args_index)\n",
    "\n",
    "# Print the results\n",
    "print(\"=========================\")\n",
    "print(\"Optimal args index:\")\n",
    "print(optimal_args_index)\n",
    "print(\"Best hyperparameters:\")\n",
    "print(optimal_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb16f678-492a-441c-8a1b-14a6ee7f56a6",
   "metadata": {},
   "source": [
    "### 2.2.2. Using a Pyll function\n",
    "The hyperopt library provides the pyll module which allows parameterized function to be placed into search spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "05d5eb9c-d732-4e16-b4b6-bfcacb9f6847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|    | 1/10000 [00:00<00:39, 249.99trial/s, best loss: -0.7696510873187563]\n",
      "=========================\n",
      "Optimal args index:\n",
      "{'model_b_x': 2.74672956303527, 'model_b_y': -3.5163806503540265, 'my_choice': 1}\n",
      "Best hyperparameters:\n",
      "{'name': 'model b', 'x': -0.7696510873187563}\n"
     ]
    }
   ],
   "source": [
    "import hyperopt\n",
    "import numpy\n",
    "\n",
    "# Define a deterministic function to use with pyll\n",
    "def foobar(x, y):\n",
    "    return x + y\n",
    "\n",
    "# Define the search space\n",
    "space = hyperopt.hp.choice('my_choice', [\n",
    "    {\n",
    "        'name': 'model a',\n",
    "        'x': hyperopt.hp.choice('model_a_x', [1,2,3,4,5])\n",
    "    },\n",
    "    {\n",
    "        'name': 'model b',\n",
    "        'x': hyperopt.pyll.scope.call(foobar, (\n",
    "            hyperopt.hp.uniform('model_b_x', -10, 10), \n",
    "            hyperopt.hp.uniform('model_b_y', -5, 5)\n",
    "        ))\n",
    "    }\n",
    "])\n",
    "\n",
    "# Define the objective function\n",
    "def objective(args):\n",
    "    x = args['x']\n",
    "    return x\n",
    "\n",
    "# Define an object to keep track of the \"trials\" in the search path\n",
    "trials = hyperopt.Trials()\n",
    "    \n",
    "# Optimize the search space and retrieve the index which points to the best points in the search space\n",
    "optimal_args_index = hyperopt.fmin(objective, space, algo=hyperopt.tpe.suggest, max_evals=10000, trials=trials, rstate= numpy.random.RandomState(42), loss_threshold=0.1)\n",
    "    \n",
    "# Retrieve the resulting hyperparameter set from the search space using the index\n",
    "optimal_hyperparams = hyperopt.space_eval(space, optimal_args_index)\n",
    "\n",
    "# Print the results\n",
    "print(\"=========================\")\n",
    "print(\"Optimal args index:\")\n",
    "print(optimal_args_index)\n",
    "print(\"Best hyperparameters:\")\n",
    "print(optimal_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbb28e0-fdbd-48bd-a2b2-478ea0f2c378",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22de9713-01d1-4e3c-85fa-0bb39fedceee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de4f833-4227-4754-bb81-b4f63e8fd4ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
