{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d82a55b-fa4d-44f3-ba75-1a29be6ea27d",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "Recently, the databricks team has contributed the SparkTrials object which allows hyperopt to distribute a tuning job across an Apache Spark cluster.\n",
    "\n",
    "If you are unfamiliar with Apache Spark, consider reviewing the [corresponding notebooks](../../Big%20Data%20And%20Big%20Compute/Apache%20Spark/README.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf06623-e664-4da0-8b2f-fa9ea75ebfd4",
   "metadata": {},
   "source": [
    "# Gotchas\n",
    "## Hyperopt version 0.2.7+ required for Apache Spark 3.0+\n",
    "It appears that there is a gotcha here. The hyperopt 0.2.5. release on pypi has a bug which makes it incompatible with spark 3.0. This is documented in an [open PR on github](https://github.com/hyperopt/hyperopt/issues/798). The PR links to a [merged PR](https://github.com/hyperopt/hyperopt/pull/765) which instructs how to install the merged code until the next released pypi package.\n",
    "\n",
    "One will need to uninstall hyperopt and then reinstall the patched version 0.2.7 using the following commands:\n",
    "\n",
    "```\n",
    "pip uninstall hyperopt\n",
    "pip install git+https://github.com/hyperopt/hyperopt.git\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d6a47d-b2d4-4c96-b4c5-8797aebd5aca",
   "metadata": {},
   "source": [
    "## Limitation of parrelel executions\n",
    "I ran into an issue where the *fmin()* function raised an error while trying to optimize the SARIMAX model:\n",
    "```\n",
    "TypeError: can't pickle _thread.RLock objects\n",
    "```\n",
    "I googled and found an [article](https://giters.com/hyperopt/hyperopt/issues/720) which stated that there is a limitation to the way hyperopt does things in parallel. Our options are:\n",
    "- Run distributed Hyperopt with single-machine training algorithms (SparkTrials)\n",
    "- Run single-machine Hyperopt with distributed training algorithms (base.Trials)\n",
    "\n",
    "AFAIK the SARIMAX model from stat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c0aba6-c646-44dc-b6ec-a240b43dd878",
   "metadata": {},
   "source": [
    "## Pickle search space\n",
    "\n",
    "If you cant pickle the search space youll have a problem. Recall search space samples are sent to spark executor nodes. \n",
    "\n",
    "```\n",
    "TypeError: can't pickle _thread.RLock objects\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d363eba1-700a-498c-b641-62155bda6606",
   "metadata": {},
   "source": [
    "# 1. How It Works\n",
    "\n",
    "Recall from the [README.md](README.md) that hyperopt's hyperparameter tuning functionlity is invoked via the *fmin()* function. As we have seen, this function allows us to pass in a trials object to which hyperopt records information about the various training trials that are conducted while it is searching the search space.\n",
    "\n",
    "Under the hood (looking at the [github code](https://github.com/hyperopt/hyperopt/blob/master/hyperopt/fmin.py#L540)) we can see that the trials object is what actually impliments the searching process. Thus the SparkTrials object's *fmin()* function is configured to run batches of training tasks in parallel, one on each Spark executor, allowing massive scale-out for tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c138ab-798b-4003-8cd0-00deecf31358",
   "metadata": {},
   "source": [
    "## 1.1. Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3f022a-f675-46d1-991f-1e3d9e2a7dbc",
   "metadata": {},
   "source": [
    "### 1.1.1. The Parallelism Parameter\n",
    "The databricks team has a [post](https://databricks.com/blog/2021/04/15/how-not-to-tune-your-model-with-hyperopt.html) providing insights about running hyperopt on spark. One important note is how one might use the **parallelism** parameter. this parameter dictates how many spark jobs to run in parallel. \n",
    "\n",
    "One of the major gotchas of this parameter comes from the fact that hyperopt's  [TPE search algorithm](Hyperopt%20Search%20Algorithms.ipynb) is iterative; information from the previous trials are used to determine where to look next. In the case where parallelism is set equal to the **max_evals** parameter then we are effectively doing random search as all the trials would be conducted in parallel.\n",
    "\n",
    "Another gotcha has to do with spark cluster utilization. Setting the parallelism parameter too low wastes resources. If running on a cluster with 32 cores, then running just 2 trials in parallel leaves 30 cores idle. Setting parallelism too high can cause a subtler problem. With a 32-core cluster, it’s natural to choose parallelism=32 of course, to maximize usage of the cluster’s resources. Setting it higher than cluster parallelism is counterproductive, as each wave of trials will see some trials waiting to execute.\n",
    "\n",
    "The article reccomends we **set parallelism to a small multiple of the number of hyperparameters**, and allocate cluster resources accordingly. For example, if searching over 4 hyperparameters, parallelism should not be much larger than 4. 8 or 16 may be fine, but 64 may not help a lot. 64 doesn’t hurt, it just may not help much."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704174e1-7f5f-40ba-9bb7-2b37dd4feb78",
   "metadata": {},
   "source": [
    "### 1.1.2. ML Library Built-in Parralelism\n",
    "Some ML libraries have the ability to take advantage of multithreading while training models. For example scikit-learn accepts the **n_jobs** parameter while xgboost accepts the **nthread** parameter. \n",
    "\n",
    "Although a single Spark task is assumed to use one core, nothing stops the task from using multiple cores. For example, with 16 cores available, one can run 16 single-threaded tasks, or 4 tasks that use 4 cores each. The latter is actually advantageous if the fitting process can efficiently use 4 cores and return the results in a timely manner. This is because Hyperopt is iterative, and returning fewer results faster improves its ability to learn from early results to schedule the next trials. That is, in this scenario, trials 5-8 could learn from the results of 1-4 if those first 4 tasks used 4 cores each to complete quickly and so on, whereas if all were run at once, none of the trials’ hyperparameter choices have the benefit of information from any of the others’ results.\n",
    "\n",
    "This affects thinking about the setting of parallelism. If a Hyperopt fitting process can reasonably use parallelism = 8, then by default one would allocate a cluster with 8 cores to execute it. But if the individual tasks can each use 4 cores, then allocating a 4 * 8 = 32-core cluster would be advantageous.\n",
    "\n",
    "One of the dangers of this approach is that Spark may schedule too many core-hungry tasks on one machine causing the cluster to be slow or unresponsive. This can be particularely troublesome if operating in a shared environment where other users/workflows are trying to share the cluster resources.\n",
    "\n",
    "A workaround is to execute the spark job using the **spark.task.cpus** parameter to tell spark the number of cores to allocate to each task. The disadvantage is that this is a cluster-wide configuration, which will cause all Spark jobs executed in the session to assume 4 cores for any task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10ef5ce-0cb9-41e5-a18a-8ca36d0f9693",
   "metadata": {},
   "source": [
    "### 1.1.3. Spark's Serialization Impacts To Objective Function Definitions\n",
    "Recall that Spark was written as a master/slave (rebranded as driver/executor) architecture. The work on the master node is split into chunks and sent to the slaves for processing. The mechanism by which this information is sent is serialization; ie. objects are converted into a serial byte stream and sent over the network and then rebuilt at the destination.\n",
    "\n",
    "This process has an obvious overhead of computation as well as network bandwidth.\n",
    "\n",
    "In the use case of hyperparameter tuning, we will likely be using the same train/test data within our objective function which is grading the search space (we likely only be changing the hyperparameters or ML algorithm). While prototyping we may be inclined to pass this data directly to the objective function within each call. This is a bad idea. This means every time we run a task, we have to serialize the data from the driver to the executor. This is unnecessary computation.\n",
    "\n",
    "We might instead decide to broadcast the data. Reading through thespark documentaiton see that Broadcast variables are read-only variables that are cached and available to tasks on all nodes in a cluster. Instead of sending this data along with every task, spark distributes broadcast variables to the machine using efficient broadcast algorithms to reduce communication costs. The problem with broadcast variables however is that they are limited to 2GB of data.\n",
    "\n",
    "As each spark task is a separate java process, the only other option is to do some advanced programming to load the data into a large shared memory object in the heap that can be used between tasks. Or possibly setting up a caching server of some kind that can quickly serve a raw byte stream. But that is outside our scope.\n",
    "\n",
    "Practically speaking, if the train/test set is larger that 2GB you might simply have to reach out to the datastore and load the data each time the function runs. The benefit to this approach is you free up resources on the executor which will likely allow your workflow to run more moothly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f422ed94-02e5-47e5-b05b-7fb90795c277",
   "metadata": {},
   "source": [
    "# 2. Suggestions on setting max_evals\n",
    "We have seen that the *fmin()* function takes a parameter called max_evals which dictates how many trials within the search space to conduct. For example, if we set max_evals=20 then the search algorithm would run 20 times on data points selected from the search space.\n",
    "\n",
    "Databricks has made a [reccomendation](https://databricks.com/blog/2021/04/15/how-not-to-tune-your-model-with-hyperopt.html) on a method for choosing the value for max_evals:\n",
    "\n",
    "<table>\n",
    "  <tbody>\n",
    "<tr>\n",
    "<th>Parameter Expression</th>\n",
    "<th>Optimal Results</th>\n",
    "<th>Fastest Results</th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>(ordinal parameters)<p></p>\n",
    "<p>hp.uniform<br>\n",
    "hp.quniform<br>\n",
    "hp.loguniform<br>\n",
    "hp.qloguniform</p></td>\n",
    "<td>20 x # parameters</td>\n",
    "<td>10 x # parameters</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>(categorical parameters)<p></p>\n",
    "<p>hp.choice</p></td>\n",
    "<td colspan=\"2\">15 x total categorical breadth*</td>\n",
    "</tr>\n",
    "</tbody>  \n",
    "</table>\n",
    "\n",
    "**Note:** “total categorical breadth” is the total number of categorical choices in the space.  If you have hp.choice with two options “on, off”, and another with five options “a, b, c, d, e”, your total categorical breadth is 10."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a0d20b-8c98-4a68-a3df-88128297bb24",
   "metadata": {},
   "source": [
    "# 3. Example\n",
    "In a previous [set of notebooks](../../Big%20Data%20And%20Big%20Compute/Apache%20Spark/README.md) we looked at using apache spark. In one [notebook](../../Big%20Data%20And%20Big%20Compute/Apache%20Spark/Running%20Scikit-Learn%20Apache%20Spark.ipynb) we looked at running scikit learn models on apache spark. We will continue where that notebook left off. This time we will optimize the hyperparameter for the scikit-learn model using hyperopt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47ac185-2298-45e9-ae64-ca5acdaab796",
   "metadata": {},
   "source": [
    "## 3.1. Get Spark Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0589317-a076-499e-a16a-60a5b74639c2",
   "metadata": {},
   "source": [
    "### 3.1.1. Confirm Kubernetes Is Online\n",
    "Recall Apache Spark is running on a Kubernetes Cluster. Before checking spark, which is higher in the tech stack, we will check the prerequisite apache spark cluster is online."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "655662bd-aaf8-4573-91db-0558df44e4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client Version: version.Info{Major:\"1\", Minor:\"21\", GitVersion:\"v1.21.0\", GitCommit:\"cb303e613a121a29364f75cc67d3d580833a7479\", GitTreeState:\"clean\", BuildDate:\"2021-04-08T16:31:21Z\", GoVersion:\"go1.16.1\", Compiler:\"gc\", Platform:\"windows/amd64\"}\n"
     ]
    }
   ],
   "source": [
    "! kubectl version --client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac63ed0e-5f7a-439e-82f3-be4577b50dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kubernetes control plane is running at https://15.4.7.11:6443\n",
      "CoreDNS is running at https://15.4.7.11:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\n",
      "\n",
      "To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
     ]
    }
   ],
   "source": [
    "! kubectl cluster-info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f71802d-a26d-453b-b962-d6a39426dd9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                           STATUS   ROLES                  AGE    VERSION\n",
      "os004k8-master001.foobar.com   Ready    control-plane,master   220d   v1.21.1\n",
      "os004k8-worker001.foobar.com   Ready    <none>                 220d   v1.21.1\n",
      "os004k8-worker002.foobar.com   Ready    <none>                 220d   v1.21.1\n",
      "os004k8-worker003.foobar.com   Ready    <none>                 220d   v1.21.1\n"
     ]
    }
   ],
   "source": [
    "! kubectl get node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ddd900-f40c-4558-a5a2-289e2d166d7d",
   "metadata": {},
   "source": [
    "### 3.1.2. Create SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "319f626e-a84d-429d-86d8-e06c663948e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting SPARK_HOME\n",
      "c:\\spark\\spark-3.1.1-bin-hadoop2.7\n",
      "\n",
      "Running findspark.init() function\n",
      "['c:\\\\spark\\\\spark-3.1.1-bin-hadoop2.7\\\\python', 'c:\\\\spark\\\\spark-3.1.1-bin-hadoop2.7\\\\python\\\\lib\\\\py4j-0.10.9-src.zip', 'c:\\\\program files\\\\python36\\\\python36.zip', 'c:\\\\program files\\\\python36\\\\DLLs', 'c:\\\\program files\\\\python36\\\\lib', 'c:\\\\program files\\\\python36', '', 'c:\\\\program files\\\\python36\\\\lib\\\\site-packages', 'c:\\\\program files\\\\python36\\\\lib\\\\site-packages\\\\win32', 'c:\\\\program files\\\\python36\\\\lib\\\\site-packages\\\\win32\\\\lib', 'c:\\\\program files\\\\python36\\\\lib\\\\site-packages\\\\Pythonwin', 'c:\\\\program files\\\\python36\\\\lib\\\\site-packages\\\\IPython\\\\extensions', 'C:\\\\Users\\\\Administrator\\\\.ipython']\n",
      "\n",
      "Setting PYSPARK_PYTHON\n",
      "/usr/bin/python3\n",
      "\n",
      "Determine IP Of Server\n",
      "The ip was detected as: 192.168.1.14\n",
      "\n",
      "Create SparkContext\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from spark_helper import create_spark_session\n",
    "spark_app_name = \"spark-jupyter-win\"\n",
    "docker_image = \"tschneider/pyspark:v6-beta\"\n",
    "k8_master_ip = \"15.4.7.11\"\n",
    "spark_session = create_spark_session(spark_app_name, docker_image, k8_master_ip)\n",
    "sc = spark_session.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43676db7-bdbc-41da-be54-bd84df108025",
   "metadata": {},
   "source": [
    "### 3.1.3. Ensure Kubernetes Has Created Containers\n",
    "We can look at kubernetes to see that out worker nodes were created. \n",
    "\n",
    "The first time we create the spark context with a given docker image, the image will need to be downloaded (which takes some time). As a result, we may see the pods with a status of \"ContainerCreating\". In this case, we will need to wait until the containers are in a \"Running\" state.\n",
    "\n",
    "```\n",
    "kubectl -n spark get pod\n",
    "NAME                                        READY   STATUS              RESTARTS   AGE\n",
    "spark-jupyter-win-3ed7f27984f7563a-exec-1   0/1     ContainerCreating   0          12m\n",
    "spark-jupyter-win-3ed7f27984f7563a-exec-2   0/1     ContainerCreating   0          12m\n",
    "spark-jupyter-win-3ed7f27984f7563a-exec-3   0/1     ContainerCreating   0          12m\n",
    "```\n",
    "\n",
    "Sometimes creating the container takes time as the docker image needs to be downloaded from the registry. If we want to check on the status of the container being created we can log into the pod and run the *docker pull* command. This command will attach to an inprogress pull command if one exists and will show the current pull status. An example would be as follows:\n",
    "\n",
    "```\n",
    "kubectl -n spark exec -ti docker pull tschneider/pyspark:v3 docker pull tschneider/pyspark:v4\n",
    "v3: Pulling from tschneider/pyspark\n",
    "2d473b07cdd5: Already exists\n",
    "71d236fb1195: Already exists\n",
    "2e22160d8cab: Already exists\n",
    "e99d962ac218: Pull complete\n",
    "Digest: sha256:eb74701b4ae909c40046ff68b1044b09b11895e175c955dfd8afe9fe680309cf\n",
    "Status: Downloaded newer image for tschneider/pyspark:v3\n",
    "docker.io/tschneider/pyspark:v3\n",
    "[root@os004k8-worker002 ~]# docker pull tschneider/pyspark:v4\n",
    "v4: Pulling from tschneider/pyspark\n",
    "2d473b07cdd5: Already exists\n",
    "71d236fb1195: Already exists\n",
    "2e22160d8cab: Already exists\n",
    "c556a717fe5d: Downloading [=======================>                           ]  578.7MB/1.246GB\n",
    "```\n",
    "\n",
    "We check the status of our pod with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c084584-29a6-4643-ad02-a85ca05299aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                        READY   STATUS    RESTARTS   AGE\n",
      "spark-jupyter-win-127dfb7d808790bc-exec-1   1/1     Running   0          32s\n",
      "spark-jupyter-win-127dfb7d808790bc-exec-2   1/1     Running   0          31s\n",
      "spark-jupyter-win-127dfb7d808790bc-exec-3   1/1     Running   0          31s\n"
     ]
    }
   ],
   "source": [
    "! kubectl -n spark get pod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad75bd5e-bea7-4bdb-bb54-d49f881dea1d",
   "metadata": {},
   "source": [
    "### 3.1.4. Create web server to host data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45b7167-66b6-4503-b7c6-33caeb76ab9a",
   "metadata": {},
   "source": [
    "Determine the current working directory. \n",
    "\n",
    "Note: There is a trick to doing this inside a jupyter notebook and so we will use a special library to get that information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b78d10dd-9015-4d77-a3a7-665272b6f435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\git\\ml-training-jupyter-notebooks\n"
     ]
    }
   ],
   "source": [
    "import pyprojroot\n",
    "project_root_dir  = pyprojroot.here()\n",
    "print(project_root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa58f76-fe09-44ad-acd1-de7091d3b722",
   "metadata": {},
   "source": [
    "Load the module for the webserver from our utilities directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87351703-0320-4d15-8645-21fa8591776f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the module for the web server we wrote\n",
    "import importlib.util\n",
    "spec = importlib.util.spec_from_file_location(\"PythonHttpFileServer\", \"../../../Utilities/PythonHttpFileServer.py\")\n",
    "PythonHttpFileServer = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(PythonHttpFileServer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f483ed10-65d2-4879-9f77-e9a282583234",
   "metadata": {},
   "source": [
    "Configure logging so that messages are collected and displayed asynchronously so that the server can run in the background without casuing a jupyter cell to block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "568b3b29-af36-4451-a3ef-94714de57094",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Starting server on port 80\n",
      "INFO:root:Web root specified as: C:\\Users\\Administrator\\git\\ml-training-jupyter-notebooks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app 'PythonHttpFileServer' (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:werkzeug: * Running on all addresses.\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "INFO:werkzeug: * Running on http://192.168.1.14:80/ (Press CTRL+C to quit)\n",
      "INFO:root:Get C:\\Users\\Administrator\\git\\ml-training-jupyter-notebooks\\Example Data Sets/nasdaq_2019.csv\n",
      "INFO:werkzeug:192.168.1.14 - - [03/Dec/2021 07:43:43] \"GET /Example%20Data%20Sets/nasdaq_2019.csv HTTP/1.1\" 200 -\n",
      "INFO:root:Get C:\\Users\\Administrator\\git\\ml-training-jupyter-notebooks\\Example Data Sets/nasdaq_2019.csv\n",
      "INFO:werkzeug:15.4.11.1 - - [03/Dec/2021 07:43:53] \"GET /Example%20Data%20Sets/nasdaq_2019.csv HTTP/1.1\" 200 -\n",
      "INFO:root:Get C:\\Users\\Administrator\\git\\ml-training-jupyter-notebooks\\favicon.ico\n",
      "ERROR:PythonHttpFileServer:Exception on /favicon.ico [GET]\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\program files\\python36\\lib\\site-packages\\flask\\app.py\", line 2051, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"c:\\program files\\python36\\lib\\site-packages\\flask\\app.py\", line 1501, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"c:\\program files\\python36\\lib\\site-packages\\flask\\app.py\", line 1499, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"c:\\program files\\python36\\lib\\site-packages\\flask\\app.py\", line 1485, in dispatch_request\n",
      "    return self.ensure_sync(self.view_functions[rule.endpoint])(**req.view_args)\n",
      "  File \"../../../Utilities/PythonHttpFileServer.py\", line 63, in get_file\n",
      "    with open(full_file_path, 'r') as file:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Administrator\\\\git\\\\ml-training-jupyter-notebooks\\\\favicon.ico'\n",
      "INFO:werkzeug:15.4.11.1 - - [03/Dec/2021 07:44:08] \"\u001b[35m\u001b[1mGET /favicon.ico HTTP/1.1\u001b[0m\" 500 -\n"
     ]
    }
   ],
   "source": [
    "# Configure the logger and log level\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Remove all handlers\n",
    "for handler in logger.handlers: \n",
    "    logger.removeHandler(handler)\n",
    "for handler in logger.handlers: \n",
    "    logger.removeHandler(handler)\n",
    "    \n",
    "# Start the webserver in a thread so the cell is not stuck in a running state\n",
    "import threading\n",
    "web_server_port = 80\n",
    "web_server_args = (web_server_port, project_root_dir)\n",
    "web_server_thread = threading.Thread(target=PythonHttpFileServer.run_server, args=web_server_args)\n",
    "web_server_thread.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f62026-623d-443e-8d90-28e9035747de",
   "metadata": {},
   "source": [
    "### 3.1.5. Load The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e518e8-3ecc-4828-b99d-c2ee147ab6d3",
   "metadata": {},
   "source": [
    "Instruct the spark cluster to download a file from the web server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d685e848-374b-462b-9aa8-43a9e513be52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data file should be available at: 'http://15.4.11.1:80/Example%20Data%20Sets/nasdaq_2019.csv'\n"
     ]
    }
   ],
   "source": [
    "from spark_helper import determine_ip_address\n",
    "csv_file_name = \"Example%20Data%20Sets/nasdaq_2019.csv\"\n",
    "ip_address = determine_ip_address()\n",
    "ip_address = \"15.4.11.1\"\n",
    "csv_file_url = \"http://{0}:{1}/{2}\".format(ip_address, web_server_port, csv_file_name)\n",
    "print(\"Data file should be available at: '{0}'\".format(csv_file_url))\n",
    "sc.addFile(csv_file_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26a1963-a3cf-495d-8460-25cf5f0796dd",
   "metadata": {},
   "source": [
    "Import the utility function to convert a date string to a datetime object from our utilities module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a4b3eff-bc6e-49c4-ba34-3c3d49c98dd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the utilities module we wrote\n",
    "import importlib.util\n",
    "spec = importlib.util.spec_from_file_location(\"utilities\", \"../../../Utilities/utilities.py\")\n",
    "utilities = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(utilities)\n",
    "\n",
    "# Define a mapping to convert our data field to the correct type\n",
    "converter_mapping = {\n",
    "    \"date\": utilities.convert_date_string_to_date\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2036b9-1172-4bab-848a-5764bdc37f40",
   "metadata": {
    "tags": []
   },
   "source": [
    "Load our OHCLV data Into a koalas dataframe and pull out a single day in the say way we would in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32115da3-a4fe-4737-a61f-8712b72d6d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:spark:Patching spark automatically. You can disable it by setting SPARK_KOALAS_AUTOPATCH=false in your environment\n"
     ]
    }
   ],
   "source": [
    "# Avoid a warning\n",
    "import os\n",
    "os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\n",
    "\n",
    "# Load the data\n",
    "from databricks import koalas\n",
    "koalas_dataframe = koalas.read_csv(u\"file:////nasdaq_2019.csv\", converters=converter_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8ceca1-9d73-42d1-8388-4080e98feb9a",
   "metadata": {},
   "source": [
    "We should see the workers download the file in the logs. If we log into the nodes we can see the file is located on the filesystem root."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1a32c0-3a24-49f5-a271-411fe737fd84",
   "metadata": {},
   "source": [
    "With the data loaded into a koalas dataframe we can access the data in the same way we would from a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fe0bba-65fe-4757-b508-f91ccd18f7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "koalas_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1e9982-0135-4e24-a055-a8d1de3966de",
   "metadata": {},
   "source": [
    "### 3.1.6. We need to prepare our worker nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7dc37f-6e00-4361-a20c-feec7f6779b3",
   "metadata": {},
   "source": [
    "Note: We need to install relevant python libraries on the worker nodes. If you do not, you might see an error as follows:\n",
    "```\n",
    "PythonException: \n",
    "  An exception was thrown from the Python worker. Please see the stack trace below.\n",
    "Traceback (most recent call last):\n",
    "  File \"/usr/local/lib/python3.6/site-packages/pyspark/worker.py\", line 588, in main\n",
    "    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n",
    "  File \"/usr/local/lib/python3.6/site-packages/pyspark/worker.py\", line 421, in read_udfs\n",
    "    arg_offsets, f = read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=0)\n",
    "  File \"/usr/local/lib/python3.6/site-packages/pyspark/worker.py\", line 249, in read_single_udf\n",
    "    f, return_type = read_command(pickleSer, infile)\n",
    "  File \"/usr/local/lib/python3.6/site-packages/pyspark/worker.py\", line 69, in read_command\n",
    "    command = serializer._read_with_length(file)\n",
    "  File \"/usr/local/lib/python3.6/site-packages/pyspark/serializers.py\", line 160, in _read_with_length\n",
    "    return self.loads(obj)\n",
    "  File \"/usr/local/lib/python3.6/site-packages/pyspark/serializers.py\", line 430, in loads\n",
    "    return pickle.loads(obj, encoding=encoding)\n",
    "  File \"/usr/local/lib/python3.6/site-packages/pyspark/cloudpickle/cloudpickle.py\", line 562, in subimport\n",
    "    __import__(name)\n",
    "ModuleNotFoundError: No module named 'pandas'\n",
    "```\n",
    "\n",
    "In our case we needed to install pandas, numpy, koalas, scikit-learn, sklearn. If you are unsure of what is installed on your workers, we can log into the kubernetes pods and execute shell commands.\n",
    "\n",
    "Note: We must do this on all workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923174a6-337e-4ce8-9ffc-1046f80689d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "! kubectl -n spark get pod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582d355a-8493-4325-b72f-d584d31e8346",
   "metadata": {},
   "outputs": [],
   "source": [
    "! kubectl -n spark exec -ti spark-jupyter-win-7716697d71777b7b-exec-1 -- pip3 list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89418a0-2fb3-419f-9fe7-416c6f29d029",
   "metadata": {},
   "source": [
    "## 3.2. Submit Python Code To Spark Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069d525e-1586-4544-9121-d7d42c4f9b77",
   "metadata": {},
   "source": [
    "In this section of the notebook we are going to apply the kmeans algorithm from sklearn to each date in our koalas_dataframe object.\n",
    "To do this, we are going to write a function that applies the algorithm to a dataframe; the assumption being the dataframe only contains data related to the same date.\n",
    "Note: Most of this is a review and reworking of the content contained in \n",
    "<a href=\"../Cluster%20Analysis/K-Means.ipynb\">Cluster Analysis/K-Means.ipynb</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc132af1-3528-455d-8f68-f21f474e5a04",
   "metadata": {},
   "source": [
    "### 3.2.1. Process Data\n",
    "To kick things off we need a dataframe representing a given stock. We will use the AABA stock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d620b5cf-ff8b-4cc5-9657-3fa2777673c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort based on the date column\n",
    "koalas_dataframe = koalas_dataframe.sort_values(\"date\")\n",
    "aaba_df = koalas_dataframe.loc[koalas_dataframe[\"ticker\"] == 'AABA'].copy()\n",
    "aaba_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0dba977-4ee1-4783-a039-139b45bca019",
   "metadata": {},
   "source": [
    "### 3.2.2. Define Search Space\n",
    "We will be using several models to make predictions about future data points. Here is where we define these models and their hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3002ac7e-6009-45ef-96da-de9c514ae469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import hyperopt\n",
    "import numpy\n",
    "import pandas\n",
    "import sklearn\n",
    "import statsmodels.api\n",
    "\n",
    "# Define the search space\n",
    "space = hyperopt.hp.choice('model_scenarios', [\n",
    "    {\n",
    "        'name': 'sarimax',\n",
    "        'model_type': statsmodels.api.tsa.statespace.SARIMAX,\n",
    "        'model_args': [],\n",
    "        'model_kwargs': {},\n",
    "        'feature_names': [\"open\"]\n",
    "    }\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5e237c-442d-4444-8f95-0ebd88e326e3",
   "metadata": {},
   "source": [
    "### 3.2.3. Define Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73e7673-9d3e-481e-a763-ec56e507f8a3",
   "metadata": {},
   "source": [
    "Our ML workflow requires a few functions to work properly. The basic process for selecting hyper parameters will be as follows:\n",
    "1. Perform train/test split\n",
    "2. Select training scenario (select a point from the hyperparameter search space)\n",
    "3. Train model (calibrate model to fit the data)\n",
    "4. Test model (make forcasts and calculate accuracy scores)\n",
    "We will define these as separate functions\n",
    "\n",
    "We will define some of these functions as standalone functions while others will be defined in an OOP way (ie. we will use classes, polymorphism, and interface programming)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c338fc07-9f94-4827-bad7-9471d3c5c4b7",
   "metadata": {},
   "source": [
    "#### 3.2.2.1. Perform the train test split\n",
    "We will split the data using the so that there are three days to look ahead and predict for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673596f7-7fb9-4c77-a542-cd2c7165679c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(df):\n",
    "    return df.iloc[0:-3], df.iloc[-3:]\n",
    "\n",
    "aaba_train_df, aaba_test_df = train_test_split(aaba_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4629c21-d982-4123-9125-c35968b35866",
   "metadata": {},
   "source": [
    "#### 3.2.2.2. Select a training scenario\n",
    "The scenario will be selected by the *fmin()* function provided by hyperopt so we do not need to define anything.\n",
    "\n",
    "But We can get a glimps of what will be passed to the objective function during a trial using the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de55de4b-ed75-4c90-965c-2186732c9903",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario = hyperopt.pyll.stochastic.sample(space)\n",
    "import pprint\n",
    "pprint.pprint(scenario)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864b22ba-f5ae-4297-b033-9f4f527fc944",
   "metadata": {},
   "source": [
    "Check we can serialize the scenario (per gotchas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f105a8-c88d-4ac5-aae4-09a39e9c47da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "d = pickle.dumps(scenario)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3755dabf-0c20-43e6-8939-c3508744da44",
   "metadata": {},
   "source": [
    "#### 3.2.2.3. Train a model\n",
    "We need to write a generic function that can train our algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9282a20b-6c78-4568-bf3e-0f74b2a2d63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class hyperopt_model():\n",
    "    def __init__(self, scenario):\n",
    "        \n",
    "        # Set some params for later\n",
    "        self.model_type = None\n",
    "        self.model = None\n",
    "        self.trained_model = None\n",
    "        self.is_built = False\n",
    "        self.is_trained = False\n",
    "        self.is_tested = False\n",
    "        self.scores = {}\n",
    "        \n",
    "        # Retrieve override parameters from the scenario\n",
    "        self.__dict__.update(scenario)\n",
    "        \n",
    "    def train(self, train_df):\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def test(self, train_df):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabd21ee-00e5-4d9c-8bd8-cfcf70e51117",
   "metadata": {},
   "outputs": [],
   "source": [
    "class sarimax(hyperopt_model):\n",
    "    def __init__(self, scenario):\n",
    "        super(sarimax, self).__init__(scenario)\n",
    "\n",
    "    def train(self, train_df):\n",
    "        try:\n",
    "            # Convert data from pandas to numpy\n",
    "            # Annoyingly, some algorithms cannot handle the koalas datatype\n",
    "            model_parameters = train_df[[*self.feature_names]].to_numpy()\n",
    "\n",
    "            args = list(self.model_args)\n",
    "            args.insert(0, model_parameters)\n",
    "\n",
    "            # Create an instance fit to the data\n",
    "            self.model = self.model_type(*args, **self.model_kwargs)\n",
    "            self.trained_model = self.model.fit()\n",
    "\n",
    "            # Set some flags\n",
    "            self.is_built = True\n",
    "            self.is_trained = True\n",
    "        except Exception as e:\n",
    "            raise Exception(\"Unable to train the model\") from e\n",
    "    \n",
    "    def test(self, test_df):\n",
    "        \n",
    "        try:\n",
    "            # Overcome issue with koalas api\n",
    "            from databricks.koalas.config import set_option, reset_option\n",
    "            set_option(\"compute.ops_on_diff_frames\", True)\n",
    "\n",
    "            # Predict next data points\n",
    "            # Note: We need to do some index magic\n",
    "            #       https://stackoverflow.com/questions/67662652/adding-a-new-column-to-an-existing-koalas-dataframe-results-in-nans\n",
    "            #\n",
    "            actuals = test_df[self.feature_names[0]]\n",
    "            predictions = self.trained_model.forecast(steps=test_df.shape[0])\n",
    "            test_df[\"predictions\"] = koalas.Series(predictions, test_df.index.tolist())\n",
    "\n",
    "            # Calculate the error for each prediction\n",
    "            test_df[\"error\"] = actuals - test_df[\"predictions\"]\n",
    "            \n",
    "            # Calculate aggregate error metrics\n",
    "            #     mae - mean absolute error\n",
    "            #     rmse - root mean squared error\n",
    "            #     mape - mean absolute percentage error\n",
    "            \n",
    "            self.scores = {\n",
    "                \"mae\": test_df[\"error\"].abs().sum(),\n",
    "                \"rmse\": numpy.sqrt((test_df[\"error\"]**2).mean()),\n",
    "                \"mape\": 100 * test_df[\"error\"].abs().sum() / actuals.sum()\n",
    "            }\n",
    "            self.is_tested = True\n",
    "            \n",
    "            return self.scores, test_df\n",
    "\n",
    "        except Exception as e:\n",
    "            raise Exception(\"Unable to train the model\") from e            \n",
    "        finally:\n",
    "            # Overcome issue with koalas api\n",
    "            reset_option(\"compute.ops_on_diff_frames\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f51055f-0c01-4536-b273-a81b250c108a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperopt_model_factory(scenario):\n",
    "    name = scenario[\"name\"]\n",
    "    constructor = globals()[name]\n",
    "    return constructor(scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a0e979-818a-4041-baf3-7bac5e48b603",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_hyperopt_model = hyperopt_model_factory(scenario)\n",
    "my_hyperopt_model.train(aaba_train_df)\n",
    "my_hyperopt_model.trained_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b9def8-d6e0-48cf-b0d5-c452ed44df24",
   "metadata": {},
   "source": [
    "#### 3.2.2.4. Test the model\n",
    "We will make predictions for the test data set and calculate an accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a729f0c-aa3d-44e3-a082-191f3dff470e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, error_df = my_hyperopt_model.test(aaba_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517a432c-7185-429d-9405-57e587e9edc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646a798f-b13c-48d9-949a-7a9c6db33b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928420a1-9e33-41c1-abe7-7c73db246ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores[\"mape\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b39894-fe2d-41de-aea2-fcbebdb33b34",
   "metadata": {},
   "source": [
    "### 3.2.4. Define The Objective Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad885702-c908-4e99-ab5b-3ec265423343",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function(scenario):\n",
    "    \n",
    "    my_hyperopt_model = hyperopt_model_factory(scenario)\n",
    "    my_hyperopt_model.train(aaba_train_df)\n",
    "    scores, error_df = my_hyperopt_model.test(aaba_test_df)\n",
    "    \n",
    "    return scores[\"mape\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad90d38-6f2b-49c2-a0cb-a416c1c8bbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "objective_function(scenario)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b46f52-d928-4689-a188-7dc25d90f875",
   "metadata": {},
   "source": [
    "### 3.2.6. Confirm Everything Can Pickle\n",
    "As we mentioned in the gotcha, some objects cannot pickle and thus cannot ...\n",
    "\n",
    "Our call to the objective function will look like this:\n",
    "\n",
    "```\n",
    "best_hyperparameters = hyperopt.fmin(\n",
    "  fn = objective_function,\n",
    "  space = space,\n",
    "  algo = hyperopt.tpe.suggest,\n",
    "  max_evals = 200,\n",
    "  trials = spark_trials,\n",
    "  loss_threshold = 0.05,\n",
    "  rstate = numpy.random.default_rng(42))\n",
    "```\n",
    "\n",
    "So we need to test if we can pickle all the parameters being passed in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379b9fc8-5eb7-4638-9714-d514c42cc6f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26de6cfc-a6dd-4f57-ad67-61bfba3f0da4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b075c7d-2ed3-4d6e-b251-c6201307dac0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.2.5. Run Hyperopt Against Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e97436-95ce-4d62-ab7a-c840e49a2b45",
   "metadata": {},
   "source": [
    "We first create an instance of the SparkTrials object to hold the trials from our hyperopt search. This can accept a *spark_session* parameter however it can also auto-detect the active spark session as we see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd2e5f1-fe89-491c-87ef-5ca02b4b1fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_trials = hyperopt.SparkTrials(parallelism=3, spark_session = spark_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6979cb56-af1e-4280-ad8e-6636f821b7f0",
   "metadata": {},
   "source": [
    "Once this object is create, we run the *fmin()* function as we have done in the past."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9d954f-6e57-4195-9396-234c85fbc44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = numpy.random.RandomState(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eed026c-2637-4536-81c6-344f5586b35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(seed)\n",
    "seed.rand()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d9a5aa-1559-4f2e-b696-bc29096b894f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def foobar(scenario):\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bbe764-94f4-4191-9639-842c9be8ad54",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyperparameters = hyperopt.fmin(\n",
    "  fn = foobar,\n",
    "  space = space,\n",
    "  algo = hyperopt.tpe.suggest,\n",
    "  max_evals = 200,\n",
    "  trials = spark_trials,\n",
    "  loss_threshold = 0.05,\n",
    "  rstate = numpy.random.default_rng(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f6f4bf-96c8-4302-88d3-232cef094f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyperparameters = hyperopt.fmin(\n",
    "  fn = objective_function,\n",
    "  space = space,\n",
    "  algo = hyperopt.tpe.suggest,\n",
    "  max_evals = 200,\n",
    "  trials = spark_trials,\n",
    "  loss_threshold = 0.05,\n",
    "  rstate= numpy.random.RandomState(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9554bdaa-2077-466d-83dd-38a6e6211a63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58025f34-bcfb-4a71-a40e-2a92a0a34caf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c143318-439e-484d-b6d3-4a60719de9b4",
   "metadata": {},
   "source": [
    "# 6. Cleanup Spark Cluster On Kubernetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f00a9f-a213-44b2-970f-625bb556a3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7d40ba-358e-4ceb-a1e7-f09529f29000",
   "metadata": {},
   "outputs": [],
   "source": [
    "! kubectl -n spark get pod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad59d93a-5881-41dc-b7e1-cfd2dde290cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28a5eff-812a-49f2-9940-69baae628181",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
