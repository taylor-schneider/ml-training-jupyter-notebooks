{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ec1342b-86a1-44b2-a1e8-877328153d06",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "A **Search Space** is a definition of all the posisble combinations of our models and coresponding hyperparameters through which we will search for the model and hyperparameters which yield the best predictions.\n",
    "\n",
    "There are many ways to define a search space and these definitions are typically tied to the algorithm used for the search. For example, when performing a grid search on might define the set of possible point (the grid) or one might have a function that returns the next point (rather than all points) to be more efficient.\n",
    "\n",
    "When doing a random search, search spaces must be defined in a way that ties a probability distribution to a parameter in the search space. Recall that in order for a random search to randomly select a value, it must know the distribution by which to make the selection.\n",
    "\n",
    "Recall that random variables defined according to a probability distribution are also referred to as stochastic variable. As we will see, the hyperopt framework defines the hyperparameters as stochastic variables using stochastic expressions.\n",
    "\n",
    "It's interesting that by defining the distribution, one is also defining how the search space will be explored. Thinking about the normal distribution for example, we know that the samples taken from the distribution will center around the mean. We have the possibility to explore points outside the mean, but those opportunities will be less likely as you get further from the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9878818c-d4a0-4eef-86bf-d5aa4a92ad71",
   "metadata": {},
   "source": [
    "# 1. Parameter Expressions\n",
    "The hyperopt framework provides **parameter expressions** as a way for a user to define stochastic variables.\n",
    "\n",
    "```\n",
    "space = expression(label, parameters)\n",
    "```\n",
    "\n",
    "**Note**: The label must be globally unique or you may see a *DuplicateLabel* Exception get raised. The label helps the hyperopt framework define a graph like structure through which the search is conducted. If the labels were not uniquely identified, the framework woudl not be able to understand what parameters it is looking for. This will become more clear as we look at the complex nexted examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86df9a4f-f40d-447d-b94e-f6a9bcb70c3a",
   "metadata": {},
   "source": [
    "There are two types of parameter expressions which we can leverage:\n",
    "- **Stochastic Expressions** - Define the hyperparameter using built in distributions\n",
    "- **Pyll Expressions** - Define the hyperparamter using a user defined distirbution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a57c96f-d502-453f-b3e6-397db8800d4b",
   "metadata": {},
   "source": [
    "## 1.1. Parameter Expressions Using Built-In Distributions\n",
    "Hyperopt has a number of built in distributions which we can use to define our hyperparameters. These include:\n",
    "- choice\n",
    "- pchoice\n",
    "- randint\n",
    "- uniform\n",
    "- quniform\n",
    "- quniformint\n",
    "- loguniform\n",
    "- qloguniform\n",
    "- normal\n",
    "- qnormal\n",
    "- lognormal\n",
    "- qlognormal\n",
    "\n",
    "All of these parameter expressions are found in the hyperopt.hp module. After looking through the code and making some inferences, I believe that hp stands for hyperparemter. Thus these expressions are hyperparameter expressions.\n",
    "\n",
    "We will take a closer look at these individually in the next few sections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642885e2-e98b-4bb1-897b-14c2f7558125",
   "metadata": {},
   "source": [
    "### 1.1.1. The Choice Expression\n",
    "With the choice expression, we can define a choice as a set of posisble outcomes with no specific distribution attached to it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11c89d7-60ab-41d0-8995-53dbc5a0e0c3",
   "metadata": {},
   "source": [
    "#### 1.1.1.1. A Univariate Choice\n",
    "Below we can define a seach space as a binary choice. In this simple case we are choosing between two models (generically named \"model a\" and \"model b\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef9c60f1-49c2-42c5-b214-04e892db8c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hyperopt\n",
    "\n",
    "space = hyperopt.hp.choice('my_choice', [\n",
    "    {'name': 'model a'},\n",
    "    {'name': 'model b'}\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b494ddcf-1be5-4f32-84b0-c631c162c27b",
   "metadata": {},
   "source": [
    "We can use the hyperopt framework to sable from this search space and examine the arguments that would be provided to our objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da1978dc-4bce-4dca-8828-f527a9e69cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'model a'}\n",
      "{'name': 'model b'}\n",
      "{'name': 'model b'}\n"
     ]
    }
   ],
   "source": [
    "print(hyperopt.pyll.stochastic.sample(space))\n",
    "print(hyperopt.pyll.stochastic.sample(space))\n",
    "print(hyperopt.pyll.stochastic.sample(space))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335db2e1-0ebb-4bea-be73-db94312f35b9",
   "metadata": {},
   "source": [
    "We can define a loss function and search through our parameter space for the optimal value using the hyperopt framework:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6941b1ec-4b0b-483f-ade2-8066cc1da49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'model b'}                                                            \n",
      "{'name': 'model a'}                                                            \n",
      "{'name': 'model a'}                                                            \n",
      "{'name': 'model a'}                                                            \n",
      "{'name': 'model b'}                                                            \n",
      "{'name': 'model b'}                                                            \n",
      "{'name': 'model a'}                                                            \n",
      "{'name': 'model a'}                                                            \n",
      "{'name': 'model a'}                                                            \n",
      "{'name': 'model a'}                                                            \n",
      "100%|██████████████████████| 10/10 [00:00<00:00, 125.00trial/s, best loss: 0.0]\n",
      "=========================\n",
      "Optimal args index:\n",
      "{'my_choice': 0}\n",
      "Best hyperparameters:\n",
      "{'name': 'model a'}\n"
     ]
    }
   ],
   "source": [
    "import hyperopt\n",
    "import numpy\n",
    "\n",
    "# Define the objective function\n",
    "def objective(args):\n",
    "    print(args)\n",
    "    if args[\"name\"] == \"model a\":\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "# Define an object to keep track of the \"trials\" in the search path\n",
    "trials = hyperopt.Trials()\n",
    "    \n",
    "# Optimize the search space and retrieve the index which points to the best points in the search space\n",
    "optimal_args_index = hyperopt.fmin(objective, space, algo=hyperopt.tpe.suggest, max_evals=10, trials=trials, rstate= numpy.random.RandomState(42))\n",
    "    \n",
    "# Retrieve the resulting hyperparameter set from the search space using the index\n",
    "optimal_hyperparams = hyperopt.space_eval(space, optimal_args_index)\n",
    "\n",
    "# Print the results\n",
    "print(\"=========================\")\n",
    "print(\"Optimal args index:\")\n",
    "print(optimal_args_index)\n",
    "print(\"Best hyperparameters:\")\n",
    "print(optimal_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7406ea6e-7161-4148-a5d9-206ae8e44b73",
   "metadata": {},
   "source": [
    "We can see that \"model a\" was selected as it yields the minimal results from the objective function.\n",
    "\n",
    "**Note:** We see that the search algorithm has a lot of repetitions... This is because it the only boundary on the search is the max_evals parameter. We will look at optimizing the search algorithm later on. For example when we utilize the loss_threshold to allow for early termination."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b12d795-dd3f-48ba-8369-8f590a04f55f",
   "metadata": {},
   "source": [
    "Having a look at the trials opject we inspect it's type and the useful properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05dbbebf-0999-468a-8b58-cc99f1ae33bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<hyperopt.base.Trials at 0x328d3550>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72f96ade-cf7d-42fb-9541-0b9f0ed2cd81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_dynamic_trials',\n",
       " '_exp_key',\n",
       " '_ids',\n",
       " '_insert_trial_docs',\n",
       " '_trials',\n",
       " 'aname',\n",
       " 'argmin',\n",
       " 'assert_valid_trial',\n",
       " 'asynchronous',\n",
       " 'attachments',\n",
       " 'average_best_error',\n",
       " 'best_trial',\n",
       " 'count_by_state_synced',\n",
       " 'count_by_state_unsynced',\n",
       " 'delete_all',\n",
       " 'fmin',\n",
       " 'idxs',\n",
       " 'idxs_vals',\n",
       " 'insert_trial_doc',\n",
       " 'insert_trial_docs',\n",
       " 'losses',\n",
       " 'miscs',\n",
       " 'new_trial_docs',\n",
       " 'new_trial_ids',\n",
       " 'refresh',\n",
       " 'results',\n",
       " 'source_trial_docs',\n",
       " 'specs',\n",
       " 'statuses',\n",
       " 'tids',\n",
       " 'trial_attachments',\n",
       " 'trials',\n",
       " 'vals',\n",
       " 'view']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bbe1529-a6dc-4237-8801-a6b0a0ffd197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'state': 2,\n",
       "  'tid': 0,\n",
       "  'spec': None,\n",
       "  'result': {'loss': 1.0, 'status': 'ok'},\n",
       "  'misc': {'tid': 0,\n",
       "   'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'workdir': None,\n",
       "   'idxs': {'my_choice': [0]},\n",
       "   'vals': {'my_choice': [1]}},\n",
       "  'exp_key': None,\n",
       "  'owner': None,\n",
       "  'version': 0,\n",
       "  'book_time': datetime.datetime(2021, 11, 24, 23, 27, 44, 377000),\n",
       "  'refresh_time': datetime.datetime(2021, 11, 24, 23, 27, 44, 377000)},\n",
       " {'state': 2,\n",
       "  'tid': 1,\n",
       "  'spec': None,\n",
       "  'result': {'loss': 0.0, 'status': 'ok'},\n",
       "  'misc': {'tid': 1,\n",
       "   'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'workdir': None,\n",
       "   'idxs': {'my_choice': [1]},\n",
       "   'vals': {'my_choice': [0]}},\n",
       "  'exp_key': None,\n",
       "  'owner': None,\n",
       "  'version': 0,\n",
       "  'book_time': datetime.datetime(2021, 11, 24, 23, 27, 44, 377000),\n",
       "  'refresh_time': datetime.datetime(2021, 11, 24, 23, 27, 44, 387000)}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trials.trials[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622f4063-b48d-44ad-8d44-17de0d2f55f2",
   "metadata": {},
   "source": [
    "#### 1.1.1.2. A Nested Univariate Choice\n",
    "In the example below, we define the search space as a choice between two models. Each model accepts a single hyperparameter which ranges in values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5dc48f8e-ac60-46dd-b8c7-990356156644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████| 10/10 [00:00<00:00, 333.33trial/s, best loss: 0.0]\n",
      "=========================\n",
      "Optimal args index:\n",
      "{'model_a_x': 0, 'my_choice': 0}\n",
      "Best hyperparameters:\n",
      "{'name': 'model a', 'x': 0}\n"
     ]
    }
   ],
   "source": [
    "import hyperopt\n",
    "import numpy\n",
    "\n",
    "# Define the search space\n",
    "space = hyperopt.hp.choice('my_choice', [\n",
    "    {\n",
    "        'name': 'model a',\n",
    "        'x': hyperopt.hp.choice('model_a_x', [0,2,4,6,8])\n",
    "    },\n",
    "    {\n",
    "        'name': 'model b',\n",
    "        'x': hyperopt.hp.choice('model_b_x', [1,3,5,7,9])\n",
    "    }\n",
    "])\n",
    "\n",
    "# Define the objective function\n",
    "def objective(args):\n",
    "    x = args['x']\n",
    "    return x\n",
    "\n",
    "# Define an object to keep track of the \"trials\" in the search path\n",
    "trials = hyperopt.Trials()\n",
    "    \n",
    "# Optimize the search space and retrieve the index which points to the best points in the search space\n",
    "optimal_args_index = hyperopt.fmin(objective, space, algo=hyperopt.tpe.suggest, max_evals=10, trials=trials, rstate= numpy.random.RandomState(42))\n",
    "    \n",
    "# Retrieve the resulting hyperparameter set from the search space using the index\n",
    "optimal_hyperparams = hyperopt.space_eval(space, optimal_args_index)\n",
    "\n",
    "# Print the results\n",
    "print(\"=========================\")\n",
    "print(\"Optimal args index:\")\n",
    "print(optimal_args_index)\n",
    "print(\"Best hyperparameters:\")\n",
    "print(optimal_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c9fdee-674d-42e3-8b01-91829b96ad58",
   "metadata": {},
   "source": [
    "#### 1.1.1.3. A multivariate nested choice\n",
    "In the next example we will make things a bit more interesting. We will use models which accept a different set of hyper parameters (one of them being multivariate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f819c920-10ed-4713-b085-856240fb46ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 62%|███████████▋       | 618/1000 [00:05<00:03, 104.57trial/s, best loss: 0.0]\n",
      "=========================\n",
      "Optimal args index:\n",
      "{'model_b_x': 0, 'model_b_y': 0, 'my_choice': 1}\n",
      "Best hyperparameters:\n",
      "{'name': 'model b', 'x': 0, 'y': 0}\n"
     ]
    }
   ],
   "source": [
    "# Define the search space\n",
    "space = hyperopt.hp.choice('my_choice', [\n",
    "    {\n",
    "        'name': 'model a',\n",
    "        'x': hyperopt.hp.choice('model_a_x', [1,2,4,6,8])\n",
    "    },\n",
    "    {\n",
    "        'name': 'model b',\n",
    "        'x': hyperopt.hp.choice('model_b_x', [0,1,2,3,4]),\n",
    "        'y': hyperopt.hp.choice('model_b_y', [0,3,5,7,9])        \n",
    "    }\n",
    "])\n",
    "\n",
    "# Define the objective function\n",
    "def objective(args):\n",
    "    x = args['x']\n",
    "    y = args['y'] if 'y' in args.keys() else 0\n",
    "    return x + y\n",
    "\n",
    "# Define an object to keep track of the \"trials\" in the search path\n",
    "trials = hyperopt.Trials()\n",
    "    \n",
    "# Optimize the search space and retrieve the index which points to the best points in the search space\n",
    "optimal_args_index = hyperopt.fmin(objective, space, algo=hyperopt.tpe.suggest, max_evals=1000, trials=trials, rstate= numpy.random.RandomState(42), loss_threshold=0.1)\n",
    "    \n",
    "# Retrieve the resulting hyperparameter set from the search space using the index\n",
    "optimal_hyperparams = hyperopt.space_eval(space, optimal_args_index)\n",
    "\n",
    "# Print the results\n",
    "print(\"=========================\")\n",
    "print(\"Optimal args index:\")\n",
    "print(optimal_args_index)\n",
    "print(\"Best hyperparameters:\")\n",
    "print(optimal_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6a3cee-b3cb-488b-8c0a-961aeaae3754",
   "metadata": {},
   "source": [
    "**Note:** We had to significantly increase the mas_evals and add the loss threshold param to allow us to terminate early if the loss is less than the threshold supplied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538dad2c-2826-4673-9615-1900bc3d1273",
   "metadata": {},
   "source": [
    "#### 1.1.1.4. A Conditional Parameters\n",
    "In the documentation we see an example of a conditional parameter. The idea is that a varibale is used if come condition is met. In the example below 'c1' and 'c2' are conditional parameters. Each of 'c1' and 'c2' only figures in the returned sample for a particular value of 'a'. If 'a' is 0, then 'c1' is used but not 'c2'. If 'a' is 1, then 'c2' is used but not 'c1'. Whenever it makes sense to do so, you should encode parameters as conditional ones this way, rather than simply ignoring parameters in the objective function. If you expose the fact that 'c1' sometimes has no effect on the objective function (because it has no effect on the argument to the objective function) then search can be more efficient about credit assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5519087-20f2-4dfd-a7ce-aa67b8cb336d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|       | 1/1000 [00:00<02:19,  7.14trial/s, best loss: -7.032761300708053]\n",
      "=========================\n",
      "Optimal args index:\n",
      "{'c2': -7.032761300708053, 'x': 1}\n",
      "Best hyperparameters:\n",
      "('case 2', -7.032761300708053)\n"
     ]
    }
   ],
   "source": [
    "# Define the search space\n",
    "space = hyperopt.hp.choice('x',\n",
    "    [\n",
    "        ('case 1', 1 + hyperopt.hp.lognormal('c1', 0, 1)),\n",
    "        ('case 2', hyperopt.hp.uniform('c2', -10, 10))\n",
    "    ])\n",
    "\n",
    "# Define the objective function\n",
    "def objective(args):\n",
    "    case, value = args\n",
    "    return value\n",
    "\n",
    "# Define an object to keep track of the \"trials\" in the search path\n",
    "trials = hyperopt.Trials()\n",
    "    \n",
    "# Optimize the search space and retrieve the index which points to the best points in the search space\n",
    "optimal_args_index = hyperopt.fmin(objective, space, algo=hyperopt.tpe.suggest, max_evals=1000, trials=trials, rstate= numpy.random.RandomState(42), loss_threshold=0.1)\n",
    "    \n",
    "# Retrieve the resulting hyperparameter set from the search space using the index\n",
    "optimal_hyperparams = hyperopt.space_eval(space, optimal_args_index)\n",
    "\n",
    "# Print the results\n",
    "print(\"=========================\")\n",
    "print(\"Optimal args index:\")\n",
    "print(optimal_args_index)\n",
    "print(\"Best hyperparameters:\")\n",
    "print(optimal_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27ed31f-5cb7-46e5-bb8c-2a38f12609d3",
   "metadata": {},
   "source": [
    "#### 3.2.1.2. The Randint Expression\n",
    "The Randint Expression allows us to define a hyperparameter as a random integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1b8192a-e21f-4872-aa1b-4a8f14f5bec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hyperopt\n",
    "\n",
    "# define the upper limit of the domain for the random integer\n",
    "space = hyperopt.hp.randint('my_choice', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eccbfd0f-002e-491c-af96-821caf7be6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "2\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "print(hyperopt.pyll.stochastic.sample(space))\n",
    "print(hyperopt.pyll.stochastic.sample(space))\n",
    "print(hyperopt.pyll.stochastic.sample(space))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d0edd9-9858-4033-9e53-042958416060",
   "metadata": {},
   "source": [
    "Incorporating this into our example decision we have: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6eb3a17b-e7b8-40b4-9e8d-8199e64bd2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████| 10/10 [00:00<00:00, 500.00trial/s, best loss: 0.0]\n",
      "=========================\n",
      "Optimal args index:\n",
      "{'model_a_x': 0, 'my_choice': 0}\n",
      "Best hyperparameters:\n",
      "{'name': 'model a', 'x': 0}\n"
     ]
    }
   ],
   "source": [
    "import hyperopt\n",
    "import numpy\n",
    "\n",
    "# Define the search space\n",
    "space = hyperopt.hp.choice('my_choice', [\n",
    "    {\n",
    "        'name': 'model a',\n",
    "        'x': hyperopt.hp.randint('model_a_x', 10)\n",
    "    },\n",
    "    {\n",
    "        'name': 'model b',\n",
    "        'x': hyperopt.hp.randint('model_b_x', 15)\n",
    "    }\n",
    "])\n",
    "\n",
    "# Define the objective function\n",
    "def objective(args):\n",
    "    x = args['x']\n",
    "    return x\n",
    "\n",
    "# Define an object to keep track of the \"trials\" in the search path\n",
    "trials = hyperopt.Trials()\n",
    "    \n",
    "# Optimize the search space and retrieve the index which points to the best points in the search space\n",
    "optimal_args_index = hyperopt.fmin(objective, space, algo=hyperopt.tpe.suggest, max_evals=10, trials=trials, rstate= numpy.random.RandomState(42))\n",
    "    \n",
    "# Retrieve the resulting hyperparameter set from the search space using the index\n",
    "optimal_hyperparams = hyperopt.space_eval(space, optimal_args_index)\n",
    "\n",
    "# Print the results\n",
    "print(\"=========================\")\n",
    "print(\"Optimal args index:\")\n",
    "print(optimal_args_index)\n",
    "print(\"Best hyperparameters:\")\n",
    "print(optimal_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce41d26d-22ea-47d0-bc4f-9778e09b823c",
   "metadata": {},
   "source": [
    "## 1.2. User Defined Expresisons with Pyll\n",
    "In some cases the built-in distributions may not provide the descriptive capabilities we want to model our hyperparameter. for example if we wanted to express our random variable as:\n",
    "\n",
    "$$ Z = X + Y; \\ \\ where \\ X \\sim \\mathcal{N}, Y \\sim \\mathcal{U}  $$\n",
    "\n",
    "\n",
    "To accomodate this need, Hyperopt gives us the ability to write complex expressions in two ways:\n",
    "- Embedding an expression in a search space\n",
    "- Using a Pyll function\n",
    "\n",
    "We will see examples of these below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d72fce7-e2a7-4d21-bd73-a45190720c61",
   "metadata": {},
   "source": [
    "### 1.2.1. Embedding an expression in a search space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dfd8e245-1959-4419-82ec-2e22f37816af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|             | 1/10000 [00:00<?, ?trial/s, best loss: -0.7696510873187563]\n",
      "=========================\n",
      "Optimal args index:\n",
      "{'model_b_x': 2.74672956303527, 'model_b_y': -3.5163806503540265, 'my_choice': 1}\n",
      "Best hyperparameters:\n",
      "{'name': 'model b', 'x': -0.7696510873187563}\n"
     ]
    }
   ],
   "source": [
    "import hyperopt\n",
    "import numpy\n",
    "\n",
    "# Define the search space\n",
    "space = hyperopt.hp.choice('my_choice', [\n",
    "    {\n",
    "        'name': 'model a',\n",
    "        'x': hyperopt.hp.choice('model_a_x', [1,2,3,4,5])\n",
    "    },\n",
    "    {\n",
    "        'name': 'model b',\n",
    "        'x': hyperopt.hp.uniform('model_b_x', -10, 10) + hyperopt.hp.uniform('model_b_y', -5, 5)\n",
    "    }\n",
    "])\n",
    "\n",
    "# Define the objective function\n",
    "def objective(args):\n",
    "    x = args['x']\n",
    "    return x\n",
    "\n",
    "# Define an object to keep track of the \"trials\" in the search path\n",
    "trials = hyperopt.Trials()\n",
    "    \n",
    "# Optimize the search space and retrieve the index which points to the best points in the search space\n",
    "optimal_args_index = hyperopt.fmin(objective, space, algo=hyperopt.tpe.suggest, max_evals=10000, trials=trials, rstate= numpy.random.RandomState(42), loss_threshold=0.1)\n",
    "    \n",
    "# Retrieve the resulting hyperparameter set from the search space using the index\n",
    "optimal_hyperparams = hyperopt.space_eval(space, optimal_args_index)\n",
    "\n",
    "# Print the results\n",
    "print(\"=========================\")\n",
    "print(\"Optimal args index:\")\n",
    "print(optimal_args_index)\n",
    "print(\"Best hyperparameters:\")\n",
    "print(optimal_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb16f678-492a-441c-8a1b-14a6ee7f56a6",
   "metadata": {},
   "source": [
    "### 1.2.2. Using a Pyll function\n",
    "The hyperopt library provides the pyll module which allows parameterized function to be placed into search spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05d5eb9c-d732-4e16-b4b6-bfcacb9f6847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|    | 1/10000 [00:00<01:39, 100.00trial/s, best loss: -0.7696510873187563]\n",
      "=========================\n",
      "Optimal args index:\n",
      "{'model_b_x': 2.74672956303527, 'model_b_y': -3.5163806503540265, 'my_choice': 1}\n",
      "Best hyperparameters:\n",
      "{'name': 'model b', 'x': -0.7696510873187563}\n"
     ]
    }
   ],
   "source": [
    "import hyperopt\n",
    "import numpy\n",
    "\n",
    "# Define a deterministic function to use with pyll\n",
    "def foobar(x, y):\n",
    "    return x + y\n",
    "\n",
    "# Define the search space\n",
    "space = hyperopt.hp.choice('my_choice', [\n",
    "    {\n",
    "        'name': 'model a',\n",
    "        'x': hyperopt.hp.choice('model_a_x', [1,2,3,4,5])\n",
    "    },\n",
    "    {\n",
    "        'name': 'model b',\n",
    "        'x': hyperopt.pyll.scope.call(foobar, (\n",
    "            hyperopt.hp.uniform('model_b_x', -10, 10), \n",
    "            hyperopt.hp.uniform('model_b_y', -5, 5)\n",
    "        ))\n",
    "    }\n",
    "])\n",
    "\n",
    "# Define the objective function\n",
    "def objective(args):\n",
    "    x = args['x']\n",
    "    return x\n",
    "\n",
    "# Define an object to keep track of the \"trials\" in the search path\n",
    "trials = hyperopt.Trials()\n",
    "    \n",
    "# Optimize the search space and retrieve the index which points to the best points in the search space\n",
    "optimal_args_index = hyperopt.fmin(objective, space, algo=hyperopt.tpe.suggest, max_evals=10000, trials=trials, rstate= numpy.random.RandomState(42), loss_threshold=0.1)\n",
    "    \n",
    "# Retrieve the resulting hyperparameter set from the search space using the index\n",
    "optimal_hyperparams = hyperopt.space_eval(space, optimal_args_index)\n",
    "\n",
    "# Print the results\n",
    "print(\"=========================\")\n",
    "print(\"Optimal args index:\")\n",
    "print(optimal_args_index)\n",
    "print(\"Best hyperparameters:\")\n",
    "print(optimal_hyperparams)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
