{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c0ac83e-559d-4fbc-8967-ef530283f3b2",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "As stated earlier, the objective function is responsible for evaluating the \"goodness\" or \"performance\" of a model or function when it is provided with a particular parameter set. This is sometimes referred to as a loss function within the machine learning space. The evaluation is provided as a globally comparable metric or measurement (some type of number).\n",
    "\n",
    "```\n",
    "metric = objective(parameter_set)\n",
    "```\n",
    "\n",
    "Defining this measure is outside the scope of this notebook. At a high level, the function might be defined as:\n",
    "\n",
    "```\n",
    "def objective(parameters_set):\n",
    "\n",
    "    // Create a machine learning model\n",
    "    // Train the model\n",
    "    // Test the model\n",
    "    // return test results metric\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "For educational purposed we will consider a trivial measure. Our objective function will return a number representing the sum of the sample parameters it was provided from the search space. For example, consider the following:\n",
    "\n",
    "```\n",
    "metric = objective(0)\n",
    "# metric == 0\n",
    "\n",
    "metric = objective(1,2,3)\n",
    "# metric == 6\n",
    "\n",
    "```\n",
    "\n",
    "As stated earlier, defining the objective function depends on the search space. If the schema of the search space changes, so must the objective function. As we go through our examples, we will see the objective function's definition change to fit our search space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce977c8-a974-48db-bc3b-c04aac47d2a9",
   "metadata": {},
   "source": [
    "# Examples\n",
    "TODO: Add exampls of objective functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792679f7-b39b-43b2-a5cf-a619f0bcb387",
   "metadata": {},
   "source": [
    "# Considerations\n",
    "## Cross Validation\n",
    "Databricks has made some [comments](https://databricks.com/blog/2021/04/15/how-not-to-tune-your-model-with-hyperopt.html) regarding cross validation that are worth considering. \n",
    "\n",
    "Before getting into this, let's review the topic. In a nutshell, cross validation techniques, like k-folds will split the data into multiple sets intended for training and testing. It then trains the model multiple times on these (k) different sets of training and testing data to see how it performs accross them. The idea being that overfitting can be detected if a model performs inexplicably better on one set than another.\n",
    "\n",
    "The argument from databricks is that the k-1 extra training sessions could have been spent exploring new points within the hyperparameter space. In other words, databricks is calling out the tradeoff between hyperparameter exploration and certainty of model accuracy. They argue that increasing max_evals by a factor of k is probably better than adding k-fold cross-validation, all else equal.\n",
    "\n",
    "I might offer a hybrid approach and suggest performing cross validation on the top performing hyperparameter sets rather than all the sets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
