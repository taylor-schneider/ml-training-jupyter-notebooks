{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "446a7a2b-b25b-4349-8586-01feaf454755",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "17a2ab60-4b2f-4090-af75-4daf1e09acac",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5cd01307-41fd-43a1-ad96-4ec872f2d7f4",
   "metadata": {},
   "source": [
    "Is the correlation matrix a transformation? We want the variance unchanged?\n",
    "\n",
    "The regression lines will maximize the variance. We can write an equation for these regression lines using coefficients and the variables. We can derive the variance of the regression lines as a function of the variance (covariance matrix) of the variables and the coefficients. \n",
    "\n",
    "A linear function is referred to as a component of our standardizded matrix Z. It can be shown that the dispersion matrix of a standardized variable is a correlation matrix; thus the correlation matrix shows dispertion. \n",
    "\n",
    "It is easier to optimize sets of variables when they are uncorrelated because we can isolate and focus on a single variable and work our way to the others. Ideally our regression would consider uncorrelated variables.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a78e405-5b69-4017-a65b-dc7608a68623",
   "metadata": {},
   "source": [
    "#### Single Value Decomp\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1da0e1-e184-4165-ad63-7cfcd0c1d75f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd5930c-2bfc-45b0-b3b4-8aee226063ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8326f1-b195-41ee-be3d-4d16a723c836",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13647b50-e38b-46e5-a7f0-c3a8375b4163",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27f21696-4bec-49df-bc0e-a5b58d93bd63",
   "metadata": {},
   "source": [
    "# 4. Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c620260f-17a1-41a3-a3f1-06e6c427b6a5",
   "metadata": {},
   "source": [
    "## 4.1. Intuitive Definition\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9849161-fb80-4639-a392-40f5e2726972",
   "metadata": {},
   "source": [
    "## 4.2. History\n",
    "As we will see, PCA is reliant on the Pearson Correlation Coefficient which was published py pearson in 1896.\n",
    "\n",
    "In 1901 Carl Pearson wrote *On Lines and Planes of Closest Fit to Systems of Points in Space* which introduced the concept of Principal Component Analysis (PCA). \n",
    "\n",
    "Harold Hotelling is credited with independently developing and naming PCA as Canonical Correlation Analysis (CCA) in 1930. The name implies that the principal components are canonical and derived using correlation (we will see more about this later).\n",
    "\n",
    "It is stated that PCA is the analogue of Principal Axis Theorem in mechanics which attemps to decompose a multidimensional space into \"principal axes\" (ie. a mathematically convenient coordinate system). Interestingly, these \"principal axes\" are also referred to as \"canonical axes\" as they are derived using the Spectral Theorem and exibit common characteristics which are well suited for mathematical analysis and treatments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089d2aa8-f00c-4349-b10f-92487024fb23",
   "metadata": {},
   "source": [
    "## 4.3. Proceedure\n",
    "\n",
    "The process of finding principal components is essentially an optimization problem with multiple steps. \n",
    "\n",
    "### 4.3.1. Step 1: Find the principal component\n",
    "The first principal component is the linear combination of variables that has a maximum variance (ie. line of best fit).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51487ebf-f354-441c-b877-0857e33fd658",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbac0eb1-c017-4134-8c99-873ecfecb2ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b42c3ec-61c6-4ac7-a59b-7227be72f9f5",
   "metadata": {},
   "source": [
    "# Calculate Variance Of Best Fit Line\n",
    "\n",
    "## Using non-matrix notation\n",
    "The simplest definition of variance is defined as follows\n",
    "\n",
    "$$ \\sigma_X = \\mathbb{E} \\left[ (X-\\mu )^{2} \\right] = \\frac{1}{n}\\sum^n_i (X_i-\\mu )^{2} $$\n",
    "\n",
    "We can derive the variance of $Y_i$ using this formula\n",
    "\n",
    "$$ \\sigma_{Y_i} = \\frac{1}{n} \\sum^n_j (y_j - \\mu_{Y_i})^2 $$\n",
    "\n",
    "Where $y_j = \\beta_{i, j}X_j \\in Y_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53da4e3-0d7a-4520-af5c-82dd4ee21598",
   "metadata": {},
   "source": [
    "$$ = \\frac{1}{n} \\sum^n_j (\\beta_{i,j}X_j - \\mu_{Y_i})^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031d853f-a900-4746-94ea-a3988cadab93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fdf51d0c-d74d-4c01-91c1-00f6d3eb4395",
   "metadata": {},
   "source": [
    "## Using matrix notation\n",
    "In matrix form we have:\n",
    "\n",
    "$$ \\sigma_{Y_i} = \\frac{1}{n} (Y_i - \\mu_{Y_i})^T(Y_i - \\mu_{Y_i}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f805700f-3920-44ac-845c-c5aa9dc9f556",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588de141-f2ad-4a0c-ab53-8f55725039ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6000456d-0795-4508-b5b5-67087537c5ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16a8872-73da-4112-abe6-fd74656ec180",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aefbad6a-522a-428f-b4c4-49c4bee2448b",
   "metadata": {},
   "source": [
    "Recalling our definition of $ Y_i = \\beta_i I X $ we can see the equation expanded as\n",
    "\n",
    "$$ = \\frac{1}{n} (\\beta_i I X - \\mu_{Y_i})^T(\\beta_j I X - \\mu_{Y_j}) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0aa158-3f85-49ce-9ae5-bc1a41e673e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9ab236-4b5e-428e-a4ad-d04b1c08ded4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0a70b8-b771-4cef-a03f-74d442fc544d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b121867c-1662-4bbb-aa35-048dc04d740a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fb2782-ae9c-40d5-bb62-855c28c9904d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ade396a-e1cf-49a3-aae1-4f5b946856eb",
   "metadata": {},
   "source": [
    "# Calculate Covariance\n",
    "Similarely, the covariance between $Y_i$ and $Y_j$ in matrix form is:\n",
    "\n",
    "$$ \\sigma_{Y_i,Y_j} = \\frac{1}{n} (Y_i - \\mu_{Y_i})^T(Y_j - \\mu_{Y_j}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edf9c1b-5ffa-4573-9752-b703409d875d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2fa3abc-e08a-4ff4-ace2-367875bd917c",
   "metadata": {},
   "source": [
    "Instead of calculating a scalar, we could calculate a vector as an (nxn) matrix by writing a different but equivalent matrix equation:\n",
    "\n",
    "$$ \\Sigma = \\frac{1}{n} (Y_i - \\mu_{Y_i})^T(Y_j - \\mu_{Y_j}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2afbd26-dfc4-418f-a447-9b907ca1cc09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3147d47b-d58b-4a38-b8bd-f8e51650e91f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03392113-48e7-4919-8723-20d8a4c8bc12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
