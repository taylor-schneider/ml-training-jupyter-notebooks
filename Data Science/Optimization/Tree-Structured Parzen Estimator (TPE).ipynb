{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b102abbc-fc67-40e6-a2e7-dbaff7700ff7",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34d722e-c775-445b-805c-bd00116be87b",
   "metadata": {},
   "source": [
    "The Tree-Structured Parzen Estimator (TPE) algorithm was developed for selecting the best model and/or hyperparameters for a machine learning algorithm. It uses bayesian inference and a tree like structure to arive at the optimal parameter set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1d1279-7c3c-4e58-839c-c547706cc092",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# History\n",
    "\n",
    "The TPE algorithm was originally [proposed](https://hal.inria.fr/hal-00642998/document) in 2011 by a team of researchers including James Bergstra. Bergstra is the author of the [hyperopt](../../Machine%20Learning/Hyperparameter%20Optimization%20%28HPO%29/Hyperopt/README.md) software used for hyperparameter optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8629971-d424-4e0c-8fc0-45ada903aaa3",
   "metadata": {},
   "source": [
    "# Terminology\n",
    "The naming of this technique is a bit dense and I wanted to clarify the word choice:\n",
    "\n",
    "First we need to clarify the term Parzen estimator. Parzen estimator referres to a Parzen Window (a kernel function) being used estimate a density function through a technique called [kernel density estimation](../Exploratory%20Data%20Analysis%20%28EDA%29/Kernel%20Density%20Estimation%20%28KDE%29.ipynb). Recall that density can be used to construct approximations for probability distributions (we have a [notebook](../Exploratory%20Data%20Analysis%20%28EDA%29/Density%20and%20Probability.ipynb) on this subject). With the aproximate proabability density function we have a probabilitstic framework for making predictions based on empiracle evidence.\n",
    "\n",
    "Next we need to talk about the concept of a Tree Structure. Tree structures are used to impliment what's typically referred to as a decision tree. The decision tree effectively classifies objects into various categories by employing a (somtimes chained or iterative) decision making algorithm. In the case of TPE the objest are classified into a binary tree where object on one side have desirable traits where object on the other do not.\n",
    "\n",
    "The basic algorithm can be seen below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b93b3c-0a7f-4f73-83be-383cf13230d8",
   "metadata": {},
   "source": [
    "# Algorithm\n",
    "\n",
    "Generally, the algorithm consists of the following steps:\n",
    "\n",
    "1. Define a domain of hyperparameter search space,\n",
    "\n",
    "2. Create an objective function which takes in hyperparameters and outputs a score (e.g., loss, root mean squared error, cross-entropy) that we want to minimize,\n",
    "\n",
    "3. Get couple of observations (score) using randomly selected set of hyperparameters,\n",
    "\n",
    "4. Sort the collected observations by score and divide them into two groups based on some quantile. The first group (x1) contains observations that gave the best scores and the second one (x2) - all other observations,\n",
    "\n",
    "5. Two densities l(x1) and g(x2) are modeled using Parzen Estimators (also known as kernel density estimators) which are a simple average of kernels centered on existing data points,\n",
    "\n",
    "6. Draw sample hyperparameters from l(x1), evaluating them in terms of l(x1)/g(x2), and returning the set that yields the minimum value under l(x1)/g(x1) corresponding to the greatest expected improvement. These hyperparameters are then evaluated on the objective function.\n",
    "\n",
    "7. Update the observation list from step 3\n",
    "\n",
    "8. Repeat step 4-7 with a fixed number of trials or until time limit is reached\n",
    "\n",
    "For more information see this [article](https://docs.openvino.ai/latest/pot_compression_optimization_tpe_README.html) or the [original proposal](https://hal.inria.fr/hal-00642998/document) for the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73452e5e-756c-4ee9-a065-0003fd686558",
   "metadata": {},
   "source": [
    "# Extensions\n",
    "The TPE algorithm has been extended since its original release. Some worthwhile readings can be found below:\n",
    "\n",
    "Tuning hyperparametes with TPE: https://booksc.org/book/70649874/302882\n",
    "Multi-objective TPE (2020): https://dl.acm.org/doi/pdf/10.1145/3377930.3389817"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e565b6-09d6-43f1-b607-22eb819ee19b",
   "metadata": {},
   "source": [
    "# Drawbacks\n",
    "\n",
    "The TPE algorithm does not model the interactions between hyperparameters. As such, if hyperparameters are positively or negatively correlated, the tuning of those parameters is acheived without this knowledge. As such the algorithm can be seen as less efficient."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
