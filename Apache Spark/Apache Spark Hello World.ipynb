{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Connect to apache spark cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_data': {'ALLUSERSPROFILE': 'C:\\\\ProgramData',\n",
      "           'ANT_HOME': 'c:\\\\ant',\n",
      "           'APPDATA': 'C:\\\\Users\\\\Administrator\\\\AppData\\\\Roaming',\n",
      "           'CLICOLOR': '1',\n",
      "           'COMMONPROGRAMFILES': 'C:\\\\Program Files\\\\Common Files',\n",
      "           'COMMONPROGRAMFILES(X86)': 'C:\\\\Program Files (x86)\\\\Common Files',\n",
      "           'COMMONPROGRAMW6432': 'C:\\\\Program Files\\\\Common Files',\n",
      "           'COMPUTERNAME': 'STUDENT-LAPTOP',\n",
      "           'COMSPEC': 'C:\\\\Windows\\\\system32\\\\cmd.exe',\n",
      "           'DISPLAYLINKCONFIGROOT': 'C:\\\\Program Files\\\\DisplayLink Core '\n",
      "                                    'Software\\\\8.2.1646.0\\\\',\n",
      "           'DOKANLIBRARY1': 'C:\\\\Program Files\\\\Dokan\\\\Dokan Library-1.0.0\\\\',\n",
      "           'DOKANLIBRARY1_LIBRARYPATH_X64': 'C:\\\\Program Files\\\\Dokan\\\\Dokan '\n",
      "                                            'Library-1.0.0\\\\lib\\\\',\n",
      "           'DOKANLIBRARY1_LIBRARYPATH_X86': 'C:\\\\Program Files\\\\Dokan\\\\Dokan '\n",
      "                                            'Library-1.0.0\\\\x86\\\\lib\\\\',\n",
      "           'FP_NO_HOST_CHECK': 'NO',\n",
      "           'GIT_PAGER': 'cat',\n",
      "           'GTK_BASEPATH': 'C:\\\\Program Files (x86)\\\\GtkSharp\\\\2.12\\\\',\n",
      "           'HOMEDRIVE': 'C:',\n",
      "           'HOMEPATH': '\\\\Users\\\\Administrator',\n",
      "           'IPY_INTERRUPT_EVENT': '2268',\n",
      "           'JD2_HOME': 'C:\\\\Users\\\\Administrator\\\\AppData\\\\Local\\\\JDownloader '\n",
      "                       'v2.0',\n",
      "           'JPY_INTERRUPT_EVENT': '2268',\n",
      "           'JPY_PARENT_PID': '2264',\n",
      "           'LOCALAPPDATA': 'C:\\\\Users\\\\Administrator\\\\AppData\\\\Local',\n",
      "           'LOGONSERVER': '\\\\\\\\STUDENT-LAPTOP',\n",
      "           'MPLBACKEND': 'module://ipykernel.pylab.backend_inline',\n",
      "           'NUMBER_OF_PROCESSORS': '4',\n",
      "           'OPENVC_DIR': 'C:\\\\opencv\\\\build\\\\x86\\\\vc12',\n",
      "           'OS': 'Windows_NT',\n",
      "           'PAGER': 'cat',\n",
      "           'PATH': 'C:\\\\Program Files (x86)\\\\Common '\n",
      "                   'Files\\\\Oracle\\\\Java\\\\javapath;C:\\\\Program '\n",
      "                   'Files\\\\Python\\\\Python 3.8\\\\Scripts\\\\;C:\\\\Program '\n",
      "                   'Files\\\\Python\\\\Python '\n",
      "                   '3.8\\\\;C:\\\\Rtools\\\\bin;C:\\\\ProgramData\\\\Oracle\\\\Java\\\\javapath;C:\\\\oraclexe\\\\app\\\\oracle\\\\product\\\\11.2.0\\\\server\\\\bin;C:\\\\Windows\\\\system32;C:\\\\Windows;C:\\\\Windows\\\\System32\\\\Wbem;C:\\\\Windows\\\\System32\\\\WindowsPowerShell\\\\v1.0\\\\;C:\\\\Program '\n",
      "                   'Files\\\\Microsoft SQL '\n",
      "                   'Server\\\\110\\\\Tools\\\\Binn\\\\;C:\\\\Program Files '\n",
      "                   '(x86)\\\\MySQL\\\\MySQL Fabric 1.5.4 & MySQL Utilities 1.5.4 '\n",
      "                   '1.5\\\\;C:\\\\Program Files (x86)\\\\MySQL\\\\MySQL Fabric 1.5.4 & '\n",
      "                   'MySQL Utilities 1.5.4 1.5\\\\Doctrine extensions for '\n",
      "                   'PHP\\\\;c:\\\\ant\\\\bin;%OPENCV_DIR%\\\\bin;C:\\\\Program Files '\n",
      "                   '(x86)\\\\GtkSharp\\\\2.12\\\\bin;C:\\\\Program Files '\n",
      "                   '(x86)\\\\Bitvise SSH Client;C:\\\\Program '\n",
      "                   'Files\\\\MATLAB\\\\MATLAB Production '\n",
      "                   'Server\\\\R2015a\\\\runtime\\\\win64;C:\\\\Program '\n",
      "                   'Files\\\\MATLAB\\\\MATLAB Production '\n",
      "                   'Server\\\\R2015a\\\\bin;C:\\\\Program Files\\\\MATLAB\\\\MATLAB '\n",
      "                   'Production Server\\\\R2015a\\\\polyspace\\\\bin;C:\\\\Program '\n",
      "                   'Files (x86)\\\\Smart '\n",
      "                   'Projects\\\\IsoBuster;C:\\\\Windows\\\\System32\\\\WindowsPowerShell\\\\v1.0\\\\;C:\\\\Program '\n",
      "                   'Files\\\\nodejs\\\\;C:\\\\Program '\n",
      "                   'Files\\\\Git\\\\cmd;C:\\\\Python27\\\\Lib\\\\site-packages\\\\PyQt4;C:\\\\Python27;C:\\\\Python27\\\\DLLs;C:\\\\Python27\\\\Scripts;C:\\\\Python27\\\\gnuplot\\\\binary;D:\\\\pythonxy\\\\SciTE-3.5.1-4;D:\\\\pythonxy\\\\console;C:\\\\Program '\n",
      "                   'Files '\n",
      "                   '(x86)\\\\Nmap;C:\\\\Users\\\\Administrator\\\\AppData\\\\Local\\\\Programs\\\\Microsoft '\n",
      "                   'VS '\n",
      "                   'Code\\\\bin;C:\\\\Users\\\\Administrator\\\\AppData\\\\Roaming\\\\npm',\n",
      "           'PATHEXT': '.COM;.EXE;.BAT;.CMD;.VBS;.VBE;.JS;.JSE;.WSF;.WSH;.MSC;.PY;.PYW',\n",
      "           'PROCESSOR_ARCHITECTURE': 'AMD64',\n",
      "           'PROCESSOR_IDENTIFIER': 'Intel64 Family 6 Model 61 Stepping 4, '\n",
      "                                   'GenuineIntel',\n",
      "           'PROCESSOR_LEVEL': '6',\n",
      "           'PROCESSOR_REVISION': '3d04',\n",
      "           'PROG27B48B2C052': '1',\n",
      "           'PROGRAMDATA': 'C:\\\\ProgramData',\n",
      "           'PROGRAMFILES': 'C:\\\\Program Files',\n",
      "           'PROGRAMFILES(X86)': 'C:\\\\Program Files (x86)',\n",
      "           'PROGRAMW6432': 'C:\\\\Program Files',\n",
      "           'PROMPT': '$P$G',\n",
      "           'PSMODULEPATH': 'C:\\\\Program '\n",
      "                           'Files\\\\WindowsPowerShell\\\\Modules;C:\\\\Windows\\\\system32\\\\WindowsPowerShell\\\\v1.0\\\\Modules',\n",
      "           'PUBLIC': 'C:\\\\Users\\\\Public',\n",
      "           'SESSIONNAME': 'Console',\n",
      "           'SPARK_HOME': 'C:\\\\spark\\\\spark-3.1.1-bin-hadoop2.7',\n",
      "           'SYSTEMDRIVE': 'C:',\n",
      "           'SYSTEMROOT': 'C:\\\\Windows',\n",
      "           'TEMP': 'C:\\\\Users\\\\ADMINI~1\\\\AppData\\\\Local\\\\Temp',\n",
      "           'TERM': 'xterm-color',\n",
      "           'TMP': 'C:\\\\Users\\\\ADMINI~1\\\\AppData\\\\Local\\\\Temp',\n",
      "           'TVT': 'C:\\\\Program Files (x86)\\\\Lenovo',\n",
      "           'USERDOMAIN': 'student-laptop',\n",
      "           'USERNAME': 'Administrator',\n",
      "           'USERPROFILE': 'C:\\\\Users\\\\Administrator',\n",
      "           'VBOX_MSI_INSTALL_PATH': 'C:\\\\Program Files\\\\Oracle\\\\VirtualBox\\\\',\n",
      "           'VS120COMNTOOLS': 'C:\\\\Program Files (x86)\\\\Microsoft Visual Studio '\n",
      "                             '12.0\\\\Common7\\\\Tools\\\\',\n",
      "           'VS140COMNTOOLS': 'C:\\\\Program Files (x86)\\\\Microsoft Visual Studio '\n",
      "                             '14.0\\\\Common7\\\\Tools\\\\',\n",
      "           'WINDIR': 'C:\\\\Windows',\n",
      "           'WINDOWS_TRACING_FLAGS': '3',\n",
      "           'WINDOWS_TRACING_LOGFILE': 'C:\\\\BVTBin\\\\Tests\\\\installpackage\\\\csilogfile.log'},\n",
      " 'decodekey': <class 'str'>,\n",
      " 'decodevalue': <class 'str'>,\n",
      " 'encodekey': <function _createenviron.<locals>.encodekey at 0x0000000001E59700>,\n",
      " 'encodevalue': <function _createenviron.<locals>.check_str at 0x0000000001E59670>,\n",
      " 'putenv': <built-in function putenv>,\n",
      " 'unsetenv': <function <lambda> at 0x0000000001E31310>}\n"
     ]
    }
   ],
   "source": [
    "# We can check all the environment variables with the following:\n",
    "import os\n",
    "import pprint\n",
    "pprint.pprint(os.environ.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\spark\\spark-3.1.1-bin-hadoop2.7\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(os.environ['SPARK_HOME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubernetes_master_port = \"6443\"\n",
    "kubernetes_master_ip = \"15.4.7.11\"\n",
    "spark_master_url = \"k8s://https://{0}:{1}\".format(kubernetes_master_ip, kubernetes_master_port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x5defca0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkConf = pyspark.SparkConf()\n",
    "sparkConf.setMaster(spark_master_url)\n",
    "sparkConf.setAppName(\"spark\")\n",
    "#sparkConf.set(\"spark.kubernetes.container.image\", \"pidocker-docker-registry.default.svc.cluster.local:5000/my-spark-py:v2.4.4\")\n",
    "sparkConf.set(\"spark.kubernetes.namespace\", \"spark\")\n",
    "sparkConf.set(\"spark.executor.instances\", \"7\")\n",
    "sparkConf.set(\"spark.executor.cores\", \"2\")\n",
    "sparkConf.set(\"spark.driver.memory\", \"512m\")\n",
    "sparkConf.set(\"spark.executor.memory\", \"512m\")\n",
    "sparkConf.set(\"spark.kubernetes.pyspark.pythonVersion\", \"3\")\n",
    "sparkConf.set(\"spark.kubernetes.authenticate.driver.serviceAccountName\", \"spark\")\n",
    "sparkConf.set(\"spark.kubernetes.authenticate.serviceAccountName\", \"spark\")\n",
    "sparkConf.set(\"spark.driver.port\", \"29413\")\n",
    "sparkConf.set(\"spark.driver.host\", \"my-notebook-deployment.spark.svc.cluster.local\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.nio.channels.UnresolvedAddressException\r\n\tat sun.nio.ch.Net.checkAddress(Unknown Source)\r\n\tat sun.nio.ch.ServerSocketChannelImpl.bind(Unknown Source)\r\n\tat io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:134)\r\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:550)\r\n\tat io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1334)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:506)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:491)\r\n\tat io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:973)\r\n\tat io.netty.channel.AbstractChannel.bind(AbstractChannel.java:248)\r\n\tat io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:356)\r\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\r\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\r\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\r\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\r\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\r\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-c2f5d539b7f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mspark\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparkContext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-3.1.1-bin-hadoop2.7\\python\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    226\u001b[0m                             \u001b[0msparkConf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m                         \u001b[1;31m# This SparkContext may be an existing one.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 228\u001b[1;33m                         \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    229\u001b[0m                     \u001b[1;31m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m                     \u001b[1;31m# by all sessions.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-3.1.1-bin-hadoop2.7\\python\\pyspark\\context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    382\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 384\u001b[1;33m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    385\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-3.1.1-bin-hadoop2.7\\python\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    144\u001b[0m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001b[0m\u001b[0;32m    147\u001b[0m                           conf, jsc, profiler_cls)\n\u001b[0;32m    148\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-3.1.1-bin-hadoop2.7\\python\\pyspark\\context.py\u001b[0m in \u001b[0;36m_do_init\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m         \u001b[1;31m# Create the Java SparkContext through Py4J\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjsc\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m         \u001b[1;31m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-3.1.1-bin-hadoop2.7\\python\\pyspark\\context.py\u001b[0m in \u001b[0;36m_initialize_context\u001b[1;34m(self, jconf)\u001b[0m\n\u001b[0;32m    319\u001b[0m         \u001b[0mInitialize\u001b[0m \u001b[0mSparkContext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mto\u001b[0m \u001b[0mallow\u001b[0m \u001b[0msubclass\u001b[0m \u001b[0mspecific\u001b[0m \u001b[0minitialization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m         \"\"\"\n\u001b[1;32m--> 321\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mJavaSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-3.1.1-bin-hadoop2.7\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1566\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1567\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1568\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1569\u001b[0m             answer, self._gateway_client, None, self._fqn)\n\u001b[0;32m   1570\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-3.1.1-bin-hadoop2.7\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.nio.channels.UnresolvedAddressException\r\n\tat sun.nio.ch.Net.checkAddress(Unknown Source)\r\n\tat sun.nio.ch.ServerSocketChannelImpl.bind(Unknown Source)\r\n\tat io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:134)\r\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:550)\r\n\tat io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1334)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:506)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:491)\r\n\tat io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:973)\r\n\tat io.netty.channel.AbstractChannel.bind(AbstractChannel.java:248)\r\n\tat io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:356)\r\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\r\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\r\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\r\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\r\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\r\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1000\n",
    "def inside(p):\n",
    "    x, y = random.random(), random.random()\n",
    "    return x*x + y*y < 1\n",
    "count = sc.parallelize(range(0, num_samples)).filter(inside).count()\n",
    "pi = 4 * count / num_samples\n",
    "print(pi)\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
