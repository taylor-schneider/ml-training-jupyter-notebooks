{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "312f4a72-17a6-4937-b72d-a84fe4c0b2c2",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "In order to be able to submit python code to generate spark workloads we need to setup the following prerequisites:\n",
    "1. Install Java\n",
    "2. Install Apache Spark\n",
    "3. Install Programming Language\n",
    "4. Install Language Bindings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3f4d5a-d95f-4b95-98b2-e1baeb8b6cc3",
   "metadata": {},
   "source": [
    "# 1. Install Java"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0deeb83-68be-415c-ae36-d5de4d65d7de",
   "metadata": {},
   "source": [
    "According to the documentation Apark 3.1.1 requires Java 8/11. In the case of the openjdk, we will see a version of 1.8.x coresponding to Oracle version 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84e9621e-4a7a-41c8-bf6a-fb0955fa1801",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "java version \"1.8.0_291\"\n",
      "Java(TM) SE Runtime Environment (build 1.8.0_291-b10)\n",
      "Java HotSpot(TM) 64-Bit Server VM (build 25.291-b10, mixed mode)\n"
     ]
    }
   ],
   "source": [
    "# Check the java version\n",
    "! java -version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9780464a-5136-4a9f-ad35-62a3e6480978",
   "metadata": {},
   "source": [
    "# 2. Install Apache spark\n",
    "Apache Spark is supplied as an archive file as opposed to an installation program (like an .exe, .msi, .rpm, etc). The archive needs to be downloaded and extracted to a directory location with no spaces.\n",
    "\n",
    "In my case, the archive has been extracted to the following directory:\n",
    "```\n",
    "c:\\spark\\spark-3.1.1-bin-hadoop2.7\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f95baa-02fd-430c-ad70-f816886dcd46",
   "metadata": {},
   "source": [
    "# 3. Install Programming Language\n",
    "Accodring to the documentation Spark has the following compatabilities for it's landuage bindings:\n",
    "- Scala 2.12\n",
    "- Python 3.6+\n",
    "- R 3.5+\n",
    "\n",
    "Insure a compatable version is installed. In our case we are using python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3c4a11e-c72d-4b6d-9b4a-36a24eb3bdc7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.8 (tags/v3.6.8:3c6b436a57, Dec 24 2018, 00:16:47) [MSC v.1916 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print (sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69469fc-62a6-4f67-974a-1ec55eedf60f",
   "metadata": {},
   "source": [
    "# 4. Install Language Bindings\n",
    "There are a few python libraries we will be using today:\n",
    "- **findspark** - a utility which adds spark to the PATH variable. By doing so, it allows the pyspark library to find and use the spark libraries and binaries.\n",
    "- **pyspark** - the python spark library which gives us access to spark through python.\n",
    "- **py4j** - a library which enables Python programs running in a Python interpreter to dynamically access Java objects in a Java Virtual Machine. This library is consumed by pyspark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df21dc79-6ef0-4df6-8892-34bd05e8d151",
   "metadata": {},
   "source": [
    "We can check the installed version of these libraries with the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37e1b130-73c0-400b-97c6-b254e9a68284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "findspark           1.4.2\n"
     ]
    }
   ],
   "source": [
    "# Check if pyspark is intalled\n",
    "\n",
    "! pip list | findstr \"findspark\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff2b0be8-e3f1-4e67-9836-828bac9205f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyspark             3.1.1\n"
     ]
    }
   ],
   "source": [
    "# Check if pyspark is intalled\n",
    "\n",
    "! pip list | findstr \"pyspark\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67c2ca36-70ec-4ef8-9880-19935bdaf9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "py4j                0.10.9\n"
     ]
    }
   ],
   "source": [
    "# Check if py4j is intalled\n",
    "\n",
    "! pip list | findstr \"py4j\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
