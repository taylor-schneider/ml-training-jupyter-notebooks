{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "Previously we have seen examples of running the k-means algorithm provided by scikit-learn. In this notebook we are going to look at the Apache Spark MLib implimentation instead. Unlike the models packaged with scikit-learn, Apache Spark models are bbilt to be distributed and can parallelize calculations.\n",
    "\n",
    "It assumes you have already read the following notebooks:\n",
    "- [Install Apache Spark Prerequisites](Install%20Apache%20Spark%20Prerequisites.ipynb)\n",
    "- [Spark Pi - The Hello World Example For Apache spark](Spark%20Pi%20-%20The%20Hello%20World%20Example%20For%20Apache%20spark.ipynb)\n",
    "- [Intro To Koalas](Intro%20To%20Koalas.ipynb)\n",
    "- <a href=\"../Cluster%20Analysis/K-Means.ipynb\">Cluster Analysis/K-Means</a>\n",
    "\n",
    "The instructions are basically the same as [Running Scikit-Learn Apache Spark](Running%20Scikit-Learn%20Apache%20Spark.ipynb) once you get the kubernetes stuff setup.\n",
    "\n",
    "## Adjenda\n",
    "1. Create SparkContext\n",
    "2. Create Web Server To Host Data\n",
    "3. Load The Data\n",
    "8. Prepare Worker Nodes\n",
    "9. Submit Python Code To Spark Cluster\n",
    "10. Cleanup Spark and Kubernetes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Create SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting SPARK_HOME\n",
      "c:\\spark\\spark-3.1.1-bin-hadoop2.7\n",
      "\n",
      "Running findspark.init() function\n",
      "['c:\\\\spark\\\\spark-3.1.1-bin-hadoop2.7\\\\python', 'c:\\\\spark\\\\spark-3.1.1-bin-hadoop2.7\\\\python\\\\lib\\\\py4j-0.10.9-src.zip', 'c:\\\\program files\\\\python36\\\\python36.zip', 'c:\\\\program files\\\\python36\\\\DLLs', 'c:\\\\program files\\\\python36\\\\lib', 'c:\\\\program files\\\\python36', '', 'c:\\\\program files\\\\python36\\\\lib\\\\site-packages', 'c:\\\\program files\\\\python36\\\\lib\\\\site-packages\\\\win32', 'c:\\\\program files\\\\python36\\\\lib\\\\site-packages\\\\win32\\\\lib', 'c:\\\\program files\\\\python36\\\\lib\\\\site-packages\\\\Pythonwin', 'c:\\\\program files\\\\python36\\\\lib\\\\site-packages\\\\IPython\\\\extensions', 'C:\\\\Users\\\\Administrator\\\\.ipython']\n",
      "\n",
      "Setting PYSPARK_PYTHON\n",
      "/usr/bin/python3\n",
      "\n",
      "Determine IP Of Server\n",
      "The ip was detected as: 15.1.1.122\n",
      "\n",
      "Create SparkContext\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from spark_helper import create_spark_context\n",
    "spark_app_name = \"spark-jupyter-win\"\n",
    "docker_image = \"tschneider/pyspark:v5\"\n",
    "k8_master_ip = \"15.4.7.11\"\n",
    "sc = create_spark_context(spark_app_name, docker_image, k8_master_ip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at kubernetes to see that out worker nodes were created. \n",
    "\n",
    "The first time we create the spark context with a given docker image, the image will need to be downloaded (which takes some time). As a result, we may see the pods with a status of \"ContainerCreating\". In this case, we will need to wait until the containers are in a \"Running\" state.\n",
    "\n",
    "```\n",
    "kubectl -n spark get pod\n",
    "NAME                                        READY   STATUS              RESTARTS   AGE\n",
    "spark-jupyter-win-3ed7f27984f7563a-exec-1   0/1     ContainerCreating   0          12m\n",
    "spark-jupyter-win-3ed7f27984f7563a-exec-2   0/1     ContainerCreating   0          12m\n",
    "spark-jupyter-win-3ed7f27984f7563a-exec-3   0/1     ContainerCreating   0          12m\n",
    "```\n",
    "\n",
    "We can check the status of the docker pull by logging into the container and running the docker pull command to attach to the running process:\n",
    "```\n",
    "kubectl -n spark exec -ti docker pull tschneider/pyspark:v3 docker pull tschneider/pyspark:v4\n",
    "v3: Pulling from tschneider/pyspark\n",
    "2d473b07cdd5: Already exists\n",
    "71d236fb1195: Already exists\n",
    "2e22160d8cab: Already exists\n",
    "e99d962ac218: Pull complete\n",
    "Digest: sha256:eb74701b4ae909c40046ff68b1044b09b11895e175c955dfd8afe9fe680309cf\n",
    "Status: Downloaded newer image for tschneider/pyspark:v3\n",
    "docker.io/tschneider/pyspark:v3\n",
    "[root@os004k8-worker002 ~]# docker pull tschneider/pyspark:v4\n",
    "v4: Pulling from tschneider/pyspark\n",
    "2d473b07cdd5: Already exists\n",
    "71d236fb1195: Already exists\n",
    "2e22160d8cab: Already exists\n",
    "c556a717fe5d: Downloading [=======================>                           ]  578.7MB/1.246GB\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                        READY   STATUS    RESTARTS   AGE\n",
      "spark-jupyter-win-bfbc6179c97163d6-exec-1   1/1     Running   0          36s\n",
      "spark-jupyter-win-bfbc6179c97163d6-exec-2   1/1     Running   0          35s\n",
      "spark-jupyter-win-bfbc6179c97163d6-exec-3   1/1     Running   0          35s\n"
     ]
    }
   ],
   "source": [
    "! kubectl -n spark get pod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Create web server to host data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the current working directory. \n",
    "\n",
    "Note: There is a trick to doing this inside a jupyter notebook and so we will use a special library to get that information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\git\\ml-training-jupyter-notebooks\n"
     ]
    }
   ],
   "source": [
    "import pyprojroot\n",
    "project_root_dir  = pyprojroot.here()\n",
    "print(project_root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the module for the webserver from our utilities directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the module for the web server we wrote\n",
    "import importlib.util\n",
    "spec = importlib.util.spec_from_file_location(\"PythonHttpFileServer\", \"../Utilities/PythonHttpFileServer.py\")\n",
    "PythonHttpFileServer = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(PythonHttpFileServer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure logging so that messages are collected and displayed asynchronously so that the server can run in the background without casuing a jupyter cell to block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Starting server on port 80\n",
      "INFO:root:Web root specified as: C:\\Users\\Administrator\\git\\ml-training-jupyter-notebooks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app 'PythonHttpFileServer' (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:werkzeug: * Running on all addresses.\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "INFO:werkzeug: * Running on http://15.1.1.122:80/ (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "# Configure the logger and log level\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Remove all handlers\n",
    "for handler in logger.handlers: \n",
    "    logger.removeHandler(handler)\n",
    "for handler in logger.handlers: \n",
    "    logger.removeHandler(handler)\n",
    "    \n",
    "# Start the webserver in a thread so the cell is not stuck in a running state\n",
    "import threading\n",
    "web_server_port = 80\n",
    "web_server_args = (web_server_port, project_root_dir)\n",
    "web_server_thread = threading.Thread(target=PythonHttpFileServer.run_server, args=web_server_args)\n",
    "web_server_thread.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Load The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instruct the spark cluster to download a file from the web server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spark_helper import determine_ip_address\n",
    "csv_file_name = \"nasdaq_2019.csv\"\n",
    "ip_address = determine_ip_address()\n",
    "csv_file_url = \"http://{0}:{1}/{2}\".format(ip_address, web_server_port, csv_file_name)\n",
    "sc.addFile(csv_file_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the utility function to convert a date string to a datetime object from our utilities module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the utilities module we wrote\n",
    "import importlib.util\n",
    "spec = importlib.util.spec_from_file_location(\"utilities\", \"../Utilities/utilities.py\")\n",
    "utilities = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(utilities)\n",
    "\n",
    "# Define a mapping to convert our data field to the correct type\n",
    "converter_mapping = {\n",
    "    \"date\": utilities.convert_date_string_to_date\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Load our OHCLV data Into a koalas dataframe and pull out a single day in the say way we would in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks import koalas\n",
    "koalas_dataframe = koalas.read_csv(u\"file:////nasdaq_2019.csv\", converters=converter_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should see the workers download the file in the logs. If we log into the nodes we can see the file is located on the filesystem root."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data loaded into a koalas dataframe we can access the data in the same way we would from a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>interval</th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AABA</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>70.90</td>\n",
       "      <td>71.5200</td>\n",
       "      <td>70.3250</td>\n",
       "      <td>70.57</td>\n",
       "      <td>10234800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAL</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>33.14</td>\n",
       "      <td>33.6632</td>\n",
       "      <td>32.5301</td>\n",
       "      <td>32.88</td>\n",
       "      <td>8995100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAME</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>2.43</td>\n",
       "      <td>2.4300</td>\n",
       "      <td>2.4000</td>\n",
       "      <td>2.40</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAOI</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>10.70</td>\n",
       "      <td>10.8900</td>\n",
       "      <td>10.0100</td>\n",
       "      <td>10.18</td>\n",
       "      <td>883100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAON</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>50.57</td>\n",
       "      <td>50.9850</td>\n",
       "      <td>48.5600</td>\n",
       "      <td>49.73</td>\n",
       "      <td>180200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker interval        date   open     high      low  close    volume\n",
       "0   AABA        D  2019-07-01  70.90  71.5200  70.3250  70.57  10234800\n",
       "1    AAL        D  2019-07-01  33.14  33.6632  32.5301  32.88   8995100\n",
       "2   AAME        D  2019-07-01   2.43   2.4300   2.4000   2.40       500\n",
       "3   AAOI        D  2019-07-01  10.70  10.8900  10.0100  10.18    883100\n",
       "4   AAON        D  2019-07-01  50.57  50.9850  48.5600  49.73    180200"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "koalas_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. We need to prepare our worker nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: We need to install relevant python libraries on the worker nodes. If you do not, you might see an error as follows:\n",
    "```\n",
    "PythonException: \n",
    "  An exception was thrown from the Python worker. Please see the stack trace below.\n",
    "Traceback (most recent call last):\n",
    "  File \"/usr/local/lib/python3.6/site-packages/pyspark/worker.py\", line 588, in main\n",
    "    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n",
    "  File \"/usr/local/lib/python3.6/site-packages/pyspark/worker.py\", line 421, in read_udfs\n",
    "    arg_offsets, f = read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=0)\n",
    "  File \"/usr/local/lib/python3.6/site-packages/pyspark/worker.py\", line 249, in read_single_udf\n",
    "    f, return_type = read_command(pickleSer, infile)\n",
    "  File \"/usr/local/lib/python3.6/site-packages/pyspark/worker.py\", line 69, in read_command\n",
    "    command = serializer._read_with_length(file)\n",
    "  File \"/usr/local/lib/python3.6/site-packages/pyspark/serializers.py\", line 160, in _read_with_length\n",
    "    return self.loads(obj)\n",
    "  File \"/usr/local/lib/python3.6/site-packages/pyspark/serializers.py\", line 430, in loads\n",
    "    return pickle.loads(obj, encoding=encoding)\n",
    "  File \"/usr/local/lib/python3.6/site-packages/pyspark/cloudpickle/cloudpickle.py\", line 562, in subimport\n",
    "    __import__(name)\n",
    "ModuleNotFoundError: No module named 'pandas'\n",
    "```\n",
    "\n",
    "In our case we needed to install pandas, numpy, koalas, scikit-learn, sklearn. If you are unsure of what is installed on your workers, we can log into the kubernetes pods and execute shell commands.\n",
    "\n",
    "Note: We must do this on all workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                        READY   STATUS    RESTARTS   AGE\n",
      "spark-jupyter-win-bfbc6179c97163d6-exec-1   1/1     Running   0          6m37s\n",
      "spark-jupyter-win-bfbc6179c97163d6-exec-2   1/1     Running   0          6m36s\n",
      "spark-jupyter-win-bfbc6179c97163d6-exec-3   1/1     Running   0          6m36s\n"
     ]
    }
   ],
   "source": [
    "! kubectl -n spark get pod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package         Version\n",
      "--------------- -------\n",
      "cycler          0.10.0\n",
      "joblib          1.0.1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unable to use a TTY - input is not a terminal or the right kind of file\n",
      "WARNING: You are using pip version 21.1.1; however, version 21.1.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "kiwisolver      1.3.1\n",
      "kneed           0.7.0\n",
      "koalas          1.8.0\n",
      "matplotlib      3.3.4\n",
      "numpy           1.19.5\n",
      "pandas          1.1.5\n",
      "Pillow          8.2.0\n",
      "pip             21.1.1\n",
      "progressbar     2.5\n",
      "py4j            0.10.9\n",
      "pyarrow         4.0.0\n",
      "pyparsing       2.4.7\n",
      "pyspark         3.1.1\n",
      "python-dateutil 2.8.1\n",
      "pytz            2021.1\n",
      "scikit-learn    0.24.2\n",
      "scipy           1.5.4\n",
      "setuptools      39.2.0\n",
      "six             1.16.0\n",
      "sklearn         0.0\n",
      "threadpoolctl   2.1.0\n"
     ]
    }
   ],
   "source": [
    "! kubectl -n spark exec -ti spark-jupyter-win-bfbc6179c97163d6-exec-1 -- pip3 list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Submit Python Code To Spark Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section of the notebook we are going to apply the kmeans algorithm from sklearn to each date in our koalas_dataframe object.\n",
    "To do this, we are going to write a function that applies the algorithm to a dataframe; the assumption being the dataframe only contains data related to the same date.\n",
    "Note: Most of this is a review and reworking of the content contained in \n",
    "<a href=\"../Cluster%20Analysis/K-Means.ipynb\">Cluster Analysis/K-Means.ipynb</a>.\n",
    "\n",
    "We create our data frame for testing based on a subset of our real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>interval</th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>93620</th>\n",
       "      <td>AABA</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>57.94</td>\n",
       "      <td>57.94</td>\n",
       "      <td>57.94</td>\n",
       "      <td>57.94</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93621</th>\n",
       "      <td>AAL</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>32.11</td>\n",
       "      <td>32.11</td>\n",
       "      <td>32.11</td>\n",
       "      <td>32.11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93622</th>\n",
       "      <td>AAME</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>2.41</td>\n",
       "      <td>2.41</td>\n",
       "      <td>2.41</td>\n",
       "      <td>2.41</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93623</th>\n",
       "      <td>AAOI</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>15.43</td>\n",
       "      <td>15.43</td>\n",
       "      <td>15.43</td>\n",
       "      <td>15.43</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93624</th>\n",
       "      <td>AAON</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>35.06</td>\n",
       "      <td>35.06</td>\n",
       "      <td>35.06</td>\n",
       "      <td>35.06</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ticker interval        date   open   high    low  close  volume\n",
       "93620   AABA        D  2019-01-01  57.94  57.94  57.94  57.94       0\n",
       "93621    AAL        D  2019-01-01  32.11  32.11  32.11  32.11       0\n",
       "93622   AAME        D  2019-01-01   2.41   2.41   2.41   2.41       0\n",
       "93623   AAOI        D  2019-01-01  15.43  15.43  15.43  15.43       0\n",
       "93624   AAON        D  2019-01-01  35.06  35.06  35.06  35.06       0"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "koalas_dataframe.loc[koalas_dataframe[\"date\"] == '2019-01-01'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>interval</th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>96799</th>\n",
       "      <td>AABA</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>56.78</td>\n",
       "      <td>58.01</td>\n",
       "      <td>56.47</td>\n",
       "      <td>57.49</td>\n",
       "      <td>10532400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96800</th>\n",
       "      <td>AAL</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>31.46</td>\n",
       "      <td>32.65</td>\n",
       "      <td>31.05</td>\n",
       "      <td>32.48</td>\n",
       "      <td>5229400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96801</th>\n",
       "      <td>AAME</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>2.43</td>\n",
       "      <td>2.49</td>\n",
       "      <td>2.43</td>\n",
       "      <td>2.49</td>\n",
       "      <td>1700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96802</th>\n",
       "      <td>AAOI</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>15.00</td>\n",
       "      <td>16.29</td>\n",
       "      <td>14.85</td>\n",
       "      <td>15.88</td>\n",
       "      <td>478300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96803</th>\n",
       "      <td>AAON</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>34.57</td>\n",
       "      <td>35.40</td>\n",
       "      <td>34.37</td>\n",
       "      <td>35.07</td>\n",
       "      <td>124800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ticker interval        date   open   high    low  close    volume\n",
       "96799   AABA        D  2019-01-02  56.78  58.01  56.47  57.49  10532400\n",
       "96800    AAL        D  2019-01-02  31.46  32.65  31.05  32.48   5229400\n",
       "96801   AAME        D  2019-01-02   2.43   2.49   2.43   2.49      1700\n",
       "96802   AAOI        D  2019-01-02  15.00  16.29  14.85  15.88    478300\n",
       "96803   AAON        D  2019-01-02  34.57  35.40  34.37  35.07    124800"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_01_02_2019 = koalas_dataframe.loc[koalas_dataframe[\"date\"] == '2019-01-02'].copy()\n",
    "df_01_02_2019.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then write and test our function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "model = KMeans().setK(5).setSeed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_kmeans_on_dataframe(df, column_names=[\"open\"]):\n",
    "    \n",
    "    # Create a copy of our dataframe so we can play around\n",
    "    tmp = df.copy()\n",
    "    columns = column_names.copy()\n",
    "\n",
    "    # IF we only supplied one column name, we will need to create a bogus column\n",
    "    # Kmeans requires a 2D array so we will add a static column\n",
    "    bogus_column = None\n",
    "    if len(column_names) < 2:\n",
    "        bogus_column = \"Y\"\n",
    "        tmp[bogus_column] = [1 for x in range(0, tmp.shape[0])]\n",
    "        columns.append(bogus_column)\n",
    "        \n",
    "    # Set the parameters for our model\n",
    "    # It expects a 2D array where the columns are our features\n",
    "    model_parameters = tmp[[*columns]].to_numpy()\n",
    "    \n",
    "    # Train the model\n",
    "    trained_model = model.fit(model_parameters)\n",
    "    \n",
    "    # Extract the information\n",
    "    cluster_indices = trained_model.predict(model_parameters)\n",
    "    cluster_centroids = [str(trained_model.cluster_centers_[i]) for i in cluster_indices]\n",
    "\n",
    "    tmp[\"cluster_indices\"] = cluster_indices.tolist()\n",
    "    tmp[\"cluster_centroids\"] = cluster_centroids\n",
    "\n",
    "    \n",
    "    # Determine which columns we want to return\n",
    "    columns = tmp.columns.to_list()\n",
    "    if bogus_column in columns:\n",
    "        columns.remove(bogus_column)\n",
    "    \n",
    "    return tmp[[*columns]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>interval</th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>cluster_indices</th>\n",
       "      <th>cluster_centroids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>98639</th>\n",
       "      <td>MIND</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>2.570</td>\n",
       "      <td>3.250</td>\n",
       "      <td>2.52</td>\n",
       "      <td>3.200</td>\n",
       "      <td>24700</td>\n",
       "      <td>1</td>\n",
       "      <td>[12.8841972  13.16998677]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99694</th>\n",
       "      <td>TUSA</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>29.060</td>\n",
       "      <td>29.450</td>\n",
       "      <td>29.06</td>\n",
       "      <td>29.450</td>\n",
       "      <td>1300</td>\n",
       "      <td>1</td>\n",
       "      <td>[12.8841972  13.16998677]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97315</th>\n",
       "      <td>CELGZ</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>0.525</td>\n",
       "      <td>0.637</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.637</td>\n",
       "      <td>7500</td>\n",
       "      <td>1</td>\n",
       "      <td>[12.8841972  13.16998677]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98029</th>\n",
       "      <td>GFN</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>10.030</td>\n",
       "      <td>10.620</td>\n",
       "      <td>9.70</td>\n",
       "      <td>9.980</td>\n",
       "      <td>88300</td>\n",
       "      <td>1</td>\n",
       "      <td>[66.30115483 67.14421858]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98371</th>\n",
       "      <td>JBSS</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>55.200</td>\n",
       "      <td>57.100</td>\n",
       "      <td>53.12</td>\n",
       "      <td>54.980</td>\n",
       "      <td>46200</td>\n",
       "      <td>3</td>\n",
       "      <td>[12.8841972  13.16998677]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ticker interval        date    open    high    low   close  volume  cluster_indices          cluster_centroids\n",
       "98639   MIND        D  2019-01-02   2.570   3.250   2.52   3.200   24700                1  [12.8841972  13.16998677]\n",
       "99694   TUSA        D  2019-01-02  29.060  29.450  29.06  29.450    1300                1  [12.8841972  13.16998677]\n",
       "97315  CELGZ        D  2019-01-02   0.525   0.637   0.46   0.637    7500                1  [12.8841972  13.16998677]\n",
       "98029    GFN        D  2019-01-02  10.030  10.620   9.70   9.980   88300                1  [66.30115483 67.14421858]\n",
       "98371   JBSS        D  2019-01-02  55.200  57.100  53.12  54.980   46200                3  [12.8841972  13.16998677]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perform_kmeans_on_dataframe(df_01_02_2019, column_names=[\"open\", \"close\"]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\spark\\spark-3.1.1-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\group_ops.py:84: UserWarning: It is preferred to use 'applyInPandas' over this API. This API will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  \"more details.\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total calculation time: 2.606149s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>interval</th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>cluster_indices</th>\n",
       "      <th>cluster_centroids</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2019-01-01</th>\n",
       "      <th>93620</th>\n",
       "      <td>AABA</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>57.94</td>\n",
       "      <td>57.94</td>\n",
       "      <td>57.94</td>\n",
       "      <td>57.94</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[67.53273665  1.        ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93621</th>\n",
       "      <td>AAL</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>32.11</td>\n",
       "      <td>32.11</td>\n",
       "      <td>32.11</td>\n",
       "      <td>32.11</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>[13.05660117  1.        ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93622</th>\n",
       "      <td>AAME</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>2.41</td>\n",
       "      <td>2.41</td>\n",
       "      <td>2.41</td>\n",
       "      <td>2.41</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>[13.05660117  1.        ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93623</th>\n",
       "      <td>AAOI</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>15.43</td>\n",
       "      <td>15.43</td>\n",
       "      <td>15.43</td>\n",
       "      <td>15.43</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>[13.05660117  1.        ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93624</th>\n",
       "      <td>AAON</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>35.06</td>\n",
       "      <td>35.06</td>\n",
       "      <td>35.06</td>\n",
       "      <td>35.06</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>[13.05660117  1.        ]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ticker interval        date   open   high    low  close  volume  cluster_indices          cluster_centroids\n",
       "date                                                                                                                        \n",
       "2019-01-01 93620   AABA        D  2019-01-01  57.94  57.94  57.94  57.94       0                0  [67.53273665  1.        ]\n",
       "           93621    AAL        D  2019-01-01  32.11  32.11  32.11  32.11       0                2  [13.05660117  1.        ]\n",
       "           93622   AAME        D  2019-01-01   2.41   2.41   2.41   2.41       0                2  [13.05660117  1.        ]\n",
       "           93623   AAOI        D  2019-01-01  15.43  15.43  15.43  15.43       0                2  [13.05660117  1.        ]\n",
       "           93624   AAON        D  2019-01-01  35.06  35.06  35.06  35.06       0                2  [13.05660117  1.        ]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "result = df_01_01_2019.groupby(\"date\").apply(perform_kmeans_on_dataframe, column_names=[\"open\"])\n",
    "later = datetime.now()\n",
    "difference = (later - now).total_seconds()\n",
    "print(\"total calculation time: {0}s\".format(difference))\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run this function against out entire dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\spark\\spark-3.1.1-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\group_ops.py:84: UserWarning: It is preferred to use 'applyInPandas' over this API. This API will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  \"more details.\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total calculation time: 2.423139s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>interval</th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>cluster_indices</th>\n",
       "      <th>cluster_centroids</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2019-01-01</th>\n",
       "      <th>93620</th>\n",
       "      <td>AABA</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>57.94</td>\n",
       "      <td>57.94</td>\n",
       "      <td>57.94</td>\n",
       "      <td>57.94</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[67.53273665  1.        ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93621</th>\n",
       "      <td>AAL</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>32.11</td>\n",
       "      <td>32.11</td>\n",
       "      <td>32.11</td>\n",
       "      <td>32.11</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>[13.05660117  1.        ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93622</th>\n",
       "      <td>AAME</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>2.41</td>\n",
       "      <td>2.41</td>\n",
       "      <td>2.41</td>\n",
       "      <td>2.41</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>[13.05660117  1.        ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93623</th>\n",
       "      <td>AAOI</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>15.43</td>\n",
       "      <td>15.43</td>\n",
       "      <td>15.43</td>\n",
       "      <td>15.43</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>[13.05660117  1.        ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93624</th>\n",
       "      <td>AAON</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>35.06</td>\n",
       "      <td>35.06</td>\n",
       "      <td>35.06</td>\n",
       "      <td>35.06</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>[13.05660117  1.        ]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ticker interval        date   open   high    low  close  volume  cluster_indices          cluster_centroids\n",
       "date                                                                                                                        \n",
       "2019-01-01 93620   AABA        D  2019-01-01  57.94  57.94  57.94  57.94       0                0  [67.53273665  1.        ]\n",
       "           93621    AAL        D  2019-01-01  32.11  32.11  32.11  32.11       0                2  [13.05660117  1.        ]\n",
       "           93622   AAME        D  2019-01-01   2.41   2.41   2.41   2.41       0                2  [13.05660117  1.        ]\n",
       "           93623   AAOI        D  2019-01-01  15.43  15.43  15.43  15.43       0                2  [13.05660117  1.        ]\n",
       "           93624   AAON        D  2019-01-01  35.06  35.06  35.06  35.06       0                2  [13.05660117  1.        ]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "result = koalas_dataframe.groupby(\"date\").apply(perform_kmeans_on_dataframe, column_names=[\"open\"])\n",
    "later = datetime.now()\n",
    "difference = (later - now).total_seconds()\n",
    "print(\"total calculation time: {0}s\".format(difference))\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that computing every date is almost as fast as computing a single date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Cleanup Spark Cluster On Kubernetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                        READY   STATUS        RESTARTS   AGE\n",
      "spark-jupyter-win-bfbc6179c97163d6-exec-5   0/1     Terminating   0          47m\n",
      "spark-jupyter-win-bfbc6179c97163d6-exec-6   0/1     Terminating   0          47m\n"
     ]
    }
   ],
   "source": [
    "! kubectl -n spark get pod"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
