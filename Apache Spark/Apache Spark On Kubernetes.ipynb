{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Spark On Kubernetes\n",
    "\n",
    "Kubernetes is an open source container orchestration system created by google and maintained by the Cloud Native Computing Foundation.\n",
    "Kubernetes allows us to efficiently manage our Apace Spark infrastructure by hosting it inside containers.\n",
    "As of March 20th 2020, with release of Apache Spark 3.1, there is an even tighter integration between the two projects..\n",
    "\n",
    "\n",
    "The classical hello world example on most Spark tutorials will compute the value of pi. This is commonly referred to as \"Spark Pi\".\n",
    "In this notebook we will see how we can connect to apache spark cluster, dynamically spin up a Spark cluster, submit the Spark Pi workload to it, and cleanup automatically.\n",
    "\n",
    "## Adjenda\n",
    "1. Configure Kubernetes Cluster For Spark\n",
    "2. Install the Kubectl CLI for Kubernetes\n",
    "3. Install Apache Spark Prereqs\n",
    "4. Install Python Libraries\n",
    "5. Download and install Apache Spark\n",
    "6. Set Environment variables\n",
    "7. Create SparKConf\n",
    "8. Create SparkContext\n",
    "9. Submit Python Code To Spark Cluster\n",
    "10. Cleanup Spark and Kubernetes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configure Kubernetes Cluster For Spark\n",
    "In order for our kubernetes cluster to successfully run a spark cluster we need to do a few things:\n",
    "1. Configure RBAC - We will need to set permissions so that our jupyter notebook and spark components have the appropriate permissions.\n",
    "2. Build containers - We will need to build the contaienrs which host our spark cluster nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Configure Kubernetes RBAC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Build Spark Containers For Kubernetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Install and Configure Kubectl\n",
    "Kubectl is the CLI for kubernetes. It will allow our jupyter notebook to connect to the kubernetes cluster and spin up containers to run our Spark work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Install Kubectl\n",
    "There are a number of ways to install kubectl. The easiest and fully featured way is to use the chocolatey installation process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://kubernetes.io/docs/tasks/tools/install-kubectl-windows/#install-on-windows-using-chocolatey-or-scoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client Version: version.Info{Major:\"1\", Minor:\"21\", GitVersion:\"v1.21.0\", GitCommit:\"cb303e613a121a29364f75cc67d3d580833a7479\", GitTreeState:\"clean\", BuildDate:\"2021-04-08T16:31:21Z\", GoVersion:\"go1.16.1\", Compiler:\"gc\", Platform:\"windows/amd64\"}\n"
     ]
    }
   ],
   "source": [
    "! kubectl version --client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Configure Kubectl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cd %USERPROFILE% & mkdir .kube 2> NUL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the kubeconfi file... We can copy it from the master\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kubernetes control plane is running at https://15.4.7.11:6443\n",
      "CoreDNS is running at https://15.4.7.11:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\n",
      "\n",
      "To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
     ]
    }
   ],
   "source": [
    "! kubectl cluster-info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                           STATUS   ROLES                  AGE   VERSION\n",
      "os004k8-master001.foobar.com   Ready    control-plane,master   16d   v1.21.0\n",
      "os004k8-worker001.foobar.com   Ready    <none>                 16d   v1.21.0\n",
      "os004k8-worker002.foobar.com   Ready    <none>                 16d   v1.21.0\n",
      "os004k8-worker003.foobar.com   Ready    <none>                 16d   v1.21.0\n"
     ]
    }
   ],
   "source": [
    "! kubectl get node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Install Apache Spark Prerequisites\n",
    "According to the documentation Apark 3.1.1 requires Java 8/11. In the case of the openjdk, we will see a version of 1.8.x coresponding to Oracle version 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "java version \"1.8.0_291\"\n",
      "Java(TM) SE Runtime Environment (build 1.8.0_291-b10)\n",
      "Java HotSpot(TM) 64-Bit Server VM (build 25.291-b10, mixed mode)\n"
     ]
    }
   ],
   "source": [
    "# Check the java version\n",
    "! java -version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accodring to the documentation Spark has the following compatabilities for it's landuage bindings:\n",
    "- Scala 2.12\n",
    "- Python 3.6+\n",
    "- R 3.5+\n",
    "\n",
    "As mentioned in section 1.2, the Spark nodes have to have the same major version of Python as our Jupyter node. Make sure these versions match!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.8 (tags/v3.6.8:3c6b436a57, Dec 24 2018, 00:16:47) [MSC v.1916 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print (sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Install Python packages\n",
    "There are two python libraries we will be using today:\n",
    "- **findspark** - a utility which adds spark to the PATH variable. By doing so, it allows the pyspark library to find and use the spark libraries and binaries.\n",
    "- **pyspark** - the python spark library which gives us access to spark through python.\n",
    "- **py4j** - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "findspark           1.4.2\n"
     ]
    }
   ],
   "source": [
    "# Check if pyspark is intalled\n",
    "\n",
    "! pip list | findstr \"findspark\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyspark             3.1.1\n"
     ]
    }
   ],
   "source": [
    "# Check if pyspark is intalled\n",
    "\n",
    "! pip list | findstr \"pyspark\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "py4j                0.10.9\n"
     ]
    }
   ],
   "source": [
    "# Check if py4j is intalled\n",
    "\n",
    "! pip list | findstr \"py4j\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Download and Install Apache Spark\n",
    "The pyspark python package (which we previously installed) relies on the spark binaries (java jar files) to be installed and available.\n",
    "It also requires that there be no spaces in the path to the spark application.\n",
    "The spark binaries are available as an archive file at the [Spark Downloads Page](https://spark.apache.org/downloads.html).\n",
    "In our case we will download the [spark-3.1.1-bin-hadoop2.7.tgz](https://apache.osuosl.org/spark/spark-3.1.1/spark-3.1.1-bin-hadoop2.7.tgz)\n",
    "We will extract the archive at the C:\\spark location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Set Environment Variables\n",
    "We can use the os package to set environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. Set SPARK_HOME variable\n",
    "This variable configures our system to understand where spark is installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['SPARK_HOME'] = \"c:\\\\spark\\\\spark-3.1.1-bin-hadoop2.7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\spark\\spark-3.1.1-bin-hadoop2.7\n"
     ]
    }
   ],
   "source": [
    "print(os.environ['SPARK_HOME'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. Run findspark.init() to add Spark to PATH\n",
    "PySpark isn't on sys.path by default, but that doesn't mean it can't be used as a regular library. You can address this by either symlinking pyspark into your site-packages, or adding pyspark to sys.path at runtime. findspark does the latter.\n",
    "\n",
    "https://github.com/minrk/findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c:\\\\spark\\\\spark-3.1.1-bin-hadoop2.7\\\\python', 'c:\\\\spark\\\\spark-3.1.1-bin-hadoop2.7\\\\python\\\\lib\\\\py4j-0.10.9-src.zip', 'c:\\\\program files\\\\python36\\\\python36.zip', 'c:\\\\program files\\\\python36\\\\DLLs', 'c:\\\\program files\\\\python36\\\\lib', 'c:\\\\program files\\\\python36', '', 'c:\\\\program files\\\\python36\\\\lib\\\\site-packages', 'c:\\\\program files\\\\python36\\\\lib\\\\site-packages\\\\win32', 'c:\\\\program files\\\\python36\\\\lib\\\\site-packages\\\\win32\\\\lib', 'c:\\\\program files\\\\python36\\\\lib\\\\site-packages\\\\Pythonwin', 'c:\\\\program files\\\\python36\\\\lib\\\\site-packages\\\\IPython\\\\extensions', 'C:\\\\Users\\\\Administrator\\\\.ipython']\n"
     ]
    }
   ],
   "source": [
    "# Print the PATH variable to show the spark directory is set\n",
    "import sys\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3. Set PYSPARK_PYTHON variable\n",
    "This variable configures spark to understand where python is installed on the spark nodes. Recall, these are the linux containers we built earlier. By default, the local windows file path may be set, but this will not work. If improperly confiugred we may see an error like this one:\n",
    "```\n",
    "Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n",
    ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 0.0 failed 4 times, most recent failure: Lost task 2.3 in stage 0.0 (TID 17) (10.36.0.2 executor 1): java.io.IOException: Cannot run program \"c:\\program files\\python36\\python.exe\": error=2, No such file or directory\n",
    "```\n",
    "We need to set this variable equal to path of python on the container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_PYTHON'] = \"/usr/bin/python3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/python3\n"
     ]
    }
   ],
   "source": [
    "print(os.environ['PYSPARK_PYTHON'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Create SparKConf Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some vars to specify where the kubernetes master is\n",
    "kubernetes_master_ip = \"15.4.7.11\"\n",
    "kubernetes_master_port = \"6443\"\n",
    "spark_master_url = \"k8s://https://{0}:{1}\".format(kubernetes_master_ip, kubernetes_master_port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x563d048>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wire up the SparkConf object\n",
    "sparkConf = pyspark.SparkConf()\n",
    "sparkConf.setMaster(spark_master_url)\n",
    "\n",
    "sparkConf.setAppName(\"spark-jupyter-win\")\n",
    "\n",
    "sparkConf.set(\"spark.submit.deploy.mode\", \"cluster\")\n",
    "sparkConf.set(\"spark.kubernetes.container.image\", \"tschneider/pyspark:v2\") \n",
    "sparkConf.set(\"spark.kubernetes.namespace\", \"spark\")\n",
    "sparkConf.set(\"spark.kubernetes.pyspark.pythonVersion\", \"3\")\n",
    "sparkConf.set(\"spark.kubernetes.authenticate.driver.serviceAccountName\", \"spark-sa\")\n",
    "sparkConf.set(\"spark.kubernetes.authenticate.serviceAccountName\", \"spark-sa\")\n",
    "\n",
    "sparkConf.set(\"spark.executor.instances\", \"3\")\n",
    "sparkConf.set(\"spark.executor.cores\", \"2\")\n",
    "sparkConf.set(\"spark.executor.memory\", \"1024m\")\n",
    "sparkConf.set(\"spark.driver.memory\", \"1024m\")\n",
    "\n",
    "sparkConf.set(\"spark.driver.host\", \"15.1.1.34\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Create SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at kubernetes to see that out worker nodes were created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                        READY   STATUS    RESTARTS   AGE\n",
      "spark-jupyter-win-78f2567966e1afb2-exec-1   1/1     Running   0          14s\n",
      "spark-jupyter-win-78f2567966e1afb2-exec-2   1/1     Running   0          13s\n",
      "spark-jupyter-win-78f2567966e1afb2-exec-3   1/1     Running   0          13s\n"
     ]
    }
   ],
   "source": [
    "! kubectl -n spark get pod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Submit Python Code To Spark Cluster\n",
    "## 9.1. The Spark Pi Problem\n",
    "We are going to run the Spark Pi example which uses a \"Monte Carlo Method\" and the \"Circle Method\" to approximate the value of pi. \n",
    "In short; We will generate a large number or random points within a unit square and determine the ratio of the points within the unit circle; This will give us an approximation for the value of pi.\n",
    "\n",
    "Recall that the area of a circle is defined as:\n",
    "$$ A_c = \\pi r^2 $$\n",
    "Considering we are dealing with a unit circle, we have $r = 0.5$, and therfore\n",
    "\n",
    "$$ A_c = 0.5^2 \\pi  = 0.25\\pi = \\frac{\\pi}{4}$$\n",
    "Recall that the area of a square is defined as:\n",
    "$$ A_s = l^2 = 1^2 = 1 $$\n",
    "If we divide the area of the circle (smaller) by the area of the square (larger) we have the following equality:\n",
    "\n",
    "$$ \\frac{A_c}{A_s} = \\frac{\\pi / 4}{1} = \\frac{\\pi}{4}$$\n",
    "And therefore we can say:\n",
    "$$ \\pi = 4 \\frac{A_c}{A_s} $$\n",
    "With this equation we can derive the value of pi using the area of the circle and the square.\n",
    "\n",
    "\n",
    "We can approximate the ratio of these areas using a set of random numbers and a bit of logic.\n",
    "\n",
    "\n",
    "If we generate uniform random variables we can treat them as points on a discrete grid.\n",
    "The number of grid points that fall in the circle compared to the total number of points approximates the ratio of the area of the circle and the square respectively.\n",
    "\n",
    "$$ \\frac{num \\ points \\ in  \\ circle}{num \\ of \\ points} \\approx \\frac{A_c}{A_s} $$\n",
    "\n",
    "As the number of random points increases, we converge to the true areas and thus the true value of pi.\n",
    "\n",
    "<center><img src='Convergence of Monte Carlo.gif' width=\"300px\"/></center>\n",
    "\n",
    "We can determine which poitns are inside the circle vs the ones that are not by using the Pythagorean Theorem.\n",
    "Given a triangle, we can determine the length of a side if we know the length of the other two sides.\n",
    "$$ A^2 + B^2 = C^2 $$\n",
    "\n",
    "$$ C = \\sqrt{A^2 + B^2} $$\n",
    "If we compare the hypotinuse with the radius of a circle we will be able to determine whether or not a point is within a circle or not\n",
    "\n",
    "<center><img src='Circle Method Pythagorean Diameter.png' width=\"300px\"/></center>\n",
    "\n",
    "The criteria for being inside the circle thus becomes:\n",
    "\n",
    "$$ r \\le \\sqrt{X^2 + Y^2} $$\n",
    "\n",
    "Because we are dealing with a unit circle, $r = 1; \\sqrt{1} = 1$ , thus we can also say:\n",
    "\n",
    "$$ r \\le X^2 + Y^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2. The Spark Pi Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.172\n"
     ]
    }
   ],
   "source": [
    "# Define a function to generate a pair or random numbers and determine whether they corespond to a point within a circle\n",
    "import random\n",
    "\n",
    "def monte_carlo_trial(var):\n",
    "    # Generate random variables for x and y\n",
    "    x, y = random.random(), random.random()\n",
    "    # Calculate whether or not the point is inside the circle\n",
    "    inside_circle =  x*x + y*y < 1\n",
    "    # Return the value\n",
    "    return inside_circle\n",
    "\n",
    "# Set the number of trials for the monte carlo simulation\n",
    "number_of_trials = 10000\n",
    "\n",
    "# Use the SparkContext to apply the monte carlo trials in parrallel and count the positive results\n",
    "count = sc.parallelize(range(0, number_of_trials)).filter(monte_carlo_trial).count()\n",
    "\n",
    "# Compute the value of pi based on the information from the monte carlo simulation\n",
    "pi = 4 * count / number_of_trials\n",
    "\n",
    "# Print the value of pi\n",
    "print(pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Cleanup Spark Cluster On Kubernetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No resources found in spark namespace.\n"
     ]
    }
   ],
   "source": [
    "! kubectl -n spark get pod"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
