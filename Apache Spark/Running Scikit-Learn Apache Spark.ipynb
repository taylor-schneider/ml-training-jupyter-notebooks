{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "In this notebook we are going to look at a few examples of running scikit-learn modeals against an Apache Spark cluster. Unlike the models packaged with Apache Spark, scikit-learn models are not ubilt to be distributed and cannot parallelize calculations.\n",
    "\n",
    "It assumes you have already read the following notebooks:\n",
    "- [Install Apache Spark Prerequisites](Install%20Apache%20Spark%20Prerequisites.ipynb)\n",
    "- [Spark Pi - The Hello World Example For Apache spark](Spark%20Pi%20-%20The%20Hello%20World%20Example%20For%20Apache%20spark.ipynb)\n",
    "- [Intro To Koalas](Intro%20To%20Koalas.ipynb)\n",
    "- <a href=\"../Cluster%20Analysis/K-Means.ipynb\">Cluster Analysis/K-Means</a>\n",
    "\n",
    "The instructions are basically the same as [Running Apache Spark Locally](Running%20Apache%20Spark%20Locally.ipynb) once you get the kubernetes stuff setup.\n",
    "\n",
    "## Adjenda\n",
    "1. Create SparkContext\n",
    "2. Create Web Server To Host Data\n",
    "3. Load The Data\n",
    "8. Prepare Worker Nodes\n",
    "9. Submit Python Code To Spark Cluster\n",
    "10. Cleanup Spark and Kubernetes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Create SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting SPARK_HOME\n",
      "c:\\spark\\spark-3.1.1-bin-hadoop2.7\n",
      "\n",
      "Running findspark.init() function\n",
      "['c:\\\\spark\\\\spark-3.1.1-bin-hadoop2.7\\\\python', 'c:\\\\spark\\\\spark-3.1.1-bin-hadoop2.7\\\\python\\\\lib\\\\py4j-0.10.9-src.zip', 'c:\\\\program files\\\\python36\\\\python36.zip', 'c:\\\\program files\\\\python36\\\\DLLs', 'c:\\\\program files\\\\python36\\\\lib', 'c:\\\\program files\\\\python36', '', 'c:\\\\program files\\\\python36\\\\lib\\\\site-packages', 'c:\\\\program files\\\\python36\\\\lib\\\\site-packages\\\\win32', 'c:\\\\program files\\\\python36\\\\lib\\\\site-packages\\\\win32\\\\lib', 'c:\\\\program files\\\\python36\\\\lib\\\\site-packages\\\\Pythonwin', 'c:\\\\program files\\\\python36\\\\lib\\\\site-packages\\\\IPython\\\\extensions', 'C:\\\\Users\\\\Administrator\\\\.ipython']\n",
      "\n",
      "Setting PYSPARK_PYTHON\n",
      "/usr/bin/python3\n",
      "\n",
      "Determine IP Of Server\n",
      "The ip was detected as: 15.1.1.23\n",
      "\n",
      "Create SparkContext\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from spark_helper import create_spark_context\n",
    "spark_app_name = \"spark-jupyter-win\"\n",
    "docker_image = \"tschneider/pyspark:v5\"\n",
    "k8_master_ip = \"15.4.7.11\"\n",
    "sc = create_spark_context(spark_app_name, docker_image, k8_master_ip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at kubernetes to see that out worker nodes were created. \n",
    "\n",
    "The first time we create the spark context with a given docker image, the image will need to be downloaded (which takes some time). As a result, we may see the pods with a status of \"ContainerCreating\". In this case, we will need to wait until the containers are in a \"Running\" state.\n",
    "\n",
    "```\n",
    "kubectl -n spark get pod\n",
    "NAME                                        READY   STATUS              RESTARTS   AGE\n",
    "spark-jupyter-win-3ed7f27984f7563a-exec-1   0/1     ContainerCreating   0          12m\n",
    "spark-jupyter-win-3ed7f27984f7563a-exec-2   0/1     ContainerCreating   0          12m\n",
    "spark-jupyter-win-3ed7f27984f7563a-exec-3   0/1     ContainerCreating   0          12m\n",
    "```\n",
    "\n",
    "We can check the status of the docker pull by logging into the container and running the docker pull command to attach to the running process:\n",
    "```\n",
    "kubectl -n spark exec -ti docker pull tschneider/pyspark:v3 docker pull tschneider/pyspark:v4\n",
    "v3: Pulling from tschneider/pyspark\n",
    "2d473b07cdd5: Already exists\n",
    "71d236fb1195: Already exists\n",
    "2e22160d8cab: Already exists\n",
    "e99d962ac218: Pull complete\n",
    "Digest: sha256:eb74701b4ae909c40046ff68b1044b09b11895e175c955dfd8afe9fe680309cf\n",
    "Status: Downloaded newer image for tschneider/pyspark:v3\n",
    "docker.io/tschneider/pyspark:v3\n",
    "[root@os004k8-worker002 ~]# docker pull tschneider/pyspark:v4\n",
    "v4: Pulling from tschneider/pyspark\n",
    "2d473b07cdd5: Already exists\n",
    "71d236fb1195: Already exists\n",
    "2e22160d8cab: Already exists\n",
    "c556a717fe5d: Downloading [=======================>                           ]  578.7MB/1.246GB\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                        READY   STATUS    RESTARTS   AGE\n",
      "spark-jupyter-win-3155137991c6dba8-exec-1   1/1     Running   0          17s\n",
      "spark-jupyter-win-3155137991c6dba8-exec-2   1/1     Running   0          16s\n",
      "spark-jupyter-win-3155137991c6dba8-exec-3   1/1     Running   0          16s\n"
     ]
    }
   ],
   "source": [
    "! kubectl -n spark get pod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Create web server to host data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the current working directory. \n",
    "\n",
    "Note: There is a trick to doing this inside a jupyter notebook and so we will use a special library to get that information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\git\\ml-training-jupyter-notebooks\n"
     ]
    }
   ],
   "source": [
    "import pyprojroot\n",
    "project_root_dir  = pyprojroot.here()\n",
    "print(project_root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the module for the webserver from our utilities directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the module for the web server we wrote\n",
    "import importlib.util\n",
    "spec = importlib.util.spec_from_file_location(\"PythonHttpFileServer\", \"../Utilities/PythonHttpFileServer.py\")\n",
    "PythonHttpFileServer = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(PythonHttpFileServer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure logging so that messages are collected and displayed asynchronously so that the server can run in the background without casuing a jupyter cell to block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Starting server on port 80\n",
      "INFO:root:Web root specified as: C:\\Users\\Administrator\\git\\ml-training-jupyter-notebooks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app 'PythonHttpFileServer' (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Configure the logger and log level\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Remove all handlers\n",
    "for handler in logger.handlers: \n",
    "    logger.removeHandler(handler)\n",
    "for handler in logger.handlers: \n",
    "    logger.removeHandler(handler)\n",
    "    \n",
    "# Start the webserver in a thread so the cell is not stuck in a running state\n",
    "import threading\n",
    "web_server_port = 80\n",
    "web_server_args = (web_server_port, project_root_dir)\n",
    "web_server_thread = threading.Thread(target=PythonHttpFileServer.run_server, args=web_server_args)\n",
    "web_server_thread.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Load The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instruct the spark cluster to download a file from the web server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spark_helper import determine_ip_address\n",
    "csv_file_name = \"nasdaq_2019.csv\"\n",
    "ip_address = determine_ip_address()\n",
    "csv_file_url = \"http://{0}:{1}/{2}\".format(ip_address, web_server_port, csv_file_name)\n",
    "sc.addFile(csv_file_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the utility function to convert a date string to a datetime object from our utilities module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the utilities module we wrote\n",
    "import importlib.util\n",
    "spec = importlib.util.spec_from_file_location(\"utilities\", \"../Utilities/utilities.py\")\n",
    "utilities = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(utilities)\n",
    "\n",
    "# Define a mapping to convert our data field to the correct type\n",
    "converter_mapping = {\n",
    "    \"date\": utilities.convert_date_string_to_date\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Load our OHCLV data Into a koalas dataframe and pull out a single day in the say way we would in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. Koalas will set it for you but it does not work if there is a Spark context already launched.\n",
      "INFO:spark:Patching spark automatically. You can disable it by setting SPARK_KOALAS_AUTOPATCH=false in your environment\n"
     ]
    }
   ],
   "source": [
    "from databricks import koalas\n",
    "koalas_dataframe = koalas.read_csv(u\"file:////nasdaq_2019.csv\", converters=converter_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should see the workers download the file in the logs. If we log into the nodes we can see the file is located on the filesystem root."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data loaded into a koalas dataframe we can access the data in the same way we would from a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>interval</th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AABA</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>70.90</td>\n",
       "      <td>71.5200</td>\n",
       "      <td>70.3250</td>\n",
       "      <td>70.57</td>\n",
       "      <td>10234800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAL</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>33.14</td>\n",
       "      <td>33.6632</td>\n",
       "      <td>32.5301</td>\n",
       "      <td>32.88</td>\n",
       "      <td>8995100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAME</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>2.43</td>\n",
       "      <td>2.4300</td>\n",
       "      <td>2.4000</td>\n",
       "      <td>2.40</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAOI</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>10.70</td>\n",
       "      <td>10.8900</td>\n",
       "      <td>10.0100</td>\n",
       "      <td>10.18</td>\n",
       "      <td>883100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAON</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>50.57</td>\n",
       "      <td>50.9850</td>\n",
       "      <td>48.5600</td>\n",
       "      <td>49.73</td>\n",
       "      <td>180200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker interval        date   open     high      low  close    volume\n",
       "0   AABA        D  2019-07-01  70.90  71.5200  70.3250  70.57  10234800\n",
       "1    AAL        D  2019-07-01  33.14  33.6632  32.5301  32.88   8995100\n",
       "2   AAME        D  2019-07-01   2.43   2.4300   2.4000   2.40       500\n",
       "3   AAOI        D  2019-07-01  10.70  10.8900  10.0100  10.18    883100\n",
       "4   AAON        D  2019-07-01  50.57  50.9850  48.5600  49.73    180200"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "koalas_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. We need to prepare our worker nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: We need to install relevant python libraries on the worker nodes. If you do not, you might see an error as follows:\n",
    "```\n",
    "PythonException: \n",
    "  An exception was thrown from the Python worker. Please see the stack trace below.\n",
    "Traceback (most recent call last):\n",
    "  File \"/usr/local/lib/python3.6/site-packages/pyspark/worker.py\", line 588, in main\n",
    "    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n",
    "  File \"/usr/local/lib/python3.6/site-packages/pyspark/worker.py\", line 421, in read_udfs\n",
    "    arg_offsets, f = read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=0)\n",
    "  File \"/usr/local/lib/python3.6/site-packages/pyspark/worker.py\", line 249, in read_single_udf\n",
    "    f, return_type = read_command(pickleSer, infile)\n",
    "  File \"/usr/local/lib/python3.6/site-packages/pyspark/worker.py\", line 69, in read_command\n",
    "    command = serializer._read_with_length(file)\n",
    "  File \"/usr/local/lib/python3.6/site-packages/pyspark/serializers.py\", line 160, in _read_with_length\n",
    "    return self.loads(obj)\n",
    "  File \"/usr/local/lib/python3.6/site-packages/pyspark/serializers.py\", line 430, in loads\n",
    "    return pickle.loads(obj, encoding=encoding)\n",
    "  File \"/usr/local/lib/python3.6/site-packages/pyspark/cloudpickle/cloudpickle.py\", line 562, in subimport\n",
    "    __import__(name)\n",
    "ModuleNotFoundError: No module named 'pandas'\n",
    "```\n",
    "\n",
    "In our case we needed to install pandas, numpy, koalas, scikit-learn, sklearn. If you are unsure of what is installed on your workers, we can log into the kubernetes pods and execute shell commands.\n",
    "\n",
    "Note: We must do this on all workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                        READY   STATUS    RESTARTS   AGE\n",
      "spark-jupyter-win-3155137991c6dba8-exec-1   1/1     Running   0          2m3s\n",
      "spark-jupyter-win-3155137991c6dba8-exec-2   1/1     Running   0          2m2s\n",
      "spark-jupyter-win-3155137991c6dba8-exec-3   1/1     Running   0          2m2s\n"
     ]
    }
   ],
   "source": [
    "! kubectl -n spark get pod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unable to use a TTY - input is not a terminal or the right kind of file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package         Version\n",
      "--------------- -------\n",
      "cycler          0.10.0\n",
      "joblib          1.0.1\n",
      "kiwisolver      1.3.1\n",
      "kneed           0.7.0\n",
      "koalas          1.8.0\n",
      "matplotlib      3.3.4\n",
      "numpy           1.19.5\n",
      "pandas          1.1.5\n",
      "Pillow          8.2.0\n",
      "pip             21.1.1\n",
      "progressbar     2.5\n",
      "py4j            0.10.9\n",
      "pyarrow         4.0.0\n",
      "pyparsing       2.4.7\n",
      "pyspark         3.1.1\n",
      "python-dateutil 2.8.1\n",
      "pytz            2021.1\n",
      "scikit-learn    0.24.2\n",
      "scipy           1.5.4\n",
      "setuptools      39.2.0\n",
      "six             1.16.0\n",
      "sklearn         0.0\n",
      "threadpoolctl   2.1.0\n"
     ]
    }
   ],
   "source": [
    "! kubectl -n spark exec -ti spark-jupyter-win-3155137991c6dba8-exec-1 -- pip3 list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Submit Python Code To Spark Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section of the notebook we are going to apply the kmeans algorithm from sklearn to each date in our koalas_dataframe object.\n",
    "To do this, we are going to write a function that applies the algorithm to a dataframe; the assumption being the dataframe only contains data related to the same date.\n",
    "Note: Most of this is a review and reworking of the content contained in \n",
    "<a href=\"../Cluster%20Analysis/K-Means.ipynb\">Cluster Analysis/K-Means.ipynb</a>.\n",
    "\n",
    "We create our data frame for testing based on a subset of our real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>interval</th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>93620</th>\n",
       "      <td>AABA</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>57.94</td>\n",
       "      <td>57.94</td>\n",
       "      <td>57.94</td>\n",
       "      <td>57.94</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93621</th>\n",
       "      <td>AAL</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>32.11</td>\n",
       "      <td>32.11</td>\n",
       "      <td>32.11</td>\n",
       "      <td>32.11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93622</th>\n",
       "      <td>AAME</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>2.41</td>\n",
       "      <td>2.41</td>\n",
       "      <td>2.41</td>\n",
       "      <td>2.41</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93623</th>\n",
       "      <td>AAOI</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>15.43</td>\n",
       "      <td>15.43</td>\n",
       "      <td>15.43</td>\n",
       "      <td>15.43</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93624</th>\n",
       "      <td>AAON</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>35.06</td>\n",
       "      <td>35.06</td>\n",
       "      <td>35.06</td>\n",
       "      <td>35.06</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ticker interval        date   open   high    low  close  volume\n",
       "93620   AABA        D  2019-01-01  57.94  57.94  57.94  57.94       0\n",
       "93621    AAL        D  2019-01-01  32.11  32.11  32.11  32.11       0\n",
       "93622   AAME        D  2019-01-01   2.41   2.41   2.41   2.41       0\n",
       "93623   AAOI        D  2019-01-01  15.43  15.43  15.43  15.43       0\n",
       "93624   AAON        D  2019-01-01  35.06  35.06  35.06  35.06       0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort based on the date column\n",
    "koalas_dataframe = koalas_dataframe.sort_values(\"date\")\n",
    "df_01_01_2019 = koalas_dataframe.loc[koalas_dataframe[\"date\"] == '2019-01-01'].copy()\n",
    "df_01_01_2019.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then write and test our function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "model = KMeans(n_clusters=5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_kmeans_on_dataframe3(df, column_names=[\"open\"]):\n",
    "    \n",
    "    # Create a copy of our dataframe so we can play around\n",
    "    tmp = df.copy()\n",
    "    columns = column_names.copy()\n",
    "\n",
    "    # IF we only supplied one column name, we will need to create a bogus column\n",
    "    # Kmeans requires a 2D array so we will add a static column\n",
    "    bogus_column = None\n",
    "    if len(column_names) < 2:\n",
    "        bogus_column = \"Y\"\n",
    "        tmp[bogus_column] = [1 for x in range(0, tmp.shape[0])]\n",
    "        columns.append(bogus_column)\n",
    "        \n",
    "    # Set the parameters for our model\n",
    "    # It expects a 2D array where the columns are our features\n",
    "    model_parameters = tmp[[*columns]].to_numpy()\n",
    "    \n",
    "    # Train the model\n",
    "    trained_model = model.fit(model_parameters)\n",
    "    \n",
    "    # Extract the information\n",
    "    cluster_indices = trained_model.labels_.astype(int)\n",
    "    if bogus_column in columns:\n",
    "        cluster_centroids = [trained_model.cluster_centers_[i][0] for i in cluster_indices]\n",
    "    else:\n",
    "        cluster_centroids = [str(trained_model.cluster_centers_[i].tolist()) for i in cluster_indices]\n",
    "        \n",
    "    # Update the dataframe (setting special options to allow koalas to work)\n",
    "#    option_value = koalas.get_option('compute.ops_on_diff_frames')\n",
    "#    koalas.set_option('compute.ops_on_diff_frames', True)\n",
    "\n",
    "    tmp[\"cluster_indices\"] = cluster_indices.tolist()\n",
    "    tmp[\"cluster_centroids\"] = cluster_centroids\n",
    "        \n",
    "#    koalas.set_option('compute.ops_on_diff_frames', option_value)\n",
    "    \n",
    "    # Determine which columns we want to return\n",
    "    columns = tmp.columns.to_list()\n",
    "    if bogus_column in columns:\n",
    "        columns.remove(bogus_column)\n",
    "    \n",
    "    return tmp[[*columns]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>interval</th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>cluster_indices</th>\n",
       "      <th>cluster_centroids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95460</th>\n",
       "      <td>MITK</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>10.81</td>\n",
       "      <td>10.81</td>\n",
       "      <td>10.81</td>\n",
       "      <td>10.81</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>[13.05660116731515, 13.05660116731515]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96515</th>\n",
       "      <td>TWNK</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>10.94</td>\n",
       "      <td>10.94</td>\n",
       "      <td>10.94</td>\n",
       "      <td>10.94</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>[13.05660116731515, 13.05660116731515]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94136</th>\n",
       "      <td>CELH</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>3.47</td>\n",
       "      <td>3.47</td>\n",
       "      <td>3.47</td>\n",
       "      <td>3.47</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>[13.05660116731515, 13.05660116731515]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95617</th>\n",
       "      <td>NNDM</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>1.11</td>\n",
       "      <td>1.11</td>\n",
       "      <td>1.11</td>\n",
       "      <td>1.11</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>[13.05660116731515, 13.05660116731515]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95363</th>\n",
       "      <td>LSBK</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>15.06</td>\n",
       "      <td>15.06</td>\n",
       "      <td>15.06</td>\n",
       "      <td>15.06</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>[13.05660116731515, 13.05660116731515]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ticker interval        date   open   high    low  close  volume  cluster_indices                       cluster_centroids\n",
       "95460   MITK        D  2019-01-01  10.81  10.81  10.81  10.81       0                2  [13.05660116731515, 13.05660116731515]\n",
       "96515   TWNK        D  2019-01-01  10.94  10.94  10.94  10.94       0                2  [13.05660116731515, 13.05660116731515]\n",
       "94136   CELH        D  2019-01-01   3.47   3.47   3.47   3.47       0                2  [13.05660116731515, 13.05660116731515]\n",
       "95617   NNDM        D  2019-01-01   1.11   1.11   1.11   1.11       0                2  [13.05660116731515, 13.05660116731515]\n",
       "95363   LSBK        D  2019-01-01  15.06  15.06  15.06  15.06       0                2  [13.05660116731515, 13.05660116731515]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perform_kmeans_on_dataframe3(df_01_01_2019, column_names=[\"open\", \"close\"]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\spark\\spark-3.1.1-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\group_ops.py:84: UserWarning: It is preferred to use 'applyInPandas' over this API. This API will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  \"more details.\", UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>interval</th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>cluster_indices</th>\n",
       "      <th>cluster_centroids</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2019-01-01</th>\n",
       "      <th>93620</th>\n",
       "      <td>AABA</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>57.94</td>\n",
       "      <td>57.94</td>\n",
       "      <td>57.94</td>\n",
       "      <td>57.94</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>67.532737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93621</th>\n",
       "      <td>AAL</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>32.11</td>\n",
       "      <td>32.11</td>\n",
       "      <td>32.11</td>\n",
       "      <td>32.11</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>13.056601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93622</th>\n",
       "      <td>AAME</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>2.41</td>\n",
       "      <td>2.41</td>\n",
       "      <td>2.41</td>\n",
       "      <td>2.41</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>13.056601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93623</th>\n",
       "      <td>AAOI</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>15.43</td>\n",
       "      <td>15.43</td>\n",
       "      <td>15.43</td>\n",
       "      <td>15.43</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>13.056601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93624</th>\n",
       "      <td>AAON</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>35.06</td>\n",
       "      <td>35.06</td>\n",
       "      <td>35.06</td>\n",
       "      <td>35.06</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>13.056601</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ticker interval        date   open   high    low  close  volume  cluster_indices  cluster_centroids\n",
       "date                                                                                                                \n",
       "2019-01-01 93620   AABA        D  2019-01-01  57.94  57.94  57.94  57.94       0                0          67.532737\n",
       "           93621    AAL        D  2019-01-01  32.11  32.11  32.11  32.11       0                2          13.056601\n",
       "           93622   AAME        D  2019-01-01   2.41   2.41   2.41   2.41       0                2          13.056601\n",
       "           93623   AAOI        D  2019-01-01  15.43  15.43  15.43  15.43       0                2          13.056601\n",
       "           93624   AAON        D  2019-01-01  35.06  35.06  35.06  35.06       0                2          13.056601"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_01_01_2019.groupby(\"date\").apply(perform_kmeans_on_dataframe3, column_names=[\"open\"]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run this function against out dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\spark\\spark-3.1.1-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\group_ops.py:84: UserWarning: It is preferred to use 'applyInPandas' over this API. This API will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  \"more details.\", UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>interval</th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>cluster_indices</th>\n",
       "      <th>cluster_centroids</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2019-01-01</th>\n",
       "      <th>93620</th>\n",
       "      <td>AABA</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>57.94</td>\n",
       "      <td>57.94</td>\n",
       "      <td>57.94</td>\n",
       "      <td>57.94</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[67.53273664825046, 67.53273664825046]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93621</th>\n",
       "      <td>AAL</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>32.11</td>\n",
       "      <td>32.11</td>\n",
       "      <td>32.11</td>\n",
       "      <td>32.11</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>[13.05660116731514, 13.05660116731514]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93622</th>\n",
       "      <td>AAME</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>2.41</td>\n",
       "      <td>2.41</td>\n",
       "      <td>2.41</td>\n",
       "      <td>2.41</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>[13.05660116731514, 13.05660116731514]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93623</th>\n",
       "      <td>AAOI</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>15.43</td>\n",
       "      <td>15.43</td>\n",
       "      <td>15.43</td>\n",
       "      <td>15.43</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>[13.05660116731514, 13.05660116731514]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93624</th>\n",
       "      <td>AAON</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>35.06</td>\n",
       "      <td>35.06</td>\n",
       "      <td>35.06</td>\n",
       "      <td>35.06</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>[13.05660116731514, 13.05660116731514]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ticker interval        date   open   high    low  close  volume  cluster_indices                       cluster_centroids\n",
       "date                                                                                                                                     \n",
       "2019-01-01 93620   AABA        D  2019-01-01  57.94  57.94  57.94  57.94       0                0  [67.53273664825046, 67.53273664825046]\n",
       "           93621    AAL        D  2019-01-01  32.11  32.11  32.11  32.11       0                2  [13.05660116731514, 13.05660116731514]\n",
       "           93622   AAME        D  2019-01-01   2.41   2.41   2.41   2.41       0                2  [13.05660116731514, 13.05660116731514]\n",
       "           93623   AAOI        D  2019-01-01  15.43  15.43  15.43  15.43       0                2  [13.05660116731514, 13.05660116731514]\n",
       "           93624   AAON        D  2019-01-01  35.06  35.06  35.06  35.06       0                2  [13.05660116731514, 13.05660116731514]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "koalas_dataframe.groupby(\"date\").apply(perform_kmeans_on_dataframe3, column_names=[\"open\",\"close\"]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Cleanup Spark Cluster On Kubernetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                        READY   STATUS        RESTARTS   AGE\n",
      "spark-jupyter-win-3155137991c6dba8-exec-1   1/1     Terminating   0          3m30s\n",
      "spark-jupyter-win-3155137991c6dba8-exec-2   1/1     Terminating   0          3m29s\n",
      "spark-jupyter-win-3155137991c6dba8-exec-3   1/1     Terminating   0          3m29s\n"
     ]
    }
   ],
   "source": [
    "! kubectl -n spark get pod"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
