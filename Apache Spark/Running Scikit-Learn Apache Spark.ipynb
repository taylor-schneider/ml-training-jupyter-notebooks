{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "In this notebook we are going to look at a few examples of running scikit-learn modeals against an Apache Spark cluster. Unlike the models packaged with Apache Spark, scikit-learn models are not ubilt to be distributed and cannot parallelize calculations.\n",
    "\n",
    "It assumes you have already read the following notebooks:\n",
    "- [Install Apache Spark Prerequisites](Install%20Apache%20Spark%20Prerequisites.ipynb)\n",
    "- [Spark Pi - The Hello World Example For Apache spark](Spark%20Pi%20-%20The%20Hello%20World%20Example%20For%20Apache%20spark.ipynb)\n",
    "- <a href=\"../Cluster%20Analysis/K-Means.ipynb\">Cluster Analysis/K-Means</a>\n",
    "\n",
    "The instructions are basically the same as [Running Apache Spark Locally](Running%20Apache%20Spark%20Locally.ipynb) once you get the kubernetes stuff setup.\n",
    "\n",
    "## Adjenda\n",
    "1. Configure Kubernetes Cluster For Spark\n",
    "2. Install the Kubectl CLI for Kubernetes\n",
    "3. Set Environment variables\n",
    "4. Create SparKConf\n",
    "5. Create SparkContext\n",
    "6. Create Web Server To Host Data\n",
    "7. Load The Data\n",
    "8. Prepare Worker Nodes\n",
    "9. Submit Python Code To Spark Cluster\n",
    "10. Cleanup Spark and Kubernetes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configure Kubernetes Cluster For Spark\n",
    "In order for our kubernetes cluster to successfully run a spark cluster we need to do a few things:\n",
    "1. Configure RBAC - We will need to set permissions so that our jupyter notebook and spark components have the appropriate permissions.\n",
    "2. Build containers - We will need to build the contaienrs which host our spark cluster nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Configure Kubernetes RBAC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Build Spark Containers For Kubernetes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Install and Configure Kubectl\n",
    "Kubectl is the CLI for kubernetes. It will allow our jupyter notebook to connect to the kubernetes cluster and spin up containers to run our Spark work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Install Kubectl\n",
    "There are a number of ways to install kubectl. The easiest and fully featured way is to use the chocolatey installation process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://kubernetes.io/docs/tasks/tools/install-kubectl-windows/#install-on-windows-using-chocolatey-or-scoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client Version: version.Info{Major:\"1\", Minor:\"21\", GitVersion:\"v1.21.0\", GitCommit:\"cb303e613a121a29364f75cc67d3d580833a7479\", GitTreeState:\"clean\", BuildDate:\"2021-04-08T16:31:21Z\", GoVersion:\"go1.16.1\", Compiler:\"gc\", Platform:\"windows/amd64\"}\n"
     ]
    }
   ],
   "source": [
    "! kubectl version --client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Configure Kubectl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cd %USERPROFILE% & mkdir .kube 2> NUL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the kubeconfi file... We can copy it from the master\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "! kubectl cluster-info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                           STATUS   ROLES                  AGE   VERSION\n",
      "os004k8-master001.foobar.com   Ready    control-plane,master   22d   v1.21.1\n",
      "os004k8-worker001.foobar.com   Ready    <none>                 22d   v1.21.1\n",
      "os004k8-worker002.foobar.com   Ready    <none>                 22d   v1.21.1\n",
      "os004k8-worker003.foobar.com   Ready    <none>                 22d   v1.21.1\n"
     ]
    }
   ],
   "source": [
    "! kubectl get node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Set Environment Variables\n",
    "We can use the os package to set environment variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Set SPARK_HOME variable\n",
    "This variable configures our system to understand where spark is installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['SPARK_HOME'] = \"c:\\\\spark\\\\spark-3.1.1-bin-hadoop2.7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\spark\\spark-3.1.1-bin-hadoop2.7\n"
     ]
    }
   ],
   "source": [
    "print(os.environ['SPARK_HOME'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Run findspark.init() to add Spark to PATH\n",
    "PySpark isn't on sys.path by default, but that doesn't mean it can't be used as a regular library. You can address this by either symlinking pyspark into your site-packages, or adding pyspark to sys.path at runtime. findspark does the latter.\n",
    "\n",
    "https://github.com/minrk/findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c:\\\\spark\\\\spark-3.1.1-bin-hadoop2.7\\\\python', 'c:\\\\spark\\\\spark-3.1.1-bin-hadoop2.7\\\\python\\\\lib\\\\py4j-0.10.9-src.zip', 'c:\\\\program files\\\\python36\\\\python36.zip', 'c:\\\\program files\\\\python36\\\\DLLs', 'c:\\\\program files\\\\python36\\\\lib', 'c:\\\\program files\\\\python36', '', 'c:\\\\program files\\\\python36\\\\lib\\\\site-packages', 'c:\\\\program files\\\\python36\\\\lib\\\\site-packages\\\\win32', 'c:\\\\program files\\\\python36\\\\lib\\\\site-packages\\\\win32\\\\lib', 'c:\\\\program files\\\\python36\\\\lib\\\\site-packages\\\\Pythonwin', 'c:\\\\program files\\\\python36\\\\lib\\\\site-packages\\\\IPython\\\\extensions', 'C:\\\\Users\\\\Administrator\\\\.ipython']\n"
     ]
    }
   ],
   "source": [
    "# Print the PATH variable to show the spark directory is set\n",
    "import sys\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Set PYSPARK_PYTHON variable\n",
    "This variable configures spark to understand where python is installed on the spark nodes. Recall, these are the linux containers we built earlier. By default, the local windows file path may be set, but this will not work. If improperly confiugred we may see an error like this one:\n",
    "```\n",
    "Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n",
    ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 0.0 failed 4 times, most recent failure: Lost task 2.3 in stage 0.0 (TID 17) (10.36.0.2 executor 1): java.io.IOException: Cannot run program \"c:\\program files\\python36\\python.exe\": error=2, No such file or directory\n",
    "```\n",
    "We need to set this variable equal to path of python on the container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_PYTHON'] = \"/usr/bin/python3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/python3\n"
     ]
    }
   ],
   "source": [
    "print(os.environ['PYSPARK_PYTHON'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Create SparKConf Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some vars to specify where the kubernetes master is\n",
    "kubernetes_master_ip = \"15.4.7.11\"\n",
    "kubernetes_master_port = \"6443\"\n",
    "spark_master_url = \"k8s://https://{0}:{1}\".format(kubernetes_master_ip, kubernetes_master_port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ip was detected as: 15.1.1.23\n"
     ]
    }
   ],
   "source": [
    "# Determine the ip address of the machine\n",
    "import netifaces\n",
    "import re\n",
    "nic_uuid = netifaces.gateways()['default'][netifaces.AF_INET][1]\n",
    "nic_details = netifaces.ifaddresses(nic_uuid)\n",
    "ip_address = None\n",
    "for i, nic_detail, in nic_details.items():\n",
    "    if all([key in nic_detail[0].keys() for key in [\"addr\", \"netmask\", \"broadcast\"]]):\n",
    "        if re.match(\"([0-9]+\\\\.)+\", nic_detail[0][\"addr\"]):\n",
    "            ip_address = nic_detail[0][\"addr\"]\n",
    "            break\n",
    "print(\"The ip was detected as: {0}\".format(ip_address))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x730d6d8>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wire up the SparkConf object\n",
    "sparkConf = pyspark.SparkConf()\n",
    "sparkConf.setMaster(spark_master_url)\n",
    "\n",
    "sparkConf.setAppName(\"spark-jupyter-win\")\n",
    "\n",
    "sparkConf.set(\"spark.submit.deploy.mode\", \"cluster\")\n",
    "sparkConf.set(\"spark.kubernetes.container.image\", \"tschneider/pyspark:v5\") \n",
    "sparkConf.set(\"spark.kubernetes.namespace\", \"spark\")\n",
    "sparkConf.set(\"spark.kubernetes.pyspark.pythonVersion\", \"3\")\n",
    "sparkConf.set(\"spark.kubernetes.authenticate.driver.serviceAccountName\", \"spark-sa\")\n",
    "sparkConf.set(\"spark.kubernetes.authenticate.serviceAccountName\", \"spark-sa\")\n",
    "\n",
    "sparkConf.set(\"spark.executor.instances\", \"3\")\n",
    "sparkConf.set(\"spark.executor.cores\", \"2\")\n",
    "sparkConf.set(\"spark.executor.memory\", \"1024m\")\n",
    "sparkConf.set(\"spark.driver.memory\", \"1024m\")\n",
    "\n",
    "# If we are not using a hostname registered with a dns server, we need to set this parameter\n",
    "sparkConf.set(\"spark.driver.host\", ip_address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.executor.instances', '3'),\n",
       " ('spark.kubernetes.container.image', 'tschneider/pyspark:v5'),\n",
       " ('spark.app.name', 'spark-jupyter-win'),\n",
       " ('spark.driver.memory', '1024m'),\n",
       " ('spark.executor.cores', '2'),\n",
       " ('spark.kubernetes.pyspark.pythonVersion', '3'),\n",
       " ('spark.kubernetes.namespace', 'spark'),\n",
       " ('spark.kubernetes.authenticate.serviceAccountName', 'spark-sa'),\n",
       " ('spark.submit.deploy.mode', 'cluster'),\n",
       " ('spark.executor.memory', '1024m'),\n",
       " ('spark.master', 'k8s://https://15.4.7.11:6443'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.kubernetes.authenticate.driver.serviceAccountName', 'spark-sa'),\n",
       " ('spark.driver.host', '15.1.1.23'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkConf.getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Create SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at kubernetes to see that out worker nodes were created. \n",
    "\n",
    "The first time we create the spark context with a given docker image, the image will need to be downloaded (which takes some time). As a result, we may see the pods with a status of \"ContainerCreating\". In this case, we will need to wait until the containers are in a \"Running\" state.\n",
    "\n",
    "```\n",
    "kubectl -n spark get pod\n",
    "NAME                                        READY   STATUS              RESTARTS   AGE\n",
    "spark-jupyter-win-3ed7f27984f7563a-exec-1   0/1     ContainerCreating   0          12m\n",
    "spark-jupyter-win-3ed7f27984f7563a-exec-2   0/1     ContainerCreating   0          12m\n",
    "spark-jupyter-win-3ed7f27984f7563a-exec-3   0/1     ContainerCreating   0          12m\n",
    "```\n",
    "\n",
    "We can check the status of the docker pull by logging into the container and running the docker pull command to attach to the running process:\n",
    "```\n",
    "kubectl -n spark exec -ti docker pull tschneider/pyspark:v3 docker pull tschneider/pyspark:v4\n",
    "v3: Pulling from tschneider/pyspark\n",
    "2d473b07cdd5: Already exists\n",
    "71d236fb1195: Already exists\n",
    "2e22160d8cab: Already exists\n",
    "e99d962ac218: Pull complete\n",
    "Digest: sha256:eb74701b4ae909c40046ff68b1044b09b11895e175c955dfd8afe9fe680309cf\n",
    "Status: Downloaded newer image for tschneider/pyspark:v3\n",
    "docker.io/tschneider/pyspark:v3\n",
    "[root@os004k8-worker002 ~]# docker pull tschneider/pyspark:v4\n",
    "v4: Pulling from tschneider/pyspark\n",
    "2d473b07cdd5: Already exists\n",
    "71d236fb1195: Already exists\n",
    "2e22160d8cab: Already exists\n",
    "c556a717fe5d: Downloading [=======================>                           ]  578.7MB/1.246GB\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                        READY   STATUS    RESTARTS   AGE\n",
      "spark-jupyter-win-563a3b798591b8da-exec-1   1/1     Running   0          32m\n",
      "spark-jupyter-win-563a3b798591b8da-exec-2   1/1     Running   0          32m\n",
      "spark-jupyter-win-563a3b798591b8da-exec-3   1/1     Running   0          32m\n"
     ]
    }
   ],
   "source": [
    "! kubectl -n spark get pod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Create web server to host data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the current working directory. \n",
    "\n",
    "Note: There is a trick to doing this inside a jupyter notebook and so we will use a special library to get that information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\git\\ml-training-jupyter-notebooks\n"
     ]
    }
   ],
   "source": [
    "import pyprojroot\n",
    "project_root_dir  = pyprojroot.here()\n",
    "print(project_root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the module for the webserver from our utilities directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the module for the web server we wrote\n",
    "import importlib.util\n",
    "spec = importlib.util.spec_from_file_location(\"PythonHttpFileServer\", \"../Utilities/PythonHttpFileServer.py\")\n",
    "PythonHttpFileServer = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(PythonHttpFileServer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure logging so that messages are collected and displayed asynchronously so that the server can run in the background without casuing a jupyter cell to block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Starting server on port 80\n",
      "INFO:root:Web root specified as: C:\\Users\\Administrator\\git\\ml-training-jupyter-notebooks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app 'PythonHttpFileServer' (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:werkzeug: * Running on all addresses.\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "INFO:werkzeug: * Running on http://15.1.1.23:80/ (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "# Configure the logger and log level\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Remove all handlers\n",
    "for handler in logger.handlers: \n",
    "    logger.removeHandler(handler)\n",
    "for handler in logger.handlers: \n",
    "    logger.removeHandler(handler)\n",
    "    \n",
    "# Start the webserver in a thread so the cell is not stuck in a running state\n",
    "import threading\n",
    "web_server_port = 80\n",
    "web_server_args = (web_server_port, project_root_dir)\n",
    "web_server_thread = threading.Thread(target=PythonHttpFileServer.run_server, args=web_server_args)\n",
    "web_server_thread.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Load The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instruct the spark cluster to download a file from the web server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_name = \"nasdaq_2019.csv\"\n",
    "csv_file_url = \"http://{0}:{1}/{2}\".format(ip_address, web_server_port, csv_file_name)\n",
    "sc.addFile(csv_file_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the utility function to convert a date string to a datetime object from our utilities module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the utilities module we wrote\n",
    "import importlib.util\n",
    "spec = importlib.util.spec_from_file_location(\"utilities\", \"../Utilities/utilities.py\")\n",
    "utilities = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(utilities)\n",
    "\n",
    "# Define a mapping to convert our data field to the correct type\n",
    "converter_mapping = {\n",
    "    \"date\": utilities.convert_date_string_to_date\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Load our OHCLV data Into a koalas dataframe and pull out a single day in the say way we would in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. Koalas will set it for you but it does not work if there is a Spark context already launched.\n",
      "INFO:spark:Patching spark automatically. You can disable it by setting SPARK_KOALAS_AUTOPATCH=false in your environment\n"
     ]
    }
   ],
   "source": [
    "from databricks import koalas\n",
    "koalas_dataframe = koalas.read_csv(u\"file:////nasdaq_2019.csv\", converters=converter_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should see the workers download the file in the logs. If we log into the nodes we can see the file is located on the filesystem root."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data loaded into a koalas dataframe we can access the data in the same way we would from a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>interval</th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AABA</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>70.90</td>\n",
       "      <td>71.5200</td>\n",
       "      <td>70.3250</td>\n",
       "      <td>70.57</td>\n",
       "      <td>10234800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAL</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>33.14</td>\n",
       "      <td>33.6632</td>\n",
       "      <td>32.5301</td>\n",
       "      <td>32.88</td>\n",
       "      <td>8995100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAME</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>2.43</td>\n",
       "      <td>2.4300</td>\n",
       "      <td>2.4000</td>\n",
       "      <td>2.40</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAOI</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>10.70</td>\n",
       "      <td>10.8900</td>\n",
       "      <td>10.0100</td>\n",
       "      <td>10.18</td>\n",
       "      <td>883100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAON</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>50.57</td>\n",
       "      <td>50.9850</td>\n",
       "      <td>48.5600</td>\n",
       "      <td>49.73</td>\n",
       "      <td>180200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker interval        date   open     high      low  close    volume\n",
       "0   AABA        D  2019-07-01  70.90  71.5200  70.3250  70.57  10234800\n",
       "1    AAL        D  2019-07-01  33.14  33.6632  32.5301  32.88   8995100\n",
       "2   AAME        D  2019-07-01   2.43   2.4300   2.4000   2.40       500\n",
       "3   AAOI        D  2019-07-01  10.70  10.8900  10.0100  10.18    883100\n",
       "4   AAON        D  2019-07-01  50.57  50.9850  48.5600  49.73    180200"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "koalas_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. We need to prepare our worker nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: We need to install relevant python libraries on the worker nodes. If you do not, you might see an error as follows:\n",
    "```\n",
    "PythonException: \n",
    "  An exception was thrown from the Python worker. Please see the stack trace below.\n",
    "Traceback (most recent call last):\n",
    "  File \"/usr/local/lib/python3.6/site-packages/pyspark/worker.py\", line 588, in main\n",
    "    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n",
    "  File \"/usr/local/lib/python3.6/site-packages/pyspark/worker.py\", line 421, in read_udfs\n",
    "    arg_offsets, f = read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=0)\n",
    "  File \"/usr/local/lib/python3.6/site-packages/pyspark/worker.py\", line 249, in read_single_udf\n",
    "    f, return_type = read_command(pickleSer, infile)\n",
    "  File \"/usr/local/lib/python3.6/site-packages/pyspark/worker.py\", line 69, in read_command\n",
    "    command = serializer._read_with_length(file)\n",
    "  File \"/usr/local/lib/python3.6/site-packages/pyspark/serializers.py\", line 160, in _read_with_length\n",
    "    return self.loads(obj)\n",
    "  File \"/usr/local/lib/python3.6/site-packages/pyspark/serializers.py\", line 430, in loads\n",
    "    return pickle.loads(obj, encoding=encoding)\n",
    "  File \"/usr/local/lib/python3.6/site-packages/pyspark/cloudpickle/cloudpickle.py\", line 562, in subimport\n",
    "    __import__(name)\n",
    "ModuleNotFoundError: No module named 'pandas'\n",
    "```\n",
    "\n",
    "In our case we needed to install pandas, numpy, koalas, scikit-learn, sklearn. If you are unsure of what is installed on your workers, we can log into the kubernetes pods and execute shell commands.\n",
    "\n",
    "Note: We must do this on all workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                        READY   STATUS    RESTARTS   AGE\n",
      "spark-jupyter-win-563a3b798591b8da-exec-1   1/1     Running   0          12h\n",
      "spark-jupyter-win-563a3b798591b8da-exec-2   1/1     Running   0          12h\n",
      "spark-jupyter-win-563a3b798591b8da-exec-4   1/1     Running   0          11h\n"
     ]
    }
   ],
   "source": [
    "! kubectl -n spark get pod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package         Version"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unable to use a TTY - input is not a terminal or the right kind of file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------- -------\n",
      "cycler          0.10.0\n",
      "joblib          1.0.1\n",
      "kiwisolver      1.3.1\n",
      "kneed           0.7.0\n",
      "koalas          1.8.0\n",
      "matplotlib      3.3.4\n",
      "numpy           1.19.5\n",
      "pandas          1.1.5\n",
      "Pillow          8.2.0\n",
      "pip             21.1.1\n",
      "progressbar     2.5\n",
      "py4j            0.10.9\n",
      "pyarrow         4.0.0\n",
      "pyparsing       2.4.7\n",
      "pyspark         3.1.1\n",
      "python-dateutil 2.8.1\n",
      "pytz            2021.1\n",
      "scikit-learn    0.24.2\n",
      "scipy           1.5.4\n",
      "setuptools      39.2.0\n",
      "six             1.16.0\n",
      "sklearn         0.0\n",
      "threadpoolctl   2.1.0\n"
     ]
    }
   ],
   "source": [
    "! kubectl -n spark exec -ti spark-jupyter-win-563a3b798591b8da-exec-1 -- pip3 list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Submit Python Code To Spark Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section of the notebook we are going to apply the kmeans algorithm from sklearn to each date in our koalas_dataframe object.\n",
    "To do this, we are going to write a function that applies the algorithm to a dataframe; the assumption being the dataframe only contains data related to the same date.\n",
    "Note: Most of this is a review and reworking of the content contained in \n",
    "<a href=\"../Cluster%20Analysis/K-Means.ipynb\">Cluster Analysis/K-Means.ipynb</a>.\n",
    "\n",
    "We create our data frame for testing based on a subset of our real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>interval</th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>93620</th>\n",
       "      <td>AABA</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>57.94</td>\n",
       "      <td>57.94</td>\n",
       "      <td>57.94</td>\n",
       "      <td>57.94</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93621</th>\n",
       "      <td>AAL</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>32.11</td>\n",
       "      <td>32.11</td>\n",
       "      <td>32.11</td>\n",
       "      <td>32.11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93622</th>\n",
       "      <td>AAME</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>2.41</td>\n",
       "      <td>2.41</td>\n",
       "      <td>2.41</td>\n",
       "      <td>2.41</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93623</th>\n",
       "      <td>AAOI</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>15.43</td>\n",
       "      <td>15.43</td>\n",
       "      <td>15.43</td>\n",
       "      <td>15.43</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93624</th>\n",
       "      <td>AAON</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>35.06</td>\n",
       "      <td>35.06</td>\n",
       "      <td>35.06</td>\n",
       "      <td>35.06</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ticker interval        date   open   high    low  close  volume\n",
       "93620   AABA        D  2019-01-01  57.94  57.94  57.94  57.94       0\n",
       "93621    AAL        D  2019-01-01  32.11  32.11  32.11  32.11       0\n",
       "93622   AAME        D  2019-01-01   2.41   2.41   2.41   2.41       0\n",
       "93623   AAOI        D  2019-01-01  15.43  15.43  15.43  15.43       0\n",
       "93624   AAON        D  2019-01-01  35.06  35.06  35.06  35.06       0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort based on the date column\n",
    "koalas_dataframe = koalas_dataframe.sort_values(\"date\")\n",
    "df_01_01_2019 = koalas_dataframe.loc[koalas_dataframe[\"date\"] == '2019-01-01'].copy()\n",
    "df_01_01_2019.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then write and test our function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "model = KMeans(n_clusters=5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_kmeans_on_dataframe3(df, column_names=[\"open\"]):\n",
    "    \n",
    "    # Create a copy of our dataframe so we can play around\n",
    "    tmp = df.copy()\n",
    "    columns = column_names.copy()\n",
    "\n",
    "    # IF we only supplied one column name, we will need to create a bogus column\n",
    "    # Kmeans requires a 2D array so we will add a static column\n",
    "    bogus_column = None\n",
    "    if len(column_names) < 2:\n",
    "        bogus_column = \"Y\"\n",
    "        tmp[bogus_column] = [1 for x in range(0, tmp.shape[0])]\n",
    "        columns.append(bogus_column)\n",
    "        \n",
    "    # Set the parameters for our model\n",
    "    # It expects a 2D array where the columns are our features\n",
    "    model_parameters = tmp[[*columns]].to_numpy()\n",
    "    \n",
    "    # Train the model\n",
    "    trained_model = model.fit(model_parameters)\n",
    "    \n",
    "    # Extract the information\n",
    "    cluster_indices = trained_model.labels_.astype(int)\n",
    "    if bogus_column in columns:\n",
    "        cluster_centroids = [trained_model.cluster_centers_[i][0] for i in cluster_indices]\n",
    "    else:\n",
    "        cluster_centroids = [str(trained_model.cluster_centers_[i].tolist()) for i in cluster_indices]\n",
    "        \n",
    "    # Update the dataframe (setting special options to allow koalas to work)\n",
    "#    option_value = koalas.get_option('compute.ops_on_diff_frames')\n",
    "#    koalas.set_option('compute.ops_on_diff_frames', True)\n",
    "\n",
    "    tmp[\"cluster_indices\"] = cluster_indices.tolist()\n",
    "    tmp[\"cluster_centroids\"] = cluster_centroids\n",
    "        \n",
    "#    koalas.set_option('compute.ops_on_diff_frames', option_value)\n",
    "    \n",
    "    # Determine which columns we want to return\n",
    "    columns = tmp.columns.to_list()\n",
    "    if bogus_column in columns:\n",
    "        columns.remove(bogus_column)\n",
    "    \n",
    "    return tmp[[*columns]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>interval</th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>cluster_indices</th>\n",
       "      <th>cluster_centroids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95460</th>\n",
       "      <td>MITK</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>10.81</td>\n",
       "      <td>10.81</td>\n",
       "      <td>10.81</td>\n",
       "      <td>10.81</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>[13.05660116731515, 13.05660116731515]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96515</th>\n",
       "      <td>TWNK</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>10.94</td>\n",
       "      <td>10.94</td>\n",
       "      <td>10.94</td>\n",
       "      <td>10.94</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>[13.05660116731515, 13.05660116731515]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94136</th>\n",
       "      <td>CELH</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>3.47</td>\n",
       "      <td>3.47</td>\n",
       "      <td>3.47</td>\n",
       "      <td>3.47</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>[13.05660116731515, 13.05660116731515]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95617</th>\n",
       "      <td>NNDM</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>1.11</td>\n",
       "      <td>1.11</td>\n",
       "      <td>1.11</td>\n",
       "      <td>1.11</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>[13.05660116731515, 13.05660116731515]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95363</th>\n",
       "      <td>LSBK</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>15.06</td>\n",
       "      <td>15.06</td>\n",
       "      <td>15.06</td>\n",
       "      <td>15.06</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>[13.05660116731515, 13.05660116731515]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ticker interval        date   open   high    low  close  volume  cluster_indices                       cluster_centroids\n",
       "95460   MITK        D  2019-01-01  10.81  10.81  10.81  10.81       0                2  [13.05660116731515, 13.05660116731515]\n",
       "96515   TWNK        D  2019-01-01  10.94  10.94  10.94  10.94       0                2  [13.05660116731515, 13.05660116731515]\n",
       "94136   CELH        D  2019-01-01   3.47   3.47   3.47   3.47       0                2  [13.05660116731515, 13.05660116731515]\n",
       "95617   NNDM        D  2019-01-01   1.11   1.11   1.11   1.11       0                2  [13.05660116731515, 13.05660116731515]\n",
       "95363   LSBK        D  2019-01-01  15.06  15.06  15.06  15.06       0                2  [13.05660116731515, 13.05660116731515]"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perform_kmeans_on_dataframe3(df_01_01_2019, column_names=[\"open\", \"close\"]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\spark\\spark-3.1.1-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\group_ops.py:84: UserWarning: It is preferred to use 'applyInPandas' over this API. This API will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  \"more details.\", UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>interval</th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>cluster_indices</th>\n",
       "      <th>cluster_centroids</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2019-01-01</th>\n",
       "      <th>93620</th>\n",
       "      <td>AABA</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>57.94</td>\n",
       "      <td>57.94</td>\n",
       "      <td>57.94</td>\n",
       "      <td>57.94</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>67.532737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93621</th>\n",
       "      <td>AAL</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>32.11</td>\n",
       "      <td>32.11</td>\n",
       "      <td>32.11</td>\n",
       "      <td>32.11</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>13.056601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93622</th>\n",
       "      <td>AAME</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>2.41</td>\n",
       "      <td>2.41</td>\n",
       "      <td>2.41</td>\n",
       "      <td>2.41</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>13.056601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93623</th>\n",
       "      <td>AAOI</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>15.43</td>\n",
       "      <td>15.43</td>\n",
       "      <td>15.43</td>\n",
       "      <td>15.43</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>13.056601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93624</th>\n",
       "      <td>AAON</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>35.06</td>\n",
       "      <td>35.06</td>\n",
       "      <td>35.06</td>\n",
       "      <td>35.06</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>13.056601</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ticker interval        date   open   high    low  close  volume  cluster_indices  cluster_centroids\n",
       "date                                                                                                                \n",
       "2019-01-01 93620   AABA        D  2019-01-01  57.94  57.94  57.94  57.94       0                0          67.532737\n",
       "           93621    AAL        D  2019-01-01  32.11  32.11  32.11  32.11       0                2          13.056601\n",
       "           93622   AAME        D  2019-01-01   2.41   2.41   2.41   2.41       0                2          13.056601\n",
       "           93623   AAOI        D  2019-01-01  15.43  15.43  15.43  15.43       0                2          13.056601\n",
       "           93624   AAON        D  2019-01-01  35.06  35.06  35.06  35.06       0                2          13.056601"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_01_01_2019.groupby(\"date\").apply(perform_kmeans_on_dataframe3, column_names=[\"open\"]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run this function against out dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\spark\\spark-3.1.1-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\group_ops.py:84: UserWarning: It is preferred to use 'applyInPandas' over this API. This API will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  \"more details.\", UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>interval</th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>cluster_indices</th>\n",
       "      <th>cluster_centroids</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2019-01-01</th>\n",
       "      <th>93620</th>\n",
       "      <td>AABA</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>57.94</td>\n",
       "      <td>57.94</td>\n",
       "      <td>57.94</td>\n",
       "      <td>57.94</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[67.53273664825046, 67.53273664825046]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93621</th>\n",
       "      <td>AAL</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>32.11</td>\n",
       "      <td>32.11</td>\n",
       "      <td>32.11</td>\n",
       "      <td>32.11</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>[13.05660116731514, 13.05660116731514]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93622</th>\n",
       "      <td>AAME</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>2.41</td>\n",
       "      <td>2.41</td>\n",
       "      <td>2.41</td>\n",
       "      <td>2.41</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>[13.05660116731514, 13.05660116731514]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93623</th>\n",
       "      <td>AAOI</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>15.43</td>\n",
       "      <td>15.43</td>\n",
       "      <td>15.43</td>\n",
       "      <td>15.43</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>[13.05660116731514, 13.05660116731514]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93624</th>\n",
       "      <td>AAON</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>35.06</td>\n",
       "      <td>35.06</td>\n",
       "      <td>35.06</td>\n",
       "      <td>35.06</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>[13.05660116731514, 13.05660116731514]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ticker interval        date   open   high    low  close  volume  cluster_indices                       cluster_centroids\n",
       "date                                                                                                                                     \n",
       "2019-01-01 93620   AABA        D  2019-01-01  57.94  57.94  57.94  57.94       0                0  [67.53273664825046, 67.53273664825046]\n",
       "           93621    AAL        D  2019-01-01  32.11  32.11  32.11  32.11       0                2  [13.05660116731514, 13.05660116731514]\n",
       "           93622   AAME        D  2019-01-01   2.41   2.41   2.41   2.41       0                2  [13.05660116731514, 13.05660116731514]\n",
       "           93623   AAOI        D  2019-01-01  15.43  15.43  15.43  15.43       0                2  [13.05660116731514, 13.05660116731514]\n",
       "           93624   AAON        D  2019-01-01  35.06  35.06  35.06  35.06       0                2  [13.05660116731514, 13.05660116731514]"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "koalas_dataframe.groupby(\"date\").apply(perform_kmeans_on_dataframe3, column_names=[\"open\",\"close\"]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Cleanup Spark Cluster On Kubernetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                        READY   STATUS        RESTARTS   AGE\n",
      "spark-jupyter-win-563a3b798591b8da-exec-1   1/1     Terminating   0          12h\n",
      "spark-jupyter-win-563a3b798591b8da-exec-2   1/1     Terminating   0          12h\n",
      "spark-jupyter-win-563a3b798591b8da-exec-4   1/1     Terminating   0          11h\n"
     ]
    }
   ],
   "source": [
    "! kubectl -n spark get pod"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
