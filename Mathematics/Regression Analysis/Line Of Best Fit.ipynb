{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "798fda96-701c-4aa0-95fd-e9414160fa32",
   "metadata": {},
   "source": [
    "# Overview\n",
    "In this notebook we discuss the concept of a line of best fit. The line of best fit is a tool that appears in the field of regression analysis. As the name suggests linear regression is the process of studdying and modeling a process using a linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b2ad73-27f5-4870-a369-ce6269aa869d",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "- [Linearity](../Linearity/Linearity.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df92a8c-5a04-46ca-ab87-f7ee35b914eb",
   "metadata": {},
   "source": [
    "# 1. Line Of Best Fit\n",
    "## 1.1. Overview\n",
    "The line of best fit is a linear function we use to model our data. Conceptually it is typically a straight line that passes though our data in the \"best possible way\". Mathematically we can state how this \"best possible way\" is quantified.\n",
    "\n",
    "One way is to say the line of best fit minimizes the error (differences) between the points of the model (regression) $\\hat y$ and the observed data points $y$. This difference is expressed mathematically as $(\\hat y - y)$.\n",
    "\n",
    "Another definition that frequently appears is a line which maximizes the variance explained by the model. This refers to the cumulative distance between points in the data set and the main of the data set. The data set is maximized (placing the line in the center of the observed data). This is written mathematically as $(y - \\bar y)^2$.\n",
    "\n",
    "<img src=\"images/Line%20Of%20Best%20Fit%20And%20Variance.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830070a5-dd30-4462-99fd-ddc2c06a840a",
   "metadata": {},
   "source": [
    "## 1.2. The Simple Univariate Case\n",
    "\n",
    "Given an independent variable $x$ which maps to a dependent variable $y$, we can mathematically denot the equation as:\n",
    "\n",
    "$$ y = mx + b$$\n",
    "\n",
    "Given a sample of data for $x$ and $y$ we can construct a line of best fit through the data:\n",
    "\n",
    "$$ \\hat y = \\hat m x + \\hat b $$\n",
    "\n",
    "Where the \"hat\" terms represent our approximations. \n",
    "\n",
    "We can define the error in our regression as \n",
    "\n",
    "$$ \\epsilon = y - \\hat y $$\n",
    "\n",
    "Doing some mathematical substitution we see:\n",
    "\n",
    "$$ y = \\hat y + \\epsilon $$\n",
    "\n",
    "$$ 0 = y - \\hat m x - \\hat b - \\epsilon $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5ca665-83f7-4302-aa70-ff953124d214",
   "metadata": {},
   "source": [
    "## 1.2. The General Multivariate Case\n",
    "When using $n$ dimensions, a multidimensional line can be represented as\n",
    "\n",
    "$$ Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_n X_n $$\n",
    "\n",
    "Where $\\beta_0$ is the y-intercept, and $\\beta_i$ is the \"weight\" or \"regression coefficient\" assigned to each variable.\n",
    "\n",
    "Given a sample of data for $X$ and $Y$ we can construct a line of best fit through the data:\n",
    "\n",
    "$$ \\hat Y = \\hat\\beta_0 + \\hat\\beta_1 X_1 + \\hat\\beta_2 X_2 + \\cdots + \\hat\\beta_n X_n$$\n",
    "\n",
    "Rewriting this in summation notation we have:\n",
    "\n",
    "$$ Y = \\beta_0 + \\sum^n_i \\beta_i X_i$$\n",
    "\n",
    "In matrix notation this would be:\n",
    "\n",
    "$$ Y = \\beta_0 + \\beta^T X $$\n",
    "\n",
    "It is equivalent to represent the equation as:\n",
    "\n",
    "$$ Y = \\beta_0 + X \\beta^T $$\n",
    "\n",
    "In some notations, the equation is simplified by assuming $X_0=1$ and combining the lone $\\beta_0$ into the larger $\\beta$ term:\n",
    "\n",
    "$$\n",
    "\\beta = \n",
    "\\begin{pmatrix}\n",
    "\\beta_0 \\\\\n",
    "\\beta_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\beta_n\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "X = \n",
    "\\begin{pmatrix}\n",
    "1       & X_{1,1} & \\cdots & X_{n,1} \\\\\n",
    "1       & X_{1,2} & \\cdots & X_{n,2} \\\\\n",
    "\\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
    "1       & X_{1,m} & \\cdots & X_{n,m} \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "$$ Y = \\beta^T X $$\n",
    "\n",
    "**Note**: In the case of no y-intercept, $X_0 = 0$ and is omitted from the algebra.\n",
    "\n",
    "Given a sample of data for $X$ and $Y$ we can construct a line of best fit through the data:\n",
    "\n",
    "$$ \\hat Y = \\hat \\beta^T X $$\n",
    "\n",
    "We can define the error in our regression as \n",
    "\n",
    "$$ \\epsilon = Y - \\hat Y $$\n",
    "\n",
    "Doing some mathematical substitution we see:\n",
    "\n",
    "$$ Y = \\hat Y + \\epsilon $$\n",
    "\n",
    "$$ 0 = Y - X \\beta^T - \\epsilon $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354973eb-02f0-4808-8c6e-9a2728df2a32",
   "metadata": {},
   "source": [
    "# 2. Finding The Regression Coefficients\n",
    "There are many ways to find the coefficients, we will use Ordinary Least Squares (OLS).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13612f39-ec5a-48f1-80e5-995edcaec92a",
   "metadata": {},
   "source": [
    "## 2.1. Ordinary Least Squares (OLS)\n",
    "\n",
    "The basic idea is to find coefficients which minimize the difference between the actual data points of the dependent variable $y$ and those predicted by the regression line $\\hat y$ (ie. minimize $ y-\\hat y $).\n",
    "\n",
    "Mathematically speaking, we will construct a closed form a loss function that we want to minimize. For a simple linear regression (univariate) we would have the follwoing:\n",
    "\n",
    "$$ L = \\sum(y-\\hat y)^2 $$\n",
    "\n",
    "$$ = \\sum(y - \\hat m x - \\hat b)^2   $$\n",
    "\n",
    "For a multiple regression (multivariate) our loss function would be\n",
    "\n",
    "$$ L = (Y - \\hat Y)^T(Y - \\hat Y) $$\n",
    "\n",
    "$$ L = (Y - \\hat \\beta_0 - {\\hat \\beta}^T X)^T(Y - \\hat \\beta_0 - {\\hat \\beta}^T X) $$\n",
    "\n",
    "We can solve each of these closed form solutions below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2079ec-66db-468b-b3c2-6c0eaf1fe42b",
   "metadata": {},
   "source": [
    "### 3.2.1. Solve Univariate OLS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534a50df-1199-4105-ab46-20e0f402ad12",
   "metadata": {},
   "source": [
    "#### 3.2.1.1. Find $\\hat b$\n",
    "\n",
    "We can take the derivative of the loss function using the chain rule, power rule, and the property of additivity\n",
    "\n",
    "let $u = y - \\hat m x + \\hat b$\n",
    "\n",
    "$$ L = \\sum u ^2 $$\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial \\hat b} = 2 \\sum u \\ du \\  d \\hat b $$\n",
    "\n",
    "$$ = 2 \\sum (y - \\hat m x - \\hat b)(-1) $$\n",
    "\n",
    "$$ = -2 \\sum (y - \\hat m x - \\hat b) $$\n",
    "\n",
    "We then set the equation equal to 0, simplify, and solve\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial \\hat b} = 0 $$\n",
    "\n",
    "$$ 0 = -2 \\sum (y - \\hat m x - \\hat b) $$\n",
    "\n",
    "$$ 0 = \\sum (y - \\hat m x - \\hat b) $$\n",
    "\n",
    "$$ 0 = \\sum y - \\sum \\hat m x - \\sum \\hat b $$\n",
    "\n",
    "$$ \\sum y = \\sum \\hat m x + \\sum \\hat b $$\n",
    "\n",
    "$$ \\sum^n y = \\sum^n \\hat m x + n \\hat b $$\n",
    "\n",
    "$$ \\frac{\\sum^n y}{n} = \\frac{\\sum^n \\hat m x}{n} + \\hat b $$\n",
    "\n",
    "$$ \\hat b = \\frac{\\sum^n y}{n} - \\frac{\\sum^n \\hat m x}{n} $$\n",
    "\n",
    "$$ \\hat b = \\bar y - \\frac{\\sum^n \\hat m x}{n} $$\n",
    "\n",
    "$$ \\hat b = \\bar y - \\hat m \\bar x $$\n",
    "\n",
    "We can calculate the  mean values ($\\bar x$ and $\\bar y$) so really the only variable about which to minimize is $m$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898d8408-0170-40f5-896c-a07fc9e0ee07",
   "metadata": {},
   "source": [
    "#### 3.2.1.2 Find $\\hat m$\n",
    "\n",
    "We repeat the process as before\n",
    "\n",
    "let $u = y - \\hat m x + \\hat b$\n",
    "\n",
    "$$ L = \\sum u ^2 $$\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial \\hat m} = 2 \\sum u \\ du \\  d \\hat b $$\n",
    "\n",
    "$$ = 2 \\sum (y - \\hat m x - \\hat b)(-x) $$\n",
    "\n",
    "$$ = -2x \\sum (y - \\hat m x - \\hat b) $$\n",
    "\n",
    "We then set the equation equal to 0, simplify, and solve\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial \\hat m} = 0 $$\n",
    "\n",
    "$$ 0 = -2\\sum (y - \\hat m x - \\hat b)x $$\n",
    "\n",
    "$$ 0 = \\sum (y - \\hat m x - \\hat b)x $$\n",
    "\n",
    "Now substitute the value of $ \\hat b = \\bar y - \\hat m \\bar x $\n",
    "\n",
    "$$ 0 = \\sum (y - \\hat m x - \\bar y + \\hat m \\bar x)x $$\n",
    "\n",
    "We know $x$ is not trivial\n",
    "\n",
    "$$ 0 = \\sum (y - \\bar y + \\hat m \\bar x - \\hat m x ) $$\n",
    "\n",
    "$$ 0 = \\sum (y - \\bar y) - \\hat m \\sum ( \\bar x - x) $$\n",
    "\n",
    "$$ \\hat m \\sum (- \\bar x + x) = \\sum (y - \\bar y) $$\n",
    "\n",
    "$$ \\hat m \\sum (x - \\bar x) = \\sum (y - \\bar y) $$\n",
    "\n",
    "\n",
    "$$ \\hat m  = \\frac{\\sum (y - \\bar y)}{\\sum ( x - \\bar x)} $$\n",
    "\n",
    "By doing one more manipulation we see that $\\hat m$ is equal to the pearson correlation coefficient. If we multiply by one, $\\frac{\\sum ( x - \\bar x)}{\\sum ( x - \\bar x)}$, this becomes clear.\n",
    "\n",
    "$$ \\hat m  = \\frac{\\sum (y - \\bar y)( x - \\bar x)}{\\sum ( x - \\bar x)^2} $$\n",
    "\n",
    "With this derevation, $\\hat m$ and $\\hat b$ are now solavable and the minimum point can be obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7029a94-7512-47d6-8683-f4741215000e",
   "metadata": {},
   "source": [
    "### 3.2.2. Solve Multivariate OLS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6272ca03-df63-434c-a337-ba787cefe200",
   "metadata": {},
   "source": [
    "#### 3.2.2.1. Find $\\hat \\beta$\n",
    "\n",
    "We start by expanding our loss function\n",
    "\n",
    "$$ L = (Y - \\hat Y)^T(Y - \\hat Y) $$\n",
    "\n",
    "$$ = (Y - {\\hat \\beta}^T X)^T(Y - {\\hat \\beta}^T X) $$\n",
    "\n",
    "$$ = (Y^T - {\\hat \\beta} X^T)(Y - {\\hat \\beta}^T X) $$\n",
    "\n",
    "We perform the multiplication\n",
    "\n",
    "$$ = Y^T Y - Y^T \\hat \\beta X^T - \\hat \\beta X^T Y + \\hat \\beta X^T \\hat \\beta^T X$$\n",
    "\n",
    "The transpose of a scalar is equal to that scalar\n",
    "\n",
    "$$ = Y^T Y - Y^T \\hat \\beta X^T - Y^T \\hat \\beta X^T + \\hat \\beta X^T \\hat \\beta^T X$$\n",
    "\n",
    "$$ = Y^T Y - 2 Y^T \\hat \\beta X^T + \\hat \\beta X^T \\hat \\beta^T X$$\n",
    "\n",
    "We then take the derivative and set it equal to zero\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial \\hat \\beta} = 0$$ \n",
    "\n",
    "$$ \\frac{\\partial }{\\partial \\hat \\beta} \\left[ Y^T Y - 2 Y^T \\hat \\beta X^T + \\hat \\beta X^T \\hat \\beta^T X  \\right]= 0$$\n",
    "\n",
    "$$  - 2 Y X^T + 2 \\hat \\beta X^T X = 0$$\n",
    "\n",
    "$$ 2 \\hat \\beta X^T X = 2 Y X^T $$\n",
    "\n",
    "$$ \\hat \\beta X^T X = X^T Y$$\n",
    "\n",
    "If we multiply a matrix by its inverse we get the identity matrix $I$\n",
    "\n",
    "$$ (X^T X)^{-1} (X^T X) \\hat \\beta = (X^T X)^{-1} X^T Y $$\n",
    "\n",
    "$$ I \\hat \\beta = (X^T X)^{-1} X^T Y$$\n",
    "\n",
    "$$ \\hat \\beta = (X^T X)^{-1} X^T Y $$\n",
    "\n",
    "Additional notes can be found [here](https://web.stanford.edu/~mrosenfe/soc_meth_proj3/matrix_OLS_NYU_notes.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085e1e65-6e7c-4183-b0ab-10caccf2af91",
   "metadata": {},
   "source": [
    "# References\n",
    "- https://builtin.com/data-science/step-step-explanation-principal-component-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b60620f-d4a9-4c2b-a0e0-6118c9cb52f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19a7149-63cf-46aa-819b-7eafd1668bc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
