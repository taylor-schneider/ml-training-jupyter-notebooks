{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d165979-220a-4f8f-b04e-992f257b0032",
   "metadata": {},
   "source": [
    "# Overview\n",
    "In this notebook we establish basic properites and proofs related to the normal distribution family. This includes the general normal distribution as well as the standard normal which is a special case of the normal distribution.\n",
    "\n",
    "Note: The normal distribution is also referred to as the Gaussian distribution or Laplace-Gauss to acknowledge the discoverers..\n",
    "\n",
    "It's important to note that the standard normal distributions are particularely important because of the connection with the law of large numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0103bf7-c8a3-4c9b-88ed-ce0f22ed45a7",
   "metadata": {},
   "source": [
    "# 1. Univariate Normal Distribution\n",
    "\n",
    "## 1.1. Overview\n",
    "## 1.2. Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee278a06-512d-4869-a645-12f964eadfe2",
   "metadata": {},
   "source": [
    "We start with the definition of the univariate gaussian distribution:\n",
    "\n",
    "$$ f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}\n",
    "e^{-\\frac{1}{2}\\left( \\frac{x - \\mu}{\\sigma} \\right)^2}  \n",
    "\\tag{1.2.1}\n",
    "$$\n",
    "\n",
    "Which can also be stated using the $exp()$ function rather than the symbol $e$:\n",
    "\n",
    "$$ = \\frac{1}{\\sigma\\sqrt{2\\pi}}\n",
    "exp \\left\\{ -\\frac{1}{2}\\left( \\frac{x - \\mu}{\\sigma} \\right)^2  \\right\\} \n",
    "\\tag{1.2.2} \n",
    "$$\n",
    "\n",
    "It is also common to reformulate the expression in terms of variance $\\sigma^2$ rather than the standard deviation $\\sigma$. This is particularaly useful once working in multimensional spaces where $\\sigma^2$ is denoted as $\\Sigma$.\n",
    "\n",
    "$$ = \\frac{1}{\\sigma\\sqrt{2\\pi}}\n",
    "exp \\left\\{ -\\frac{1}{2}\n",
    "\\frac{(x - \\mu)^2}{\\sigma^2}  \\right\\}  \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9782d14f-1365-4a9a-bc59-aeca5612033c",
   "metadata": {},
   "source": [
    "### 1.2.3. Intuition\n",
    "Below are a few interesting notes on the structure of the equation:\n",
    "\n",
    "We can think of the leaging term $\\frac{1}{\\sigma\\sqrt{2\\pi}}$ as a \"normalization factor\" which is used to ensure that the integral of the function sums to 1 and satisfies the axioms of a probability space.\n",
    "\n",
    "Another interesting observation/connection is the quadratic expression in the exponential is what gives the distribution it's shape. We see that an even power keeps the expression inside the exponential negative which yields our non-zero values. This connection with the quadratic is an important connection that will surface again when this equation is generalized to n-dimenaions.\n",
    "\n",
    "<center><img src='images/normal_distribution_quadratic.png' height='400px' width='600px'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc40f50-0372-4c34-bb57-2932b4860055",
   "metadata": {},
   "source": [
    "## 1.3. Derevation\n",
    "\n",
    "We can see the makings for the derevation in the proof for the CLT mentioned in section 2 of this article.\n",
    "\n",
    "We see the original derevation, which is standard normal, can be generalized by expressing a variable as a linear combination. This also applies to the derevation of a multivariate normal distribution.\n",
    "\n",
    "A proof of deriving the general normal distribution function can be found [here](https://web.sonoma.edu/users/w/wilsonst/papers/Normal/default.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11299caf-e474-4f46-a2d9-f29218bdc243",
   "metadata": {},
   "source": [
    "## 1.3. History\n",
    "\n",
    "The formula for the normal density function was discovered by Abraham de Moivre in 1738 while he was working on solving a gambling problem. The solution depended on finding the sum of the terms of a binomial distribution. Ultimately de Movre proved that the binomial distribution converged to the gaussian (standard normal) distribution. A nice history on the subject can be found [here](https://higherlogicdownload.s3.amazonaws.com/AMSTAT/1484431b-3202-461e-b7e6-ebce10ca8bcd/UploadedImages/Classroom_Activities/HS_2__Origin_of_the_Normal_Curve.pdf). You can also find this information in *A History of Probability and Statistics and Their Applications Before 1750* and the follow up volume by Anders Hald.\n",
    "\n",
    "\n",
    "We can see a proof of de Moivre's claim [here](https://noahgolmant.com/writings/derivationsunivariatemultivariate.pdf). \n",
    "\n",
    "This discovery laid the fondations for the discovery of the CLT.\n",
    "\n",
    "**TODO** Something to verify:\n",
    "> Gauss and the Irish American mathematician Robert Adrain first derived the normal distribution as the only continuous distribution for which the sample mean is the value that maximises what Fisher later called the likelihood function, i.e. the joint probability of the observations considered as a function of the parameter—the actual observations being known. \n",
    ">\n",
    "> https://www.quora.com/How-did-humans-derive-the-normal-distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47f06d7-b5d3-49bc-a29d-e15aeea205a8",
   "metadata": {},
   "source": [
    "# 2. Central Limit Theorem (CLT)\n",
    "\n",
    "## 2.1. Overview\n",
    "\n",
    "## 2.2. Definition\n",
    "Using the characteristic function of a random variable it can be shown that a finite sequence $S_n$ of independent identically idstributed ($iid.$) variables with a common mean and variance will converge to a standard normal distribution.\n",
    "\n",
    "$$ S_n = \\sum X_i $$\n",
    "\n",
    "$$ \\mu = \\mathbb{E}[X_i] $$\n",
    "\n",
    "$$ \\sigma = Var[X_i] $$\n",
    "\n",
    "$$\\lim\\limits_{n \\to \\infty}\n",
    "\\mathbb{P}\\left( \\frac{S_n - n\\mu}{\\sigma \\sqrt{n}} \\le c\\right)\n",
    "= \\phi(c)\n",
    "= \\int_{-\\infty}^{c} \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{1}{2}x^2} dx\n",
    "\\tag{2.2.1}\n",
    "$$\n",
    "\n",
    "## 2.3. Derevation\n",
    "\n",
    "The proof relies on the characteristic function, fourier transform, and algebra.\n",
    "\n",
    "Proof can be found [here](https://noahgolmant.com/writings/derivationsunivariatemultivariate.pdf)\n",
    "\n",
    "## 2.4. History\n",
    "See section 1.3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fb12ca-cbd5-4867-9eb5-c18fa4e11a51",
   "metadata": {},
   "source": [
    "# 3. Multivariate Normal Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ace15e-aaff-4746-8695-3c48eacabf71",
   "metadata": {},
   "source": [
    "## 3.1. Overview\n",
    "\n",
    "The multivariate normal (MVN) distribution is a probability distribution that models the linear combination of independent standard normal random variables. In other words it models a joint random variable composed of standard normal random variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabcebbc-ec8c-4be9-9106-9715effd76e2",
   "metadata": {},
   "source": [
    "## 3.2. Definition\n",
    "There are many many definitions for a multivariate normal distribution floating around. To put things simply, a MVN distribution is a symetric probability distribution which is completely described by it's two moments $\\mu$ and $\\Sigma$. This translates into two constraints: the distribution functions (pdf/cdf) must satisty the axioms of a probability space and the structure of the function must contain certain structural elements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83119c3-5b3f-406c-a2fd-14591fa642e5",
   "metadata": {},
   "source": [
    "## 3.3. Derivation\n",
    "\n",
    "There are several methods to derive the multivariate normal distribution that I am aware of.\n",
    "\n",
    "> To my knowledge, there are two primary approaches to developing the theory of multivariate Gaussian distributions. The first, and by far the most common approach in machine learning textbooks, is to define the multivariate gaussian distribution in terms of its density function, and to derive results by manipulating these density functions. With this approach, a lot of the work turns out to be elaborate matrix algebra calculations happening inside the exponent of the Gaussian density. One issue with this approach is that the multivariate Gaussian density is only defined when the covariance matrix is invertible. To keep the derivations rigorous, some care must be taken to justify that the new covariance matrices we come up with are invertible. For my taste, I find the rigor in our textbooks to be a bit light on these points. We’ve included the proof to Theorem 4 to give a flavor of the details one should add. The second major approach to multivariate Gaussian distributions does not use density functions at all and does not require invertible covariance matrices. This approach is much cleaner and more elegant, but it relies on the theory of characteristic functions and the Cramer-Wold device to get started, and these are beyond the prerequisites for this course. You can often find this development in more advanced probability and statistics books, such as Rao’s excellent Linear Statistical Inference and Its Applications (Chapter 8).\n",
    ">\n",
    "> https://davidrosenberg.github.io/mlcourse/in-prep/multivariate-gaussian.pdf\n",
    "\n",
    "\n",
    "My derrevation of the multivariate normal distribution starts with a derrevation assuming variables are independent and then matures to abandon that presumption. It also relies on several other prerequisite proofs which we will establish below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990fe19a-583b-4270-b5b5-d49d1e501803",
   "metadata": {},
   "source": [
    "### 3.3.1. Setup Space\n",
    "\n",
    "We define objects prerequisite to our proofs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15ba0a1-75b8-4cbc-8c89-e39e6ac01cc9",
   "metadata": {},
   "source": [
    "\"A\" for afine transform??\n",
    "\n",
    "\n",
    "$$A = \\begin{bmatrix}\n",
    "a_{i,k}\n",
    "\\end{bmatrix}, \\ for \\ i,k \\in \\{1, 2, \\cdots, n\\} $$\n",
    "\n",
    "$$ X = \\begin{bmatrix}\n",
    "X_1 \\\\\n",
    "X_2 \\\\\n",
    "\\vdots \\\\\n",
    "X_n\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "$$ X_i \\sim \\mathcal{N} $$\n",
    "\n",
    "$$ Z = \\begin{bmatrix}\n",
    "Z_1 \\\\\n",
    "Z_2 \\\\\n",
    "\\vdots \\\\\n",
    "Z_n\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "$$Z_i \\sim \\mathcal{N}(0,1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b20995-fc9b-4e81-837e-7514445399e5",
   "metadata": {},
   "source": [
    "### 3.3.2 Derive Joint Distribution For Independent Univariate Normal Variables\n",
    "\n",
    "In this proof we will derive the joint density function and thus the joint distribution for a set of random variables $X$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2fcd9b-2e65-4a7d-b9bc-27095d5e0b3c",
   "metadata": {},
   "source": [
    "We start by acknowledging that the variables are independent. The joint distribution tells us the probability of a set of variables (in our case two) realizing specific values at the same time.\n",
    "\n",
    "Using the definition of conditional probability we know that\n",
    "\n",
    "$$ f_X = p(X_1 | X_2) = \\frac{p(X_1 \\cap X_2)}{p(X_2)}$$\n",
    "\n",
    "However, using the definition of independence, the joint probability for this specific case can be restated as:\n",
    "\n",
    "$$ p(X_1 \\cap X_2) = p(X_1 | X_2)p(X_2)$$\n",
    "\n",
    "$$ p(X_1 \\cap X_2) = p(X_1)p(X_2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfbaf4d-a318-4e36-9cf7-e0a901ed3f6c",
   "metadata": {},
   "source": [
    "Generalizing this to n-dimensions we have:\n",
    "\n",
    "$$ p(X_1 \\cap X_2 \\cap \\cdots \\cap X_n) = \\prod p(X_i) $$\n",
    "\n",
    "When $X_i \\perp X_j \\ \\ \\forall i,j$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985439b3-11f9-4b74-b395-de248f6de12e",
   "metadata": {},
   "source": [
    "Injecting the unviariate normal density equation (1.3.1) we have:\n",
    "\n",
    "$$ = \\prod \n",
    "\\frac{1}{\\sigma_{X_i}\\sqrt{2\\pi}}\n",
    "exp \\left\\{ -\\frac{1}{2}\n",
    "\\frac{(X_i - \\mu_{X_i})^2}{\\sigma_{X_i}^2}  \\right\\}  \n",
    "$$\n",
    "\n",
    "$$ = \n",
    "\\frac{1}{\\sqrt[n]{2\\pi}}\n",
    "\\prod \n",
    "\\frac{1}{\\sigma_{X_i}}\n",
    "exp \\left\\{ -\\frac{1}{2}\n",
    "\\frac{(X_i - \\mu_{X_i})^2}{\\sigma_{X_i}^2}  \\right\\}  \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc57aff-3e9f-4a59-a2ed-97be9352a03c",
   "metadata": {
    "tags": []
   },
   "source": [
    "Removing the exponential term from the product operator using the properties of exponents $(x^ax^b = x^{a+b})$ we have:\n",
    "\n",
    "$$ = \n",
    "\\frac{1}{\\sqrt[n]{2\\pi}}\n",
    "exp \n",
    "\\left\\{ \n",
    "-\\frac{1}{2}\n",
    "\\frac{(X_i - \\mu_{X_i})^2}{\\sigma_{X_i}^2}  \n",
    "\\cdots\n",
    "-\\frac{1}{2}\n",
    "\\frac{(X_n - \\mu_{X_n})^2}{\\sigma_{X_n}^2}  \n",
    "\\right\\} \n",
    "\\prod \n",
    "\\frac{1}{\\sigma_{X_i}} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5c359d-b053-4569-b738-32e7f7800794",
   "metadata": {
    "tags": []
   },
   "source": [
    "Adjusting the esponent in the exponential term to use multi-dimensional matrix notation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b811b117-fb24-4097-af21-a91880a06698",
   "metadata": {},
   "source": [
    "$$ = \n",
    "\\frac{1}{\\sqrt[n]{2\\pi}}\n",
    "exp \n",
    "\\left\\{ \n",
    "-\\frac{1}{2}\n",
    "(X - \\mu_{X})^T\\Sigma^{-1}(X - \\mu_{X})  \n",
    "\\right\\} \n",
    "\\prod \n",
    "\\frac{1}{\\sigma_{X_i}} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474f7850-ab3a-44ad-8735-39b02a28894c",
   "metadata": {},
   "source": [
    "We can finally remove the product operator by introducing the covariance matrix amd its determinant. The covariance matrix will hold the variances $\\sigma_{X_i} = \\sqrt{\\Sigma_i}$ of the $X_i$. The determinant will perform the operation of multiplying the diagonals of the covariance matrix. If we take the square root of the covariance matrix or the determiniant we then have an equivalent result as the product operator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a1b0e9-d417-4e73-93f9-0d44edae54e9",
   "metadata": {},
   "source": [
    "$$ = \n",
    "\\frac{1}{\\sqrt[n]{2\\pi}}\n",
    "exp \n",
    "\\left\\{ \n",
    "-\\frac{1}{2}\n",
    "(X - \\mu_{X})^T\\Sigma^{-1}(X - \\mu_{X})  \n",
    "\\right\\} \n",
    "\\frac{1}{\\sqrt{|\\Sigma_{X_i}|}} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd8777a-1999-41fb-a992-57e05561578a",
   "metadata": {},
   "source": [
    "$$ = \n",
    "\\frac{1}{\\sqrt{(2\\pi)^n |\\Sigma_{X_i}|}}\n",
    "exp \n",
    "\\left\\{ \n",
    "-\\frac{1}{2}\n",
    "(X - \\mu_{X})^T\\Sigma^{-1}(X - \\mu_{X})  \n",
    "\\right\\}\n",
    "\\tag{3.3.2.1}\n",
    "$$\n",
    "\n",
    "This this we now have a mulativariate density function for mutually independent normal random variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ec5993-107d-4916-8eee-4e22cc61b4f8",
   "metadata": {},
   "source": [
    "If the variables are standard normal such that $Z_i \\sim \\mathcal{N}(0,1)$ we can reduce the equation even further to see that:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54feb95-c058-4ff2-ba82-7508f1b3a0c7",
   "metadata": {},
   "source": [
    "$$ f_Z(Z) = \n",
    "\\frac{1}{\\sqrt{(2\\pi)^n}}\n",
    "exp \n",
    "\\left\\{ \n",
    "-\\frac{1}{2}\n",
    "ZZ^T \n",
    "\\right\\} \n",
    "\\tag{3.3.2.2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3175e662-7a22-4037-8163-dd4d0f3b56e8",
   "metadata": {},
   "source": [
    "But the assumption we made about univariate random variables may not be appropriate for our situation, we may want to extend things a bit further."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439f25b4-6927-4ea9-ba11-2491ba189c6f",
   "metadata": {},
   "source": [
    "### 3.3.3 Derive General Formula for Joint Normal Distribution\n",
    "\n",
    "In this proof we will further generalize (3.2.2).\n",
    "\n",
    "We will do this using the basic laws of probability and change of variables to define the deneral formula in terms of the previously derived standard iid. formula."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13806699-a47d-45ab-a998-63bca5e28aac",
   "metadata": {},
   "source": [
    "#### 3.3.3.1. Define Linear combination Of Standard Normal Random Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4a021a-7d15-45a9-9f53-afb181603c32",
   "metadata": {},
   "source": [
    "We define a new random variable $Y$ that is a linear transformation of $Z$. The difference here is that in this case $Z$ is multiavariate where in 3.3.2 it was univariate. As such $Z$ defines a joint variable that spans multiple dimensions. Its components $Z_i$ are the mutually independent univariate standard normal random variables.\n",
    "\n",
    "$$ Y = c_1Z_1 + \\cdots + c_nZ_n + d$$\n",
    "\n",
    "$$ = cZ+d $$\n",
    "\n",
    "$$ = t(Z) $$\n",
    "\n",
    "Note: We see here that the $Y$ defined in 3.3.2. was such that $c=1$ and $d=0$. See 3.4.1 and 3.4.2 for additional verification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b36a62d-4b95-44d4-9425-d22ae86caef6",
   "metadata": {},
   "source": [
    "#### 3.3.3.2. Create An Equality Between Probability Spaces\n",
    "We know that the probability density function of $Z$ and $Y$ must add up to 1 when summed over the corresponding supports (non-trival domains) in order to satisfy the axioms of a probability space. \n",
    "\n",
    "$$ \\underset{S_Z} {\\int \\cdots \\int} f_Z(Z) \\ \\partial Z = 1$$\n",
    "\n",
    "$$ \\underset{S_Y} {\\int \\cdots \\int} f_Y(Y) \\ \\partial Y = 1$$\n",
    "\n",
    "\n",
    "As such we can construct an equality which serves as the backbone of our derrevation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5014c4-9c8f-4b88-aede-980d4b5ec1e1",
   "metadata": {},
   "source": [
    "$$ \\underset{S_Z} {\\int \\cdots \\int} f_Z(Z) \\ \\partial Z =  \\underset{S_Y} {\\int \\cdots \\int} f_Y(Y) \\ \\partial Y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f8af06-1061-484d-81ff-4c355e4853cd",
   "metadata": {},
   "source": [
    "#### 3.3.3.3. Perform Change Of Variable\n",
    "We can then perform a change of variable on this equation to restate the left hand side in terms of $Y$ rather than $Z$. The general change ov variable for multiple integrals formula is as follows:\n",
    "\n",
    "$$ \\underset{S_Y} {\\int \\cdots \\int} \n",
    "f_Z\\left[ t^{-1}(Y) \\right] |\\mathbb{J}_{t^{-1}}|\n",
    "\\ \\partial Z \n",
    "=\n",
    "\\underset{S_Y} {\\int \\cdots \\int} f_Y(Y) \\ \\partial Y$$\n",
    "\n",
    "This is a bit confusing so lets build up some intuition:\n",
    "\n",
    "Our functions are well behaved, so we can use inversion to restate our transformation using an inverse function.\n",
    "\n",
    "$$ Y = cZ + d = t(Z) $$\n",
    "\n",
    "$$\\Rightarrow Z = t^{-1}(Y) $$\n",
    "\n",
    "$$ \\Rightarrow t^{-1}(Y) = \\frac{Y - d}{c} $$\n",
    "\n",
    "$$ Z = \\frac{Y - d}{c} $$\n",
    "\n",
    "\n",
    "We can substitute $Z$ with the inverse transform of $t$. This will restate the integral interms of $Y$ but this will not complete the change of variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb706d1-97b8-44db-9d4d-37a29f9219a7",
   "metadata": {},
   "source": [
    "$$ \\underset{S_Z} {\\int \\cdots \\int} f_Z\\left[ t^{-1}(Y) \\right] \\ \\partial Z \n",
    "=\n",
    "\\underset{S_Y} {\\int \\cdots \\int} f_Y(Y) \\ \\partial Y$$\n",
    "\n",
    "$$ \\underset{S_Z} {\\int \\cdots \\int} f_Z\\left[ \\frac{Y - d}{c} \\right] \\ \\partial Z \n",
    "=\n",
    "\\underset{S_Y} {\\int \\cdots \\int} f_Y(Y) \\ \\partial Y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab988a2e-8032-4c94-b585-952533e11e64",
   "metadata": {},
   "source": [
    "Recall that the multiple integreal over the support represents an n-dimensional area. As such we will need to convert the area from one unit/space to another. In other words we need to modify the integral so that it is covering $S_Z$ rather than $S_Y$ without changing the actual sum.\n",
    "\n",
    "We will see that the reason we need both sides in terms of $Y$ is because we will eventually take the derivative of both sides with respect to $Y$.\n",
    "\n",
    "\n",
    "We change the bounds of the integral using the Jacobian determinant $|\\mathbb{J}_t|$ of our transformation $t$. Essentially we have:\n",
    "\n",
    "$$ S_Y = |\\mathbb{J}_t| S_Z$$\n",
    "\n",
    "$$ S_X = |\\mathbb{J}_t|^{-1} S_Z $$\n",
    "\n",
    "$$ |\\mathbb{J}_t|^{-1} = |\\mathbb{J}_t^{-1}| = |\\mathbb{J}_{t^{-1}}|$$\n",
    "\n",
    "Given $t(Z) = cZ+d$ we have \n",
    "\n",
    "$$ |\\mathbb{J}_t| = c $$\n",
    "\n",
    "\n",
    "$$ \\Rightarrow |\\mathbb{J}_t^{-1}| \n",
    "= \\frac{1}{|\\mathbb{J}_t|}\n",
    "= \\begin{vmatrix}\\frac{1}{c}\\end{vmatrix}\n",
    "$$\n",
    "\n",
    "For more information see the [Jacobian Notebook](../Matrix%20Algebra/Jacobian.ipynb).\n",
    "\n",
    "If we plug this information into the equation we complete the change of variable within the integral and thus have a general cumulative density functionfor $Y$.\n",
    "\n",
    "$$ \\underset{S_Y} {\\int \\cdots \\int} \n",
    "f_Z\\left[ t^{-1}(Y) \\right] |\\mathbb{J}_{t^{-1}}|\n",
    "\\ \\partial Z \n",
    "=\n",
    "\\underset{S_Y} {\\int \\cdots \\int} f_Y(Y) \\ \\partial Y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b539f621-e8ac-4a79-82a9-d7166d4e77f2",
   "metadata": {},
   "source": [
    "$$ \\underset{S_Y} {\\int \\cdots \\int} \n",
    "f_Z\\left[ \\frac{Y-d}{c} \\right] \\frac{1}{c}\n",
    "\\ \\partial Z \n",
    "=\n",
    "\\underset{S_Y} {\\int \\cdots \\int} f_Y(Y) \\ \\partial Y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e828520-9675-4f76-8afc-a7fd1b0e1b2b",
   "metadata": {},
   "source": [
    "#### 3.3.3.4. Differentiate Both Sides\n",
    "We can now take the derivative of each side which yields our general solution for the probability density function.\n",
    "\n",
    "$$ f_Y(Y) = f_Z\\left[ t^{-1}(Y) \\right] |\\mathbb{J}_t^{-1}|$$\n",
    "\n",
    "$$ = f_Z\\left[ \\frac{Y-d}{c} \\right] \\begin{vmatrix}\\frac{1}{c}\\end{vmatrix} $$\n",
    "\n",
    "Note: This technique will work for any joint distirbution we may observe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82222f6d-99fd-474c-b340-3ab5810cea91",
   "metadata": {},
   "source": [
    "#### 3.3.3.5. Plug In Standard Normal Density Function\n",
    "Up until this point, nothing required that the variables $Z_i$ be standard normal or independent. We can plug in any distribution at this point. The problem with straying away from a standard normal variable is that the math becomes very complicated. We will look at this in 3.3.3.6.\n",
    "\n",
    "For now, we can now derive a solution specific to the standard normal distribution by plugging in the density function of $Z$ and the jacobian determinant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511327c6-44fa-4b9b-99fc-14582ad7b059",
   "metadata": {},
   "source": [
    "$$ = \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(\\frac{Y-d}{c}\\right)^2}\\frac{1}{|c|}$$\n",
    "\n",
    "$$ = \\frac{1}{\\sqrt{2\\pi}|c|}e^{-\\frac{1}{2}\\left(\\frac{Y-d}{c}\\right)^2}$$\n",
    "\n",
    "$$ = \\frac{1}{\\sqrt{2\\pi |c^2|}}e^{-\\frac{1}{2}\\left(\\frac{Y-d}{c}\\right)^2}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25abfd5b-bde5-4784-ac40-6a196025b67a",
   "metadata": {},
   "source": [
    "By looking at the structure of this equation we can now see clearly that this is a normal pdf as it closely reseembles the univariate normal distribution and the multivariate iid. distribution derived previously."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf881a5c-bcf9-4d26-ade5-7f457ba2c162",
   "metadata": {},
   "source": [
    "To make this extremely clear we can switch notation, let $d$ be the mean of $Y$ such that \n",
    "\n",
    "$$d = \\mu_Y$$ \n",
    "\n",
    "and let $c^2$ be the variance of $Y$ such that \n",
    "\n",
    "$$c^2 = \\Sigma_Y$$\n",
    "\n",
    "Plugging this notation into our solution yields:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76e63d0-c329-4760-b1d8-906ca4dde81c",
   "metadata": {},
   "source": [
    "$$ = \\frac{1}{\\sqrt{2\\pi |\\Sigma_Y|}}e^{-\\frac{1}{2}\\left(\\frac{Y-\\mu_Y}{\\sqrt{\\Sigma_Y}}\\right)^2}$$\n",
    "\n",
    "$$ = \\frac{1}{\\sqrt{2\\pi |\\Sigma_Y|}}e^{-\\frac{1}{2} (Y-\\mu)\\Sigma_Y^{-1}(Y-\\mu)} \\tag{3.3.3.5}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beffb1e1-7b7b-413f-9e8f-f77afea30fbf",
   "metadata": {},
   "source": [
    "#### 3.3.3.6. Define Linear Combination of Non-Standard Normal Variables\n",
    "As mentioned previously, the usage of the joint standard normal distribution was for mathematical convenience. Below we will continue generalizing by assuming that $Y$ is a linear combination of arbitrary normal random variables rather than standard normal random variables.\n",
    "\n",
    "\n",
    "Define $Y$ in terms of $X\\sim\\mathcal{N}(\\mu_X,\\Sigma_X)$ instead of $Z\\sim\\mathcal{Z}(0,1)$\n",
    "\n",
    "$$ y = cX + d $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff7097e-23f2-46f9-bf58-1fd074159170",
   "metadata": {},
   "source": [
    "Pickup at the 3.3.3.4 equation where we differentiate the inverse transform\n",
    "\n",
    "$$ f_Y(Y) = f_X\\left[ t^{-1}(Y) \\right] |\\mathbb{J}_t^{-1}|$$\n",
    "\n",
    "$$ = f_X\\left[ \\frac{Y-d}{c} \\right] \\begin{vmatrix}\\frac{1}{c}\\end{vmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c8c8d4-8653-4c76-b2fd-7121a2d29764",
   "metadata": {},
   "source": [
    "We inject the value of $X= \\frac{Y-d}{c}$ into the pdf\n",
    "\n",
    "$$ f_Y(Y) = \\frac{1}{\\sqrt{2\\pi |\\Sigma_X|}}e^{-\\frac{1}{2}\\left(\\frac{X-\\mu_X}{\\sqrt{\\Sigma_X}}\\right)^2}$$\n",
    "\n",
    "$$ = \\frac{1}{\\sqrt{2\\pi |\\Sigma_X|}}e^{-\\frac{1}{2}\\left(\\frac{\\left(\\frac{Y-d}{c}\\right)-\\mu_X}{\\sqrt{\\Sigma_X}}\\right)^2}$$\n",
    "\n",
    "$$ = \\frac{1}{\\sqrt{2\\pi |\\Sigma_X|}}\n",
    "e^{\n",
    "-\\frac{1}{2}\n",
    "\\left(\n",
    "\\frac{\\left(\\frac{Y-d}{c}\\right)-\\frac{c\\mu_X}{c}}{\\sqrt{\\Sigma_X}}\n",
    "\\right)^2}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1d21a3-b57b-4938-a39d-cabb9d7c8b81",
   "metadata": {},
   "source": [
    "$$ = \\frac{1}{\\sqrt{2\\pi |\\Sigma_X|}}\n",
    "e^{\n",
    "-\\frac{1}{2}\n",
    "\\left(\n",
    "    \\frac{Y-d-c\\mu_X}{c\\sqrt{\\Sigma_X}}\n",
    "\\right)^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0943b8-78e1-475d-ad96-04a57bdf1142",
   "metadata": {},
   "source": [
    "From 3.4.1. we know $\\mu_Y = c\\mu_X + d$ and thus\n",
    "\n",
    "$$ = \\frac{1}{\\sqrt{2\\pi |\\Sigma_X|}}\n",
    "e^{\n",
    "-\\frac{1}{2}\n",
    "\\left(\n",
    "    \\frac{Y- \\mu_Y}{c\\sqrt{\\Sigma_X}}\n",
    "\\right)^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af42a877-01ef-4c1e-accf-12731276f231",
   "metadata": {},
   "source": [
    "From 3.4.2. we know $\\Sigma_Y = c^2 \\Sigma_X = c\\Sigma_Xc^T$ and thus\n",
    "\n",
    "$$ = \\frac{1}{\\sqrt{2\\pi |\\Sigma_X|}}\n",
    "e^{\n",
    "-\\frac{1}{2}\n",
    "(Y- \\mu_Y)\\Sigma_Y^{-1}(Y- \\mu_Y)\n",
    "}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ddbc88-e42f-46e1-a9c2-8ac4f4871903",
   "metadata": {},
   "source": [
    "From 3.4.2 we deduce that $\\Sigma_X = c^{-2}\\Sigma_Y$ so we can manipulate the equation even farther and find:\n",
    "\n",
    "$$ = \\frac{1}{\\sqrt{2\\pi |c^{-2}\\Sigma_Y|}}\n",
    "e^{\n",
    "-\\frac{1}{2}\n",
    "(Y- \\mu_Y)\\Sigma_Y^{-1}(Y- \\mu_Y)\n",
    "}$$\n",
    "\n",
    "$$ = \\frac{1}{\\sqrt{2\\pi |c^{2}\\Sigma_Y|}}\n",
    "e^{\n",
    "-\\frac{1}{2}\n",
    "(Y- \\mu_Y)\\Sigma_Y^{-1}(Y- \\mu_Y)\n",
    "}$$\n",
    "\n",
    "$$ = \\frac{1}{\\sqrt{2\\pi |c\\Sigma_Yc^T|}}\n",
    "e^{\n",
    "-\\frac{1}{2}\n",
    "(Y- \\mu_Y)\\Sigma_Y^{-1}(Y- \\mu_Y)\n",
    "} \\tag{3.3.3.6}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b953c3f-e387-48a2-baa2-c871fa0d9f3d",
   "metadata": {},
   "source": [
    "## 3.4. Properties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6f1cb3-7ae8-4b4c-b518-bdf46c0a376e",
   "metadata": {},
   "source": [
    "### 3.4.1. Mean of Multivariate Normal\n",
    "\n",
    "Given $Y = cX + d$ we will see that $\\mu_Y = c\\mu_X$\n",
    "\n",
    "We start by taking the expectation:\n",
    "\n",
    "$$ \\mathbb{E}[Y] = \\mathbb{E}[cX + d] $$\n",
    "\n",
    "$$ \\mathbb{E}[cX] + \\mathbb{E}[d]$$\n",
    "\n",
    "$$ c\\mathbb{E}[X] + d$$\n",
    "\n",
    "\n",
    "$$ c\\mu_X + d \\tag{3.4.1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ba2f0e-ae4f-4424-9efe-e0a461d22d82",
   "metadata": {},
   "source": [
    "### 3.4.2. Variance of Multivariate Normal\n",
    "\n",
    "Start with the basic variance formulas\n",
    "\n",
    "$$\\mathbb{Var}[Y] = \\mathbb{E}[(Y - \\mu_Y)^2]$$\n",
    "\n",
    "$$= \\mathbb{E}[Y^2 - 2\\mu_Y Y + \\mu_Y^2]$$\n",
    "\n",
    "$$= \\mathbb{E}[Y^2] - \\mathbb{E}[2\\mu_Y Y] + \\mathbb{E}[\\mu_Y^2]$$\n",
    "\n",
    "$$= \\mathbb{E}[Y^2] - 2\\mu_Y\\mathbb{E}[ Y] + \\mu_Y^2$$\n",
    "\n",
    "$$= \\mathbb{E}[Y^2] - \\mu_Y\\mathbb{E}[ Y]$$\n",
    "\n",
    "$$= \\mathbb{E}[Y^2] - \\mathbb{E}[Y]^2$$\n",
    "\n",
    "Introduce the linear combination formula\n",
    "\n",
    "$$= \\mathbb{E}[(cX + d)^2] - \\mathbb{E}[cX+d]^2$$\n",
    "\n",
    "$$= \\mathbb{E}[(cX + d)(cX + d)] - (c\\mu_X+d)^2$$\n",
    "\n",
    "$$= \\mathbb{E}[c^2X^2+2cdX+d^2] - (c\\mu_X+d)(c\\mu_X+d)$$\n",
    "\n",
    "$$= \\mathbb{E}[c^2X^2]+\\mathbb{E}[2cdX]+\\mathbb{E}[d^2] - c^2\\mu_X^2 - 2cd\\mu_X - d^2$$\n",
    "\n",
    "$$= c^2\\mathbb{E}[X^2]+ 2cd\\mathbb{E}[X]+ d^2 - c^2\\mu_X^2 - 2cd\\mu_X - d^2$$\n",
    "\n",
    "$$= c^2\\mathbb{E}[X^2] + 2cd\\mu_X + d^2 - c^2\\mu_X^2 - 2cd\\mu_X - d^2$$\n",
    "\n",
    "$$= c^2\\mathbb{E}[X^2] - c^2\\mu_X^2$$\n",
    "\n",
    "$$= c^2\\mathbb{E}[X^2] - c^2\\mathbb{E}[X]^2$$\n",
    "\n",
    "$$= c^2(\\mathbb{E}[X^2] - \\mathbb{E}[X]^2)$$\n",
    "\n",
    "\n",
    "$$= c^2 \\mathbb{Var}[X] $$\n",
    "\n",
    "$$= c\\Sigma c^t \\tag{3.4.2}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97253e92-0338-4fa1-a4a3-20b5d1872ecd",
   "metadata": {},
   "source": [
    "### 3.4.3. Other Moments Of Multivariate Normal\n",
    "The other moments of the multivariate normal random variable can be derived using the moment generating function. For a refresher on moment generating functions see the [notebook on expectations and MGFs](../../Statistics/Expectation.ipynb#Moment-Generation-Function)\n",
    "\n",
    "Recall that the multivariate normal random variable is defined as $Y=cX + d$. The moment generating function $M_Y(c)$ for $Y$ can be derived as follows:\n",
    "\n",
    "$$ M_Y(t) = \\mathbb{E}[e^{tY}] $$ \n",
    "\n",
    "$$ = \\mathbb{E}[e^{t(cX + d)}] $$ \n",
    "\n",
    "$$ = \\mathbb{E}[e^{tc_1X_1 + \\cdots + tc_nX_n + td}] $$\n",
    "\n",
    "$$ = \\mathbb{E}[e^{tc_1X_1}]\\cdots\\mathbb{E}[e^{tc_nX_n}]\\mathbb{E}[e^{td}] $$\n",
    "\n",
    "$$ = M_{X_1}(tc_1)\\cdots M_{X_n}(tc_n)e^{td} $$ \n",
    "\n",
    "$$ = e^{td}\\prod M_{X_i}(tc_i) \\tag{3.4.3}$$ \n",
    "\n",
    "We can then differentiate this function as needed to find the coresponding moment.\n",
    "\n",
    "More on this subject can be found [here](https://online.stat.psu.edu/stat414/lesson/25/25.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f9305f-b9e8-4a72-b5e4-d000a739a62d",
   "metadata": {},
   "source": [
    "### 3.4.4. Linear Combination of Independent Normal Random Variables is Normal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d86cc1-6142-48f2-96fd-678fea91af78",
   "metadata": {},
   "source": [
    "One property that makes the normal distribution extremely tractable from an analytical viewpoint is its closure under linear combinations: the linear combination of two independent normal distributions is a normal distribution. And the linear combination of normal random variables is a normal random variable.\n",
    "\n",
    "https://www.statlect.com/probability-distributions/normal-distribution-linear-combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46de76a4-ac0d-4c75-9f4a-726ef925c09f",
   "metadata": {},
   "source": [
    "There are several way to prove this. A convenient method is to show that the moment generating function of a linear transformation has the same form as the joint normal distribution.\n",
    "\n",
    "We start from (3.4.3) and plug in the MGF for a normal random variable\n",
    "\n",
    "$$ M_X(t) =  e^{\\left( \\mu t + \\frac{\\sigma^2 t^2}{2} \\right)} $$\n",
    "\n",
    "$$ M_Y(t) = e^{td}\\prod M_{X_i}(tc_i)$$\n",
    "\n",
    "$$ = e^{td}\\prod e^{\\left( \\mu tc_i + \\frac{\\sigma^2 (tc_i)^2}{2} \\right)}$$\n",
    "\n",
    "$$ = e^{td}\\prod e^{\\mu tc_i} e^{\\frac{\\sigma^2 (tc_i)^2}{2}}$$\n",
    "\n",
    "$$ = exp\\big\\{td\\big\\}\\prod exp\\big\\{ \\mu tc_i \\big\\} exp\\Bigg\\{\\frac{\\sigma^2 (tc_i)^2}{2}\\Bigg\\}$$\n",
    "\n",
    "$$ = exp\\big\\{td\\big\\}exp\\big\\{ \\sum \\mu tc_i \\big\\} exp\\Bigg\\{\\sum \\frac{\\sigma^2 (tc_i)^2}{2}\\Bigg\\}$$\n",
    "\n",
    "$$ = exp\\big\\{td\\big\\}exp\\big\\{ t\\sum \\mu c_i \\big\\} exp\\Bigg\\{\\frac{t^2}{2}\\sum \\frac{\\sigma^2 c_i^2}{2}\\Bigg\\}$$\n",
    "\n",
    "$$ = exp\\big\\{td\\big\\}exp\\big\\{ t\\sum \\mu c_i \\big\\} exp\\Bigg\\{\\frac{t^2}{2}\\sum \\frac{\\sigma^2 c_i^2}{2}\\Bigg\\}$$\n",
    "\n",
    "$$ = exp\\big\\{td\\big\\}exp\n",
    "\\Bigg\\{ \n",
    "t \\left(\\sum \\mu c_i \\right) + \\frac{t^2}{2} \\left( \\sum \\sigma^2 c_i^2 \\right)\n",
    "\\Bigg\\}$$\n",
    "\n",
    "Based on the structure we see that this equation now looks like $M_X(t)$ but now the mean is $\\sum \\mu c_i$ and the variance is $\\sum \\sigma^2 c_i^2$. And if $d = 0$ this becomes even more clear.\n",
    "\n",
    "Therefore the multivariate normal distribution has the same moment generating function as a univariate normal distribution but uses different parameters. Thus it is normally distributed. As such we have proved that the linear combination of normal random variables is normal.\n",
    "\n",
    "\n",
    "https://online.stat.psu.edu/stat414/lesson/26/26.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a588c59-253e-4b0e-9de3-5a444ac19f8a",
   "metadata": {},
   "source": [
    "### 3.4.5 Marginal Distributions Of Joint Normal Are Normal\n",
    "\n",
    "Previously in (3.3.3) we derived the multivariate normal distribution from a set of normal random variables with normal marginal distributions. Now we show that the marginal distributions of the multivariate normal must also be normal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa6857a-656b-4bc4-b5d0-57dac4ed997c",
   "metadata": {},
   "source": [
    "Let $c$ be an $(r \\times n)$ matrix and let $Y$ be a linear combination of normal random variables $X$.\n",
    "\n",
    "If $Y=cX$, then we know $Y \\sim \\mathcal{N}(C\\mu, C\\Sigma^{-1}C^T)$ by the proof of linear combination shown in (3.4.1) through (3.4.4) we can deduces that the marginal distributions of the multivariate normal are also normal.\n",
    "\n",
    "We can apply a transform that essentially splits the vector $X$ into the two pieces. In doing so we can represent $X$ as a linar combination of $X_1$ and $X_2$. Let $X_1$ be an $(1 \\times r)$ matrix while $X_2$ is an $(1 \\times n-r)$ matrix, we would then have:\n",
    "\n",
    "$$X = \\begin{bmatrix}X_1 \\\\ X_2 \\end{bmatrix}$$\n",
    "\n",
    "We can create a linear transformation $c$ to transform $X$ into its coresponding parts $X_1$ and $X_2$ as follows:\n",
    "\n",
    "For example, to derive $X_1$, we can multiply $X$ by $c_1=(I, 0)$ where $I$ is the $(r \\times r)$ identity matrix, and $0$ is the $(r-n \\times r-n)$ matrix.\n",
    "\n",
    "By defining C in this way, we can say:\n",
    "\n",
    "$$ X_1 = C_1X $$\n",
    "\n",
    "We already proved that the linear combination of normal variables is normal. We are now showing that a marginal variable can be represented as a linear combination of a multivariate variable. As such that marginal variable must be normally distributed!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae014df0-eed5-466c-8d03-964ca8cbd507",
   "metadata": {},
   "source": [
    "### 3.4.6. Closure\n",
    "\n",
    "Gaussian distributions have the nice algebraic property of being closed under conditioning and marginalization. Being closed under conditioning and marginalization means that the resulting distributions from these operations are also Gaussian, which makes many problems in statistics and machine learning tractable.\n",
    "\n",
    "https://distill.pub/2019/visual-exploration-gaussian-processes/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2cea02-de05-4837-9e7e-b5f5c6e5faeb",
   "metadata": {},
   "source": [
    "### 3.4.7. Transformation\n",
    "\n",
    "$$\\mathcal{N}(\\mu,\\sigma^2) = \\mu + \\sigma^2 \\mathcal{N}(0,1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d6f168-0e46-41d4-a4be-6c4a18c077b9",
   "metadata": {},
   "source": [
    "## Proof: Joint distribution Of Independent Variables\n",
    "\n",
    "We will prove that if $X \\sim \\mathcal{N}$ then $X_1 | X_2 \\sim \\mathcal{N}$ with mean $\\mu_{X_1|X_2} = \\mu_1 + \\Sigma_{12} \\Sigma_{22}^{-1}[X_2 - \\mu_2]$ and a variance $\\Sigma_{x_1|X_2}=\\Sigma_{11}-\\Sigma_{12}\\Sigma_{22}^{-1}\\Sigma_{22}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95790dec-62c1-48f0-9fa0-75f5e9170a95",
   "metadata": {},
   "source": [
    "Recall:\n",
    "\n",
    "$$ Y = \\begin{bmatrix}\n",
    "Y_1 \\\\\n",
    "Y_2 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$ X = \\begin{bmatrix}\n",
    "X_1 \\\\\n",
    "X_2 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Define:\n",
    "\n",
    "$$Y_1 = X_1 - \\Sigma_{12} \\Sigma_{22}^{-1} X_{2}$$\n",
    "$$ Y_2 = X_2$$\n",
    "\n",
    "Rewiriting this system of equations in matrix form we have:\n",
    "\n",
    "$$\n",
    "Y = \\begin{bmatrix}\n",
    "1 & -\\Sigma_{12}\\Sigma_{22}^{-1} \\\\\n",
    "0 & I\n",
    "\\end{bmatrix} \n",
    "X\n",
    "$$\n",
    "\n",
    "Similar to the proof involving linear combinations we can represent $\\begin{bmatrix}\n",
    "1 & -\\Sigma_{12}\\Sigma_{22}^{-1} \\\\\n",
    "0 & I\n",
    "\\end{bmatrix}$ as $C$\n",
    "\n",
    "$$Y = CX$$\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "Y_1 \\\\\n",
    "Y_2 \\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "1 & -\\Sigma_{12}\\Sigma_{22}^{-1} \\\\\n",
    "0 & I\n",
    "\\end{bmatrix} \n",
    "\\begin{bmatrix}\n",
    "X_1 \\\\\n",
    "X_2 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "Y_1 \\\\\n",
    "Y_2 \\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "X_1 & -\\Sigma_{12}\\Sigma_{22}^{-1}X_2 \\\\\n",
    "0 & X_2\n",
    "\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "Recall that $X_1$ has $r$ datapoints while $X_2$ has $n-r$ points and $I$ is the $(r-n \\times r-n)$ identity matrix,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89acd451-3642-4775-a44a-bf468c8808b9",
   "metadata": {},
   "source": [
    "We can see that similar to the proof involving linear combinations, $Y$ is a non-singular linear transform of $X$ and is therfore normal. Additionally we proved that if $Y = CX$ then $\\mu_Y = C\\mu$ and $\\Sigma_Y = C \\Sigma C^T)$. \n",
    "\n",
    "#### Derive the mean\n",
    "So we can deduce the mean\n",
    "\n",
    "$$ \\mathbb{E}[Y] = C \\mathbb{E}[X]$$\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\mathbb{E}[Y_1] \\\\\n",
    "\\mathbb{E}[Y_2] \\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "1 & -\\Sigma_{12}\\Sigma_{22}^{-1} \\\\\n",
    "0 & I\n",
    "\\end{bmatrix} \n",
    "\\begin{bmatrix}\n",
    "\\mathbb{E}[X_1] \\\\\n",
    "\\mathbb{E}[X_2] \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\mathbb{E}[Y_1] \\\\\n",
    "\\mathbb{E}[Y_2] \\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\mathbb{E}[X_1] & -\\Sigma_{12}\\Sigma_{22}^{-1}\\mathbb{E}[X_2] \\\\\n",
    "0 & \\mathbb{E}[X_2]\n",
    "\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "Which yields:\n",
    "\n",
    "$$ \\mathbb{E}[Y_1] = -\\Sigma_{12}\\Sigma_{22}^{-1}\\mathbb{E}[X_2]$$\n",
    "$$ \\mathbb{E}[Y_2] = \\mathbb{E}[X_2]$$\n",
    "\n",
    "For compactness:\n",
    "\n",
    "$$ \\mu_{Y_1} = \\mu_{X_1} - \\Sigma_{12} \\Sigma_{22}^{-1} \\mu_{X_2}$$\n",
    "$$ \\mu_{Y_2} = \\mu_{X_2}$$\n",
    "\n",
    "If we apply these to our original definitions we see\n",
    "\n",
    "$$Y_1 = X_1 - \\Sigma_{12} \\Sigma_{22}^{-1} X_{2}$$\n",
    "$$ => Y_1 = X_1 - \\mu_{Y_1}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd89a25f-5761-41c3-8bac-90916df4da0b",
   "metadata": {},
   "source": [
    "#### Derive the variance\n",
    "We calculate the vairance of Y\n",
    "\n",
    "$$\\mathbb{E}[Y] = \\mathbb{E}[(Y - \\mu_Y)(Y - \\mu_Y)^T] $$\n",
    "\n",
    "$$= \\mathbb{E}\\begin{bmatrix}\n",
    "\\Sigma_{11} & \\Sigma_{12}\\\\\n",
    "\\Sigma_{21} & \\Sigma_{22}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "$$= \\mathbb{E}\\begin{bmatrix}\n",
    "(Y_1 - \\mu_{Y_1})(Y_1 - \\mu_{Y_1})^T & (Y_1 - \\mu_{Y_1})(Y_2 - \\mu_{Y_2})^T \\\\\n",
    "(Y_2 - \\mu_{Y_2})(Y_1 - \\mu_{Y_1})^T & (Y_2 - \\mu_{Y_2})(Y_2 - \\mu_{Y_2})^T\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671ebb11-ef6b-4dc3-97c4-8ace49f0b6c4",
   "metadata": {},
   "source": [
    "We start with $\\Sigma_{11}$:\n",
    "\n",
    "$$ \\Sigma_{11} = \\mathbb{E}[(Y_1 - \\mu_{Y_1})(Y_1 - \\mu_{Y_1})^T] $$\n",
    "\n",
    "We enforce the conditions $Y_1 = X_1-\\Sigma_{12}\\Sigma_{22}^{-1} X_{2}$ and $\\mu_{Y_1} = \\mu_{X_1} - \\Sigma_{12} \\Sigma_{22}^{-1} \\mu_{X_2}$ on a single term\n",
    "\n",
    "$$ Y_1 - \\mu_{Y_1}$$\n",
    "\n",
    "$$ = (X_1-\\Sigma_{12}\\Sigma_{22}^{-1} X_{2})-(\\mu_{X_1} - \\Sigma_{12} \\Sigma_{22}^{-1} \\mu_{X_2})$$\n",
    "\n",
    "$$ = X_1-\\Sigma_{12}\\Sigma_{22}^{-1} X_{2} - \\mu_{X_1} + \\Sigma_{12} \\Sigma_{22}^{-1} \\mu_{X_2}$$\n",
    "\n",
    "$$ = X_1 - \\mu_{X_1} -\\Sigma_{12}\\Sigma_{22}^{-1} X_{2} + \\Sigma_{12} \\Sigma_{22}^{-1} \\mu_{X_2}$$\n",
    "\n",
    "$$ = X_1 - \\mu_{X_1} -\\Sigma_{12}\\Sigma_{22}^{-1}( X_{2} - \\mu_{X_2})$$\n",
    "\n",
    "We then apply it to both terms\n",
    "\n",
    "$$ \\mathbb{E}\\left[ (Y_1 - \\mu_{Y_1})(Y_1 - \\mu_{Y_1})^T \\right]$$\n",
    "\n",
    "$$ = \\mathbb{E}\\left[ \n",
    "\\left( X_1 - \\mu_{X_1} -\\Sigma_{12}\\Sigma_{22}^{-1}( X_{2} - \\mu_{X_2}) \\right)\n",
    "\\left( X_1 - \\mu_{X_1} -\\Sigma_{12}\\Sigma_{22}^{-1}( X_{2} - \\mu_{X_2}) \\right)^T\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "$$ = \\mathbb{E}\\left[\n",
    "\\left( (X_1 - \\mu_{X_1}) -\\Sigma_{12}\\Sigma_{22}^{-1}( X_{2} - \\mu_{X_2}) \\right)\n",
    "\\left( (X_1 - \\mu_{X_1}) -\\Sigma_{12}\\Sigma_{22}^{-1}( X_{2} - \\mu_{X_2}) \\right)^T\n",
    "\\right]\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828b11e6-74b1-46c5-8ace-78ce4c7c8f56",
   "metadata": {},
   "source": [
    "$$ = \\mathbb{E}\\left[\n",
    "(X_1 - \\mu_{X_1})(X_1 - \\mu_{X_1})^T\n",
    "- \\Sigma_{12}\\Sigma_{22}^{-1}( X_{2} - \\mu_{X_2})(X_1 - \\mu_{X_1})^T\n",
    "- \\Sigma_{12}\\Sigma_{22}^{-1}( X_{2} - \\mu_{X_2})(X_1 - \\mu_{X_1})^T\n",
    "+ \\Sigma_{12}\\Sigma_{22}^{-1}( X_{2} - \\mu_{X_2})\n",
    "(\\Sigma_{12}\\Sigma_{22}^{-1})^T( X_{2} - \\mu_{X_2})^T\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a813f71c-c576-4408-9edb-d51e67334cbc",
   "metadata": {},
   "source": [
    "$$ =\n",
    "\\mathbb{E}\\left[\n",
    "(X_1 - \\mu_{X_1})(X_1 - \\mu_{X_1})^T\n",
    "- 2\\Sigma_{12}\\Sigma_{22}^{-1}( X_{2} - \\mu_{X_2})(X_1 - \\mu_{X_1})^T\n",
    "+ \\Sigma_{12}\\Sigma_{22}^{-1}( X_{2} - \\mu_{X_2})\n",
    "(\\Sigma_{12}\\Sigma_{22}^{-1})^T( X_{2} - \\mu_{X_2})^T\n",
    "\\right]\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d9ddb4-ccbd-4d15-b72d-d15685ce89c9",
   "metadata": {},
   "source": [
    "$$ =\n",
    "\\mathbb{E}\\left[\n",
    "(X_1 - \\mu_{X_1})(X_1 - \\mu_{X_1})^T\n",
    "- 2\\Sigma_{12}\\Sigma_{22}^{-1}( X_{2} - \\mu_{X_2})(X_1 - \\mu_{X_1})^T\n",
    "+ \\Sigma_{12}\\Sigma_{22}^{-1}( X_{2} - \\mu_{X_2})\n",
    "\\Sigma_{21}\\Sigma_{22}( X_{2} - \\mu_{X_2})^T\n",
    "\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb067c3-911e-43de-902f-3feb1517f12e",
   "metadata": {},
   "source": [
    "\n",
    "$$ = \n",
    "\\Sigma_{11}\n",
    "- 2\\Sigma_{12}\\Sigma_{22}^{-1}\\Sigma_{21}\n",
    "+ \\Sigma_{12}\\Sigma_{22}^{-1}\\Sigma_{22}\\Sigma_{21}\\Sigma_{22}\n",
    "$$\n",
    "\n",
    "$$ = \n",
    "\\Sigma_{11}\n",
    "- 2\\Sigma_{12}\\Sigma_{22}^{-1}\\Sigma_{21}\n",
    "+ \\Sigma_{12}\\Sigma_{21}\\Sigma_{22}\n",
    "$$\n",
    "\n",
    "$$ = \n",
    "\\Sigma_{11}\n",
    "- 2\\Sigma_{12}\\Sigma_{22}^{-1}\\Sigma_{21}\n",
    "+ \\Sigma_{12}\\Sigma_{21}\\Sigma_{22}^{-1}\n",
    "$$\n",
    "\n",
    "$$ = \n",
    "\\Sigma_{11}\n",
    "- 2\\Sigma_{12}\\Sigma_{22}^{-1}\\Sigma_{21}\n",
    "+ \\Sigma_{12}\\Sigma_{22}^{-1}\\Sigma_{21}\n",
    "$$\n",
    "\n",
    "$$ = \n",
    "\\Sigma_{11} - \\Sigma_{12}\\Sigma_{22}^{-1}\\Sigma_{21}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6399f6-66c6-4b4c-a640-3d6dc5336e57",
   "metadata": {},
   "source": [
    "If we continue applying this technique we will derive the variance of Y:\n",
    "\n",
    "$$ \\Sigma_Y =\n",
    "\\begin{bmatrix}\n",
    "\\Sigma_{11} - \\Sigma_{12}\\Sigma_{22}^{-1}\\Sigma_{21} & 0 \\\\\n",
    "0 & \\Sigma_{22}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53869ec3-c527-45c9-83d8-8cb315f252ed",
   "metadata": {},
   "source": [
    "The Covarianct matrix tells us that $Y_1 \\perp Y_2$ because of the zeros in the top right and bottom left corners."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a154b91-bd72-49dc-b303-1da249196cb5",
   "metadata": {},
   "source": [
    "Because $\\mu_{Y_2} = \\mu_{X_2}$ and $\\Sigma_{Y_2} = \\Sigma_{X_2}$ (as shown above) we see that the moments between $Y_2$ and $X_2$ are the same and thus we can say that $Y_2 = X_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728f071c-dbde-4a33-ab72-84cd62e41377",
   "metadata": {},
   "source": [
    "Previosly we proved that the joint distribution for two independent variables is $f(x)f(y)$, we proved that the marginal distributions of the joint distribution are normal, we derived the moments of the marginal variables $Y_1$ and $Y_2$ and so now we can plug these parameters into the normal density functions and derive the joint distribution for our variables:\n",
    "\n",
    "$$ \\mathcal{N}_{Y1, Y2} = \n",
    "\\mathcal{N}_{Y_1}\n",
    "\\mathcal{N}_{Y_2} $$\n",
    "\n",
    "$$ =\n",
    "\\mathcal{N}(\\mu_{Y_1}, \\sigma_{Y_1})\n",
    "\\mathcal{N}(\\mu_{Y_2}, \\sigma_{Y_2})\n",
    "$$\n",
    "\n",
    "$$ =\n",
    "\\mathcal{N}(\\mu_{X_1} - \\Sigma_{12} \\Sigma_{22}^{-1} \\mu_{X_2}, \\Sigma_{11} - \\Sigma_{12}\\Sigma_{22}^{-1}\\Sigma_{21})\n",
    "\\mathcal{N}(\\mu_{X_1}, \\sigma_{X_2})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0424aca8-20b4-409b-adb5-41b56927b6fc",
   "metadata": {},
   "source": [
    "We can also derive the joint distribution of $X_1$ and $X_2$ from the joint distribution of $Y_1$ and $Y_2$ by performing a change of variable and multiplying by the jacobian (which is conveniently equal to 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745fced7-b31a-4889-9e8b-2c4366065a7c",
   "metadata": {},
   "source": [
    "We start with the basic definition of the joint distribution:\n",
    "\n",
    "$$\\mathcal{N}(\\mu, \\Sigma) := \n",
    "\\frac{1}{ \\sqrt{(2\\pi)^k |\\Sigma|}} \n",
    "exp \\left\\{ -\\frac{1}{2}(X-\\mu)^T\\Sigma^{-1}(X-\\mu) \\right\\}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6416291-5ef9-4b56-80a7-8440f695d4a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787643ab-042e-4a68-b278-f878f4ef4e64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d9e7f6-49cc-4a1b-8c9f-36afe79b321b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7df6d4e-f50a-41df-89a7-9d52c150d1e4",
   "metadata": {},
   "source": [
    "## Proof: Conditional Distribution Of Independent Variables\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf80422-6fce-46b3-a2f3-ed107943349d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18684d84-3ed1-4a60-8d86-be95485a94cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95638200-6b82-4ba5-a518-7500ef1c254f",
   "metadata": {},
   "source": [
    "von Mises, Richard (1964). Mathematical theory of probability and statistics. Chapter VIII.9.3. Academic Press."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae8d0bc-7464-4f91-a028-366b28c21395",
   "metadata": {},
   "source": [
    "## 3.4. History"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac09a348-ebfa-4500-8e2a-b1e9a6700502",
   "metadata": {},
   "source": [
    "p432"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855c580e-a1d6-45f0-95e8-6e068c48e8e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cbd70baa-d191-496f-b5d0-b3c9bfb4f6fd",
   "metadata": {},
   "source": [
    "# 4. Gaussian Distribution\n",
    "\n",
    "The gaussian distribution is a normal distribution whos mean is zero and standard deviation is unit length ie. equal to one.\n",
    "\n",
    "$$ \\mathcal{G} \\sim \\mathcal{N}(0,1) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad73610-9833-4e85-bec0-4aa84009570a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "18ea76f8-608a-4ba3-aaa7-1ad400c49fc1",
   "metadata": {},
   "source": [
    "# 5. Swithcing between joint and conditional probability\n",
    "I think if we switch between params we can chantge the distributions etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2a68ad-6cee-466f-94ef-7b7c8b461fd1",
   "metadata": {},
   "source": [
    "# 6. References\n",
    "- http://noahgolmant.com/writings/derivationsunivariatemultivariate.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
