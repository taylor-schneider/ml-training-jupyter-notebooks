{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28018ecf-79e0-4996-b36a-76e44841b947",
   "metadata": {},
   "source": [
    "# Overview\n",
    "It is possible to decompose a matrix. With decomposition we represent a matrix as an equivelent product of other matrices. It's kind of like factoring or expanding an exuation in regular algebra."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5775ac3b-b6c1-4113-be9c-fe6a9993ea9e",
   "metadata": {},
   "source": [
    "Matrix diagonalization is the process of decomposing a matrix in such a way that it can be represented as the a product of a new set of matrices. What differentiates diagonalization from the larger field of decomposition is that the set of component matrices contains a diagonal matrix of eigenvalues (hence the name).\n",
    "\n",
    "There are several techniques that can diagonalize a matrix and several results for the set of matrices representing the decomposition.\n",
    "\n",
    "https://mathworld.wolfram.com/MatrixDiagonalization.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beefd6e9-5bef-4233-a573-87e0cc92e389",
   "metadata": {},
   "source": [
    "## 1. Single Value Decomposition\n",
    "\n",
    "**Single Value Decomposition (SVD)** is a mathematial process of solving a system of equations to derive a specific set of mathematical objects. One of these objects is called an eigenvector and as such, SVD is sometimes refferred to as **eigen-decomposition**.\n",
    "\n",
    "There are two cases by which we do SVD: the case where our vector set produces a non-square matrix (asymetrical) and the case where it does (symetrical)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0854490-56bb-494e-b2ba-26cbac1c9615",
   "metadata": {},
   "source": [
    "### 1.1. The Asymetrical Case\n",
    "We can apply single value decomposition theory to decompose an original vector $A$ into orthogonal vectors ($U$ and $V^T$) and a diagonal ($\\Sigma$) matrix.\n",
    "\n",
    "$$ A = U \\Sigma V^T $$\n",
    "\n",
    "**Note:** An important property of orthogonal vectors is that they are uncorrelated.\n",
    "\n",
    "**Note:** In the case of SVD, the vectors $U$ and $V$ are referred to as **eigenvectors**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2010559e-c0f9-46bb-8906-4c9c751970a4",
   "metadata": {},
   "source": [
    "### 1.2. The Symmetrical Case\n",
    "\n",
    "If we know a matrix $A$ is symtric semi-definite positive, we know its eigenvectors are orthogonal ($Q$ and $Q^T$) and we can write:\n",
    "\n",
    "$$ A = Q \\Lambda Q^T$$\n",
    "\n",
    "Here $\\Lambda$ represents a the eigenvalue matrix. It is a square matrix with eigenvalues along the diagonals and zeros elsewhere. \n",
    "\n",
    "**Note:** The Symetrical case is is a special case such that $U = V = Q$. This arguably makes the problem much simpler to solve.\n",
    "\n",
    "**Note:** The covariance matrix is an example of a symetric positive definitie matrix.\n",
    "\n",
    "\n",
    "[source](https://ocw.mit.edu/courses/mathematics/18-06sc-linear-algebra-fall-2011/positive-definite-matrices-and-applications/singular-value-decomposition/MIT18_06SCF11_Ses3.5sum.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edefe004-111f-49d0-bb52-5d4a5d93c9ce",
   "metadata": {},
   "source": [
    "#### 1.2.1. Positive Semi-definite Matrix\n",
    "A matrix $A \\in \\mathbb R^{nxn} $ is positive semi-definite if\n",
    "$$ v^TAv \\ge 0, \\ \\ \\ \\ \\ \\ \\ \\ \\  \\forall v \\in \\mathbb R^n\n",
    "$$\n",
    "http://theanalysisofdata.com/probability/C_4.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31c10c5-a692-42a2-a7dd-fe8a0b13006b",
   "metadata": {},
   "source": [
    "# 2. LDU Decomposition\n",
    "\n",
    "## 2.1. Definition\n",
    "\n",
    "An LDU decomposition (a special type of LU decomposition) is such that a square matrix $M$ is decomposed as a product of a lower-triangular matrix $L$, a diagonal matrix $D$, and an upper-triangular matrix.\n",
    "\n",
    "$$ M = LDU $$\n",
    "\n",
    "If $M$ is invertible, then it admits an LU (or LDU) factorization if and only if all its leading principal minors are nonzero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50a27f4-3b78-43ec-90b2-30e4d91cc2b2",
   "metadata": {},
   "source": [
    "## 2.2. Notable Properties\n",
    "### 2.2.1. Constructing A Diagonal Matrix\n",
    "\n",
    "We can use LDU decomposition to diagonalize our covariance matrix $\\Sigma$. We start with the LDU decomposition\n",
    "\n",
    "$$ M = LDU $$\n",
    "\n",
    "We then multiply each side of the equation by the inverses of the component matrices $L$ and $U$. Recall that any matrix multiplied by its inverse is equal to one.\n",
    "\n",
    "$$ L^{-1}MU^{-1} = L^{-1}LDUU^{-1} $$\n",
    "$$ L^{-1}MU^{-1} = IDI$$\n",
    "$$ L^{-1}MU^{-1} = D$$\n",
    "\n",
    "Thus we can produce a diagonal matrix if we start with two triangular matrices and a square matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ab68bd-889f-4280-bd16-9d9d5d8dc17c",
   "metadata": {},
   "source": [
    "## 2.1. Use Cases\n",
    "### 2.1.1 Deriving Multivariate Normal Distribution\n",
    "We decompose a covariance matrix into $LDU$ we will see that $L$ and $U$ are mirrors of eachother. This is a convenient property as it means that $L^T = U$ and vice-versa. We will take advantage of this fact later on.\n",
    "\n",
    "If we then apply $L$ as a linear transformation to $Y$ representing a joint random variable (a linear combination of random variables) we can see that $Y$ is transformed such that its covariance matrix becomes diagonalized. This is useful because it implies that the transformatation has produced independent random variables.\n",
    "\n",
    "Becuase the transformed $Y$ is a linear combination of independent random variables we know it is normal and we know the distribution function. Deriving the two parameter for the distirbution yields the distribution function.\n",
    "\n",
    "https://michaellindon.github.io/lindonslog/mathematics/multivariate-normal-conditional-distribution/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b80913-5c76-4b6b-be23-b32352d24f32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd58d4f-af81-41f4-9f9a-a17c3e24cb34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7b1a78d-5e56-4403-aa8e-5954d2fd2f5f",
   "metadata": {},
   "source": [
    "## References\n",
    "- https://en.wikipedia.org/wiki/LU_decomposition#Lower-diagonal-upper_(LDU)_decomposition"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
